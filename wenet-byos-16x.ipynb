{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fecbc5ab-852b-4157-916d-8feb9207b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3.__version__:1.26.8\n",
      "sagemaker.__version__:2.116.0\n",
      "bucket:sagemaker-us-east-1-348052051973\n",
      "role:arn:aws:iam::348052051973:role/service-role/AmazonSageMakerServiceCatalogProductsExecutionRole\n",
      "CPU times: user 295 ms, sys: 20.2 ms, total: 315 ms\n",
      "Wall time: 445 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.utils import unique_name_from_base\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = get_execution_role()\n",
    "prefix = 'wenet'\n",
    "output_path = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "print(\"boto3.__version__:{}\".format(boto3.__version__))\n",
    "print(\"sagemaker.__version__:{}\".format(sagemaker.__version__))\n",
    "print(\"bucket:{}\".format(bucket))\n",
    "print(\"role:{}\".format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9aad794e-6139-4422-892b-936ccaaa3c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Copy the wenet/examples/librispeech/s0/*.sh and wenet/examples/librispeech/s0/local to wenet/ as requested by Sagemaker\n",
       "Overwrite the wenet/wenet/bin/train.py with the given one\n",
       "Change the /root/wenet to /opt/ml/input in all data.list files (especially for train_960 and dev)\n",
       "The \"Librispeech\" in data.list file in Github has the wrong captalization because it's wrong when I upload it. Please change it yourself!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "Copy the wenet/examples/librispeech/s0/*.sh and wenet/examples/librispeech/s0/local to wenet/ as requested by Sagemaker\n",
    "Overwrite the wenet/wenet/bin/train.py with the given one\n",
    "Change the /root/wenet to /opt/ml/input in all data.list files (especially for train_960 and dev)\n",
    "The \"Librispeech\" in data.list file in Github has the wrong captalization because it's wrong when I upload it. Please change it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "922ce9af-f711-4702-b811-4379efbcc8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p3.16xlarge\"\n",
    "max_run = 432000\n",
    "checkpoint_s3_uri = f\"s3://{bucket}/{prefix}/checkpoints\"\n",
    "\n",
    "hyperparameters = {\n",
    "    'datadir':'/opt/ml/input/data/training',\n",
    "    'stage': '4',\n",
    "    'stop_stage': '4',\n",
    "    'train_config': 'examples/librispeech/s0/conf/train_conformer.yaml',\n",
    "    'model_dir': '/opt/ml/model',\n",
    "    'checkpoint_dir': '/opt/ml/checkpoints',\n",
    "    'output_dir': '/opt/ml/output/data',\n",
    "}\n",
    "\n",
    "est = PyTorch(\n",
    "    entry_point=\"run-8gpu.sh\",\n",
    "    source_dir=\"./wenet\",\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=200,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=prefix,\n",
    "    hyperparameters = hyperparameters,\n",
    "    checkpoint_s3_uri = checkpoint_s3_uri,\n",
    "    output_path = f\"s3://{bucket}/{prefix}/\",\n",
    "    # keep_alive_period_in_seconds=1800,\n",
    "    max_run = max_run,\n",
    "    tags = [{\"Key\": \"team\", \"Value\": \"asr\"}, {\"Key\": \"person\", \"Value\": \"andrew\"}, {\"Key\": \"project\", \"Value\": \"abc\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "991f505d-b0c4-4aa1-abae-fd3c3d4534d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "prefix_dataset = \"wenet/export\"\n",
    "loc =f\"s3://{bucket}/{prefix_dataset}\"\n",
    "\n",
    "training = TrainingInput(\n",
    "    s3_data_type='S3Prefix', # Available Options: S3Prefix | ManifestFile | AugmentedManifestFile\n",
    "    s3_data=loc,\n",
    "    distribution='FullyReplicated', # Available Options: FullyReplicated | ShardedByS3Key \n",
    "    input_mode='FastFile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653e3f8-9feb-449b-9dad-a53b1735e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-01 07:41:57 Starting - Starting the training job.........\n",
      "2022-12-01 07:43:04 Starting - Preparing the instances for training......\n",
      "2022-12-01 07:44:22 Downloading - Downloading input data......\n",
      "2022-12-01 07:45:07 Training - Downloading the training image.......................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:06,975 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:07,053 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:07,060 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\n",
      "2022-12-01 07:49:03 Training - Training image download completed. Training in progress.\u001b[34m2022-12-01 07:49:20,067 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"checkpoint_dir\": \"/opt/ml/checkpoints\",\n",
      "        \"datadir\": \"/opt/ml/input/data/training\",\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"output_dir\": \"/opt/ml/output/data\",\n",
      "        \"stage\": \"4\",\n",
      "        \"stop_stage\": \"4\",\n",
      "        \"train_config\": \"examples/librispeech/s0/conf/train_conformer.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"wenet-2022-12-01-07-40-11-426\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-348052051973/wenet-2022-12-01-07-40-11-426/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run-8gpu.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run-8gpu.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"datadir\":\"/opt/ml/input/data/training\",\"model_dir\":\"/opt/ml/model\",\"output_dir\":\"/opt/ml/output/data\",\"stage\":\"4\",\"stop_stage\":\"4\",\"train_config\":\"examples/librispeech/s0/conf/train_conformer.yaml\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run-8gpu.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run-8gpu.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-348052051973/wenet-2022-12-01-07-40-11-426/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"datadir\":\"/opt/ml/input/data/training\",\"model_dir\":\"/opt/ml/model\",\"output_dir\":\"/opt/ml/output/data\",\"stage\":\"4\",\"stop_stage\":\"4\",\"train_config\":\"examples/librispeech/s0/conf/train_conformer.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"wenet-2022-12-01-07-40-11-426\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-348052051973/wenet-2022-12-01-07-40-11-426/source/sourcedir.tar.gz\",\"module_name\":\"run-8gpu.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run-8gpu.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--checkpoint_dir\",\"/opt/ml/checkpoints\",\"--datadir\",\"/opt/ml/input/data/training\",\"--model_dir\",\"/opt/ml/model\",\"--output_dir\",\"/opt/ml/output/data\",\"--stage\",\"4\",\"--stop_stage\",\"4\",\"--train_config\",\"examples/librispeech/s0/conf/train_conformer.yaml\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINT_DIR=/opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mSM_HP_DATADIR=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_HP_STAGE=4\u001b[0m\n",
      "\u001b[34mSM_HP_STOP_STAGE=4\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_CONFIG=examples/librispeech/s0/conf/train_conformer.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221003-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./run-8gpu.sh --checkpoint_dir /opt/ml/checkpoints --datadir /opt/ml/input/data/training --model_dir /opt/ml/model --output_dir /opt/ml/output/data --stage 4 --stop_stage 4 --train_config examples/librispeech/s0/conf/train_conformer.yaml\"\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (9.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 49.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 73.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboardX\u001b[0m\n",
      "\u001b[34mDownloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.4/125.4 kB 25.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting typeguard\u001b[0m\n",
      "\u001b[34mDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting textgrid\u001b[0m\n",
      "\u001b[34mDownloading TextGrid-1.5-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytest\u001b[0m\n",
      "\u001b[34mDownloading pytest-7.2.0-py3-none-any.whl (316 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.8/316.8 kB 32.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting flake8==3.8.2\u001b[0m\n",
      "\u001b[34mDownloading flake8-3.8.2-py2.py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.7/72.7 kB 14.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting flake8-bugbear\u001b[0m\n",
      "\u001b[34mDownloading flake8_bugbear-22.10.27-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-comprehensions\u001b[0m\n",
      "\u001b[34mDownloading flake8_comprehensions-3.10.1-py3-none-any.whl (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-executable\u001b[0m\n",
      "\u001b[34mDownloading flake8_executable-2.1.2-py3-none-any.whl (35 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-pyi==20.5.0\u001b[0m\n",
      "\u001b[34mDownloading flake8_pyi-20.5.0-py36-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting mccabe\u001b[0m\n",
      "\u001b[34mDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pycodestyle==2.6.0\u001b[0m\n",
      "\u001b[34mDownloading pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.4/41.4 kB 7.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyflakes==2.2.0\u001b[0m\n",
      "\u001b[34mDownloading pyflakes-2.2.0-py2.py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 12.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mccabe\u001b[0m\n",
      "\u001b[34mDownloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs in /opt/conda/lib/python3.8/site-packages (from flake8-pyi==20.5.0->-r requirements.txt (line 13)) (21.4.0)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.51.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 85.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (65.4.1)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.1-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 17.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (3.19.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (2.2.2)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 78.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (0.37.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 51.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.14.1-py2.py3-none-any.whl (175 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175.4/175.4 kB 26.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.3.0-py3-none-any.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.6/124.6 kB 23.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tomli>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading tomli-2.0.1-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from pytest->-r requirements.txt (line 8)) (21.3)\u001b[0m\n",
      "\u001b[34mCollecting iniconfig\u001b[0m\n",
      "\u001b[34mDownloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pluggy<2.0,>=0.12\u001b[0m\n",
      "\u001b[34mDownloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting exceptiongroup>=1.0.0rc8\u001b[0m\n",
      "\u001b[34mDownloading exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 29.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 4)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2022.9.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (1.26.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 4)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->pytest->-r requirements.txt (line 8)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 4)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 17.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: textgrid, tensorboard-plugin-wit, sentencepiece, mccabe, iniconfig, typeguard, tomli, tensorboardX, tensorboard-data-server, pyflakes, pycodestyle, pyasn1-modules, pluggy, oauthlib, grpcio, exceptiongroup, cachetools, absl-py, requests-oauthlib, pytest, markdown, google-auth, flake8, google-auth-oauthlib, flake8-pyi, flake8-executable, flake8-comprehensions, flake8-bugbear, tensorboard\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.3.0 cachetools-5.2.0 exceptiongroup-1.0.4 flake8-3.8.2 flake8-bugbear-22.10.27 flake8-comprehensions-3.10.1 flake8-executable-2.1.2 flake8-pyi-20.5.0 google-auth-2.14.1 google-auth-oauthlib-0.4.6 grpcio-1.51.1 iniconfig-1.1.1 markdown-3.4.1 mccabe-0.6.1 oauthlib-3.2.2 pluggy-1.0.0 pyasn1-modules-0.2.8 pycodestyle-2.6.0 pyflakes-2.2.0 pytest-7.2.0 requests-oauthlib-1.3.1 sentencepiece-0.1.97 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorboardX-2.5.1 textgrid-1.5 tomli-2.0.1 typeguard-2.13.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mdictionary: data/lang_char/train_clean_100_unigram5000_units.txt\u001b[0m\n",
      "\u001b[34m./run-8gpu.sh: init method is file:///opt/ml/code/exp/sp_spec_aug/ddp_init\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 0\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 1\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 2\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 3\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 4\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 5\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 6\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 7\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:39,608 INFO training on multiple gpus, this gpu 5\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:39,608 INFO training on multiple gpus, this gpu 3\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:39,608 INFO training on multiple gpus, this gpu 7\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:39,608 INFO training on multiple gpus, this gpu 1\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:39,608 INFO training on multiple gpus, this gpu 4\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:39,608 INFO training on multiple gpus, this gpu 2\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:39,608 INFO training on multiple gpus, this gpu 0\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:39,609 INFO training on multiple gpus, this gpu 6\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,290 INFO Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,329 INFO Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,365 INFO Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,417 INFO Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,470 INFO Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,528 INFO Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,595 INFO Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,663 INFO Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,663 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,664 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,666 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,667 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,668 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,671 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,671 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,672 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,690 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,690 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,691 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,691 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,691 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,691 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,692 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,692 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,694 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,694 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,696 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,696 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,696 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,697 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,699 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:40,699 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,789,396\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,789,396\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,789,396\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,789,396\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,789,396\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,789,396\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,789,396\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,789,396\u001b[0m\n",
      "\u001b[34malgo-1:174:174 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:174:174 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34mNCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34malgo-1:204:204 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:204:204 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:194:194 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:194:194 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:179:179 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:179:179 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:199:199 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:208:208 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:199:199 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:208:208 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:189:189 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:189:189 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:184:184 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:184:184 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,660 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,660 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,661 INFO Checkpoint: save to checkpoint /opt/ml/checkpoints/init.pt\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,661 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,661 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,661 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,661 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,661 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,662 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,662 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,663 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,663 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,663 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,663 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:47,664 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.186 algo-1:189 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.188 algo-1:204 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.188 algo-1:199 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.189 algo-1:208 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.189 algo-1:184 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.191 algo-1:179 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.194 algo-1:194 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:48,212 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-12-01 07:49:48,220 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.355 algo-1:179 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.355 algo-1:189 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.355 algo-1:184 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.355 algo-1:208 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.355 algo-1:199 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.355 algo-1:194 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.395 algo-1:204 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.826 algo-1:174 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-12-01 07:49:48.985 algo-1:174 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m2022-12-01 07:52:48,921 DEBUG TRAIN Batch 0/0 loss 207.210144 loss_att 79.375870 loss_ctc 505.490082 lr 0.00000016 rank 5\u001b[0m\n",
      "\u001b[34m2022-12-01 07:52:48,921 DEBUG TRAIN Batch 0/0 loss 223.170288 loss_att 86.205673 loss_ctc 542.754333 lr 0.00000016 rank 4\u001b[0m\n",
      "\u001b[34m2022-12-01 07:52:48,923 DEBUG TRAIN Batch 0/0 loss 200.831024 loss_att 70.189507 loss_ctc 505.661224 lr 0.00000016 rank 7\u001b[0m\n",
      "\u001b[34m2022-12-01 07:52:48,930 DEBUG TRAIN Batch 0/0 loss 196.307953 loss_att 72.594635 loss_ctc 484.972351 lr 0.00000016 rank 6\u001b[0m\n",
      "\u001b[34m2022-12-01 07:52:48,930 DEBUG TRAIN Batch 0/0 loss 208.293701 loss_att 72.087547 loss_ctc 526.108032 lr 0.00000016 rank 3\u001b[0m\n",
      "\u001b[34m2022-12-01 07:52:48,932 DEBUG TRAIN Batch 0/0 loss 191.980988 loss_att 69.443260 loss_ctc 477.902344 lr 0.00000016 rank 2\u001b[0m\n",
      "\u001b[34m2022-12-01 07:52:48,932 DEBUG TRAIN Batch 0/0 loss 179.370239 loss_att 59.647568 loss_ctc 458.723114 lr 0.00000016 rank 1\u001b[0m\n",
      "\u001b[34m2022-12-01 07:52:48,937 DEBUG TRAIN Batch 0/0 loss 210.001801 loss_att 88.301422 loss_ctc 493.969330 lr 0.00000016 rank 0\u001b[0m\n",
      "\u001b[34m2022-12-01 07:53:26,341 DEBUG TRAIN Batch 0/100 loss 398.734985 loss_att 364.627808 loss_ctc 478.318451 lr 0.00001616 rank 2\u001b[0m\n",
      "\u001b[34m2022-12-01 07:53:26,344 DEBUG TRAIN Batch 0/100 loss 370.374329 loss_att 336.700409 loss_ctc 448.946777 lr 0.00001616 rank 5\u001b[0m\n",
      "\u001b[34m2022-12-01 07:53:26,346 DEBUG TRAIN Batch 0/100 loss 370.097839 loss_att 335.333069 loss_ctc 451.215668 lr 0.00001616 rank 4\u001b[0m\n",
      "\u001b[34m2022-12-01 07:53:26,349 DEBUG TRAIN Batch 0/100 loss 418.032349 loss_att 384.880249 loss_ctc 495.387299 lr 0.00001616 rank 7\u001b[0m\n",
      "\u001b[34m2022-12-01 07:53:26,350 DEBUG TRAIN Batch 0/100 loss 387.313202 loss_att 353.677826 loss_ctc 465.795715 lr 0.00001616 rank 1\u001b[0m\n",
      "\u001b[34m2022-12-01 07:53:26,353 DEBUG TRAIN Batch 0/100 loss 376.035858 loss_att 342.924896 loss_ctc 453.294769 lr 0.00001616 rank 6\u001b[0m\n",
      "\u001b[34m2022-12-01 07:53:26,359 DEBUG TRAIN Batch 0/100 loss 402.008789 loss_att 370.285889 loss_ctc 476.028900 lr 0.00001616 rank 3\u001b[0m\n",
      "\u001b[34m2022-12-01 07:53:26,367 DEBUG TRAIN Batch 0/100 loss 345.247650 loss_att 312.148407 loss_ctc 422.479218 lr 0.00001616 rank 0\u001b[0m\n",
      "\u001b[34m2022-12-01 07:56:18,053 DEBUG TRAIN Batch 0/200 loss 338.136353 loss_att 315.974792 loss_ctc 389.846680 lr 0.00003216 rank 2\u001b[0m\n",
      "\u001b[34m2022-12-01 07:56:18,057 DEBUG TRAIN Batch 0/200 loss 361.569305 loss_att 337.571533 loss_ctc 417.564087 lr 0.00003216 rank 5\u001b[0m\n",
      "\u001b[34m2022-12-01 07:56:18,058 DEBUG TRAIN Batch 0/200 loss 338.725922 loss_att 316.741516 loss_ctc 390.022888 lr 0.00003216 rank 3\u001b[0m\n",
      "\u001b[34m2022-12-01 07:56:18,061 DEBUG TRAIN Batch 0/200 loss 352.716675 loss_att 330.541718 loss_ctc 404.458191 lr 0.00003216 rank 1\u001b[0m\n",
      "\u001b[34m2022-12-01 07:56:18,060 DEBUG TRAIN Batch 0/200 loss 322.346527 loss_att 301.743805 loss_ctc 370.419525 lr 0.00003216 rank 4\u001b[0m\n",
      "\u001b[34m2022-12-01 07:56:18,060 DEBUG TRAIN Batch 0/200 loss 338.291290 loss_att 316.993439 loss_ctc 387.986298 lr 0.00003216 rank 6\u001b[0m\n",
      "\u001b[34m2022-12-01 07:56:18,080 DEBUG TRAIN Batch 0/200 loss 335.476257 loss_att 314.655518 loss_ctc 384.057983 lr 0.00003216 rank 0\u001b[0m\n",
      "\u001b[34m2022-12-01 07:56:18,092 DEBUG TRAIN Batch 0/200 loss 337.119110 loss_att 315.337158 loss_ctc 387.943695 lr 0.00003216 rank 7\u001b[0m\n",
      "\u001b[34m2022-12-01 07:57:58,762 DEBUG CV Batch 0/0 loss 35.991791 loss_att 34.756950 loss_ctc 38.873089 history loss 33.223191 rank 4\u001b[0m\n",
      "\u001b[34m2022-12-01 07:57:58,763 DEBUG CV Batch 0/0 loss 35.991791 loss_att 34.756950 loss_ctc 38.873089 history loss 33.223191 rank 5\u001b[0m\n",
      "\u001b[34m2022-12-01 07:57:58,767 DEBUG CV Batch 0/0 loss 35.991791 loss_att 34.756950 loss_ctc 38.873089 history loss 33.223191 rank 2\u001b[0m\n",
      "\u001b[34m2022-12-01 07:57:58,769 DEBUG CV Batch 0/0 loss 35.991791 loss_att 34.756950 loss_ctc 38.873089 history loss 33.223191 rank 1\u001b[0m\n",
      "\u001b[34m2022-12-01 07:57:58,770 DEBUG CV Batch 0/0 loss 35.991791 loss_att 34.756950 loss_ctc 38.873089 history loss 33.223191 rank 6\u001b[0m\n",
      "\u001b[34m2022-12-01 07:57:58,772 DEBUG CV Batch 0/0 loss 35.991791 loss_att 34.756950 loss_ctc 38.873089 history loss 33.223191 rank 3\u001b[0m\n",
      "\u001b[34m2022-12-01 07:57:58,774 DEBUG CV Batch 0/0 loss 35.991791 loss_att 34.756950 loss_ctc 38.873089 history loss 33.223191 rank 0\u001b[0m\n",
      "\u001b[34m2022-12-01 07:57:58,796 DEBUG CV Batch 0/0 loss 35.991791 loss_att 34.756950 loss_ctc 38.873089 history loss 33.223191 rank 7\u001b[0m\n",
      "\u001b[34m2022-12-01 07:59:13,955 DEBUG CV Batch 0/100 loss 116.566032 loss_att 111.441772 loss_ctc 128.522644 history loss 133.428017 rank 4\u001b[0m\n",
      "\u001b[34m2022-12-01 07:59:13,958 DEBUG CV Batch 0/100 loss 116.566032 loss_att 111.441772 loss_ctc 128.522644 history loss 133.428017 rank 1\u001b[0m\n",
      "\u001b[34m2022-12-01 07:59:13,965 DEBUG CV Batch 0/100 loss 116.566032 loss_att 111.441772 loss_ctc 128.522644 history loss 133.428017 rank 7\u001b[0m\n",
      "\u001b[34m2022-12-01 07:59:13,972 DEBUG CV Batch 0/100 loss 116.566032 loss_att 111.441772 loss_ctc 128.522644 history loss 133.428017 rank 5\u001b[0m\n",
      "\u001b[34m2022-12-01 07:59:13,991 DEBUG CV Batch 0/100 loss 116.566032 loss_att 111.441772 loss_ctc 128.522644 history loss 133.428017 rank 6\u001b[0m\n",
      "\u001b[34m2022-12-01 07:59:14,004 DEBUG CV Batch 0/100 loss 116.566032 loss_att 111.441772 loss_ctc 128.522644 history loss 133.428017 rank 3\u001b[0m\n",
      "\u001b[34m2022-12-01 07:59:14,012 DEBUG CV Batch 0/100 loss 116.566032 loss_att 111.441772 loss_ctc 128.522644 history loss 133.428017 rank 0\u001b[0m\n",
      "\u001b[34m2022-12-01 07:59:14,019 DEBUG CV Batch 0/100 loss 116.566032 loss_att 111.441772 loss_ctc 128.522644 history loss 133.428017 rank 2\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:27,830 DEBUG CV Batch 0/200 loss 221.546722 loss_att 211.278229 loss_ctc 245.506516 history loss 141.975129 rank 2\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:27,960 DEBUG CV Batch 0/200 loss 221.546722 loss_att 211.278229 loss_ctc 245.506516 history loss 141.975129 rank 4\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:27,966 DEBUG CV Batch 0/200 loss 221.546722 loss_att 211.278229 loss_ctc 245.506516 history loss 141.975129 rank 1\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:27,996 DEBUG CV Batch 0/200 loss 221.546722 loss_att 211.278229 loss_ctc 245.506516 history loss 141.975129 rank 3\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:28,008 DEBUG CV Batch 0/200 loss 221.546722 loss_att 211.278229 loss_ctc 245.506516 history loss 141.975129 rank 6\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:28,014 DEBUG CV Batch 0/200 loss 221.546722 loss_att 211.278229 loss_ctc 245.506516 history loss 141.975129 rank 7\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:28,062 DEBUG CV Batch 0/200 loss 221.546722 loss_att 211.278229 loss_ctc 245.506516 history loss 141.975129 rank 0\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:28,091 DEBUG CV Batch 0/200 loss 221.546722 loss_att 211.278229 loss_ctc 245.506516 history loss 141.975129 rank 5\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,551 INFO Epoch 0 CV info cv_loss 147.72076826496104\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,551 INFO Epoch 1 TRAIN info lr 4.768e-05\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,554 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,574 INFO Epoch 0 CV info cv_loss 147.72076826496104\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,574 INFO Epoch 0 CV info cv_loss 147.72076826496104\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,574 INFO Checkpoint: save to checkpoint /opt/ml/checkpoints/0.pt\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,574 INFO Epoch 1 TRAIN info lr 4.768e-05\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,576 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,585 INFO Epoch 0 CV info cv_loss 147.72076826496104\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,585 INFO Epoch 1 TRAIN info lr 4.768e-05\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,587 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,596 INFO Epoch 0 CV info cv_loss 147.72076826496104\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,597 INFO Epoch 1 TRAIN info lr 4.768e-05\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,601 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,601 INFO Epoch 0 CV info cv_loss 147.72076826496104\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,602 INFO Epoch 1 TRAIN info lr 4.768e-05\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,603 INFO Epoch 0 CV info cv_loss 147.72076826496104\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,603 INFO Epoch 1 TRAIN info lr 4.768e-05\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,604 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,606 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,630 INFO Epoch 0 CV info cv_loss 147.72076826496104\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,630 INFO Epoch 1 TRAIN info lr 4.768e-05\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:38,633 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:39,117 INFO Epoch 1 TRAIN info lr 4.768e-05\u001b[0m\n",
      "\u001b[34m2022-12-01 08:00:39,121 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-12-01 08:03:43,243 DEBUG TRAIN Batch 1/0 loss 52.854050 loss_att 50.902634 loss_ctc 57.407349 lr 0.00004784 rank 2\u001b[0m\n",
      "\u001b[34m2022-12-01 08:03:43,246 DEBUG TRAIN Batch 1/0 loss 56.650055 loss_att 54.417999 loss_ctc 61.858196 lr 0.00004784 rank 6\u001b[0m\n",
      "\u001b[34m2022-12-01 08:03:43,247 DEBUG TRAIN Batch 1/0 loss 61.219566 loss_att 58.865200 loss_ctc 66.713097 lr 0.00004784 rank 3\u001b[0m\n",
      "\u001b[34m2022-12-01 08:03:43,247 DEBUG TRAIN Batch 1/0 loss 59.073574 loss_att 56.487801 loss_ctc 65.107048 lr 0.00004784 rank 4\u001b[0m\n",
      "\u001b[34m2022-12-01 08:03:43,251 DEBUG TRAIN Batch 1/0 loss 60.844765 loss_att 58.570602 loss_ctc 66.151138 lr 0.00004784 rank 1\u001b[0m\n",
      "\u001b[34m2022-12-01 08:03:43,251 DEBUG TRAIN Batch 1/0 loss 57.918789 loss_att 55.647308 loss_ctc 63.218914 lr 0.00004784 rank 7\u001b[0m\n",
      "\u001b[34m2022-12-01 08:03:43,277 DEBUG TRAIN Batch 1/0 loss 54.602600 loss_att 52.637787 loss_ctc 59.187157 lr 0.00004784 rank 0\u001b[0m\n",
      "\u001b[34m2022-12-01 08:03:43,311 DEBUG TRAIN Batch 1/0 loss 54.760391 loss_att 52.690247 loss_ctc 59.590736 lr 0.00004784 rank 5\u001b[0m\n",
      "\u001b[34m2022-12-01 08:04:15,386 DEBUG TRAIN Batch 1/100 loss 264.990356 loss_att 251.445892 loss_ctc 296.594116 lr 0.00006384 rank 7\u001b[0m\n",
      "\u001b[34m2022-12-01 08:04:15,387 DEBUG TRAIN Batch 1/100 loss 297.553375 loss_att 282.635742 loss_ctc 332.361206 lr 0.00006384 rank 5\u001b[0m\n",
      "\u001b[34m2022-12-01 08:04:15,391 DEBUG TRAIN Batch 1/100 loss 263.067383 loss_att 249.942719 loss_ctc 293.691650 lr 0.00006384 rank 2\u001b[0m\n",
      "\u001b[34m2022-12-01 08:04:15,392 DEBUG TRAIN Batch 1/100 loss 286.908325 loss_att 271.963440 loss_ctc 321.779724 lr 0.00006384 rank 4\u001b[0m\n",
      "\u001b[34m2022-12-01 08:04:15,393 DEBUG TRAIN Batch 1/100 loss 284.907928 loss_att 270.539246 loss_ctc 318.434875 lr 0.00006384 rank 6\u001b[0m\n",
      "\u001b[34m2022-12-01 08:04:15,393 DEBUG TRAIN Batch 1/100 loss 282.085449 loss_att 267.888885 loss_ctc 315.210724 lr 0.00006384 rank 1\u001b[0m\n",
      "\u001b[34m2022-12-01 08:04:15,397 DEBUG TRAIN Batch 1/100 loss 257.746735 loss_att 244.343506 loss_ctc 289.020935 lr 0.00006384 rank 3\u001b[0m\n",
      "\u001b[34m2022-12-01 08:04:15,413 DEBUG TRAIN Batch 1/100 loss 244.858765 loss_att 232.458984 loss_ctc 273.791565 lr 0.00006384 rank 0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "job_name = est.fit({\"training\":training})\n",
    "#job_name = est.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d07b7c9-acde-4d0a-b179-3134d28f6f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact saved at:\n",
      " s3://sagemaker-us-east-1-348052051973/wenet/wenet-2022-12-01-06-28-23-628/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_data = est.model_data\n",
    "print(\"Model artifact saved at:\\n\", model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55192e1-23f5-4c2f-8c0f-5d1c43655226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
