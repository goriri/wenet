{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fecbc5ab-852b-4157-916d-8feb9207b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3.__version__:1.26.8\n",
      "sagemaker.__version__:2.116.0\n",
      "bucket:sagemaker-us-east-1-348052051973\n",
      "role:arn:aws:iam::348052051973:role/service-role/AmazonSageMakerServiceCatalogProductsExecutionRole\n",
      "CPU times: user 295 ms, sys: 20.2 ms, total: 315 ms\n",
      "Wall time: 445 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.utils import unique_name_from_base\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = get_execution_role()\n",
    "prefix = 'wenet'\n",
    "output_path = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "print(\"boto3.__version__:{}\".format(boto3.__version__))\n",
    "print(\"sagemaker.__version__:{}\".format(sagemaker.__version__))\n",
    "print(\"bucket:{}\".format(bucket))\n",
    "print(\"role:{}\".format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9aad794e-6139-4422-892b-936ccaaa3c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Copy the wenet/examples/librispeech/s0/*.sh and wenet/examples/librispeech/s0/local to wenet/ as requested by Sagemaker\n",
       "Overwrite the wenet/wenet/bin/train.py with the given one\n",
       "Change the /root/wenet to /opt/ml/input in all data.list files (especially for train_960 and dev)\n",
       "The \"Librispeech\" in data.list file in Github has the wrong captalization because it's wrong when I upload it. Please change it yourself!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "Copy the wenet/examples/librispeech/s0/*.sh and wenet/examples/librispeech/s0/local to wenet/ as requested by Sagemaker\n",
    "Overwrite the wenet/wenet/bin/train.py with the given one\n",
    "Change the /root/wenet to /opt/ml/input in all data.list files (especially for train_960 and dev)\n",
    "The \"Librispeech\" in data.list file in Github has the wrong captalization because it's wrong when I upload it. Please change it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "922ce9af-f711-4702-b811-4379efbcc8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p3.16xlarge\"\n",
    "max_run = 432000\n",
    "checkpoint_s3_uri = f\"s3://{bucket}/{prefix}/checkpoints\"\n",
    "\n",
    "hyperparameters = {\n",
    "    'datadir':'/opt/ml/input/data/training',\n",
    "    'stage': '4',\n",
    "    'stop_stage': '5',\n",
    "    'train_config': 'examples/librispeech/s0/conf/train_conformer.yaml',\n",
    "    'model_dir': '/opt/ml/model',\n",
    "    'checkpoint_dir': '/opt/ml/checkpoints',\n",
    "    'output_dir': '/opt/ml/output',\n",
    "}\n",
    "\n",
    "est = PyTorch(\n",
    "    entry_point=\"run-8gpu.sh\",\n",
    "    source_dir=\"./wenet\",\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=200,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=prefix,\n",
    "    hyperparameters = hyperparameters,\n",
    "    checkpoint_s3_uri = checkpoint_s3_uri,\n",
    "    # keep_alive_period_in_seconds=1800,\n",
    "    max_run = max_run,\n",
    "    tags = [{\"Key\": \"team\", \"Value\": \"asr\"}, {\"Key\": \"person\", \"Value\": \"andrew\"}, {\"Key\": \"project\", \"Value\": \"abc\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "991f505d-b0c4-4aa1-abae-fd3c3d4534d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "prefix_dataset = \"wenet/export\"\n",
    "loc =f\"s3://{bucket}/{prefix_dataset}\"\n",
    "\n",
    "training = TrainingInput(\n",
    "    s3_data_type='S3Prefix', # Available Options: S3Prefix | ManifestFile | AugmentedManifestFile\n",
    "    s3_data=loc,\n",
    "    distribution='FullyReplicated', # Available Options: FullyReplicated | ShardedByS3Key \n",
    "    input_mode='FastFile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653e3f8-9feb-449b-9dad-a53b1735e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-30 03:42:49 Starting - Starting the training job......\n",
      "2022-11-30 03:43:45 Starting - Preparing the instances for training.........\n",
      "2022-11-30 03:45:18 Downloading - Downloading input data......\n",
      "2022-11-30 03:46:13 Training - Downloading the training image.......................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-11-30 03:50:02,828 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-11-30 03:50:02,904 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-11-30 03:50:02,913 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\n",
      "2022-11-30 03:49:59 Training - Training image download completed. Training in progress.\u001b[34m2022-11-30 03:50:14,936 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"checkpoint_dir\": \"/opt/ml/checkpoints\",\n",
      "        \"datadir\": \"/opt/ml/input/data/training\",\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"output_dir\": \"/opt/ml/output\",\n",
      "        \"stage\": \"4\",\n",
      "        \"stop_stage\": \"5\",\n",
      "        \"train_config\": \"examples/librispeech/s0/conf/train_conformer.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"wenet-2022-11-30-03-41-08-675\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-348052051973/wenet-2022-11-30-03-41-08-675/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"datadir\":\"/opt/ml/input/data/training\",\"model_dir\":\"/opt/ml/model\",\"output_dir\":\"/opt/ml/output\",\"stage\":\"4\",\"stop_stage\":\"5\",\"train_config\":\"examples/librispeech/s0/conf/train_conformer.yaml\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-348052051973/wenet-2022-11-30-03-41-08-675/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"datadir\":\"/opt/ml/input/data/training\",\"model_dir\":\"/opt/ml/model\",\"output_dir\":\"/opt/ml/output\",\"stage\":\"4\",\"stop_stage\":\"5\",\"train_config\":\"examples/librispeech/s0/conf/train_conformer.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"wenet-2022-11-30-03-41-08-675\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-348052051973/wenet-2022-11-30-03-41-08-675/source/sourcedir.tar.gz\",\"module_name\":\"run.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--checkpoint_dir\",\"/opt/ml/checkpoints\",\"--datadir\",\"/opt/ml/input/data/training\",\"--model_dir\",\"/opt/ml/model\",\"--output_dir\",\"/opt/ml/output\",\"--stage\",\"4\",\"--stop_stage\",\"5\",\"--train_config\",\"examples/librispeech/s0/conf/train_conformer.yaml\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINT_DIR=/opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mSM_HP_DATADIR=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_HP_STAGE=4\u001b[0m\n",
      "\u001b[34mSM_HP_STOP_STAGE=5\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_CONFIG=examples/librispeech/s0/conf/train_conformer.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221003-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./run.sh --checkpoint_dir /opt/ml/checkpoints --datadir /opt/ml/input/data/training --model_dir /opt/ml/model --output_dir /opt/ml/output --stage 4 --stop_stage 5 --train_config examples/librispeech/s0/conf/train_conformer.yaml\"\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (9.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 45.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 70.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboardX\u001b[0m\n",
      "\u001b[34mDownloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.4/125.4 kB 21.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting typeguard\u001b[0m\n",
      "\u001b[34mDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting textgrid\u001b[0m\n",
      "\u001b[34mDownloading TextGrid-1.5-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytest\u001b[0m\n",
      "\u001b[34mDownloading pytest-7.2.0-py3-none-any.whl (316 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.8/316.8 kB 16.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting flake8==3.8.2\u001b[0m\n",
      "\u001b[34mDownloading flake8-3.8.2-py2.py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.7/72.7 kB 11.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting flake8-bugbear\u001b[0m\n",
      "\u001b[34mDownloading flake8_bugbear-22.10.27-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-comprehensions\u001b[0m\n",
      "\u001b[34mDownloading flake8_comprehensions-3.10.1-py3-none-any.whl (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-executable\u001b[0m\n",
      "\u001b[34mDownloading flake8_executable-2.1.2-py3-none-any.whl (35 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-pyi==20.5.0\u001b[0m\n",
      "\u001b[34mDownloading flake8_pyi-20.5.0-py36-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting mccabe\u001b[0m\n",
      "\u001b[34mDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pycodestyle==2.6.0\u001b[0m\n",
      "\u001b[34mDownloading pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.4/41.4 kB 3.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyflakes==2.2.0\u001b[0m\n",
      "\u001b[34mDownloading pyflakes-2.2.0-py2.py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 5.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mccabe\u001b[0m\n",
      "\u001b[34mDownloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs in /opt/conda/lib/python3.8/site-packages (from flake8-pyi==20.5.0->-r requirements.txt (line 13)) (21.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (0.37.1)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.3.0-py3-none-any.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.6/124.6 kB 12.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (65.4.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 44.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 70.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.51.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 77.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.14.1-py2.py3-none-any.whl (175 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175.4/175.4 kB 27.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.1-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 16.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (3.19.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (2.2.2)\u001b[0m\n",
      "\u001b[34mCollecting pluggy<2.0,>=0.12\u001b[0m\n",
      "\u001b[34mDownloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting iniconfig\u001b[0m\n",
      "\u001b[34mDownloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting exceptiongroup>=1.0.0rc8\u001b[0m\n",
      "\u001b[34mDownloading exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from pytest->-r requirements.txt (line 8)) (21.3)\u001b[0m\n",
      "\u001b[34mCollecting tomli>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading tomli-2.0.1-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 13.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 4)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (1.26.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2022.9.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 4)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->pytest->-r requirements.txt (line 8)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 4)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 27.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: textgrid, tensorboard-plugin-wit, sentencepiece, mccabe, iniconfig, typeguard, tomli, tensorboardX, tensorboard-data-server, pyflakes, pycodestyle, pyasn1-modules, pluggy, oauthlib, grpcio, exceptiongroup, cachetools, absl-py, requests-oauthlib, pytest, markdown, google-auth, flake8, google-auth-oauthlib, flake8-pyi, flake8-executable, flake8-comprehensions, flake8-bugbear, tensorboard\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.3.0 cachetools-5.2.0 exceptiongroup-1.0.4 flake8-3.8.2 flake8-bugbear-22.10.27 flake8-comprehensions-3.10.1 flake8-executable-2.1.2 flake8-pyi-20.5.0 google-auth-2.14.1 google-auth-oauthlib-0.4.6 grpcio-1.51.1 iniconfig-1.1.1 markdown-3.4.1 mccabe-0.6.1 oauthlib-3.2.2 pluggy-1.0.0 pyasn1-modules-0.2.8 pycodestyle-2.6.0 pyflakes-2.2.0 pytest-7.2.0 requests-oauthlib-1.3.1 sentencepiece-0.1.97 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorboardX-2.5.1 textgrid-1.5 tomli-2.0.1 typeguard-2.13.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mdictionary: data/lang_char/train_960_unigram5000_units.txt\u001b[0m\n",
      "\u001b[34m./run.sh: init method is file:///opt/ml/code/exp/sp_spec_aug/ddp_init\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34m2022-11-30 03:50:35,154 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-30 03:50:35,154 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34mASRModel(\n",
      "  (encoder): ConformerEncoder(\n",
      "    (global_cmvn): GlobalCMVN()\n",
      "    (embed): Conv2dSubsampling4(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (out): Sequential(\n",
      "        (0): Linear(in_features=4864, out_features=256, bias=True)\n",
      "      )\n",
      "      (pos_enc): RelPositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoders): ModuleList(\n",
      "      (0): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (1): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (2): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (3): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (4): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (5): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (6): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (7): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (8): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (9): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (10): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (11): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed): Sequential(\n",
      "      (0): Embedding(5001, 256)\n",
      "      (1): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_layer): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (decoders): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (4): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (5): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ctc): CTC(\n",
      "    (ctc_lo): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (ctc_loss): CTCLoss()\n",
      "  )\n",
      "  (criterion_att): LabelSmoothingLoss(\n",
      "    (criterion): KLDivLoss()\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34m2022-11-30 03:50:44,329 INFO Checkpoint: save to checkpoint /opt/ml/checkpoints/init.pt\u001b[0m\n",
      "\u001b[34m2022-11-30 03:50:44,712 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-11-30 03:50:44,714 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m[2022-11-30 03:50:45.265 algo-1:173 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-11-30 03:50:45.437 algo-1:173 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m2022-11-30 03:54:07,823 DEBUG TRAIN Batch 0/0 loss 196.478836 loss_att 73.712662 loss_ctc 482.933228 lr 0.00000016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 03:54:36,735 DEBUG TRAIN Batch 0/100 loss 354.576538 loss_att 319.202332 loss_ctc 437.116302 lr 0.00001616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 03:57:54,144 DEBUG TRAIN Batch 0/200 loss 328.514160 loss_att 309.293518 loss_ctc 373.362366 lr 0.00003216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:01:05,220 DEBUG TRAIN Batch 0/300 loss 220.146347 loss_att 210.322311 loss_ctc 243.069092 lr 0.00004816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:04:25,651 DEBUG TRAIN Batch 0/400 loss 280.459595 loss_att 268.066742 loss_ctc 309.376282 lr 0.00006416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:07:46,910 DEBUG TRAIN Batch 0/500 loss 45.057945 loss_att 43.396667 loss_ctc 48.934258 lr 0.00008016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:08:14,800 DEBUG TRAIN Batch 0/600 loss 269.563507 loss_att 256.356323 loss_ctc 300.380310 lr 0.00009616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:11:35,618 DEBUG TRAIN Batch 0/700 loss 283.226379 loss_att 267.846497 loss_ctc 319.112793 lr 0.00011216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:14:55,803 DEBUG TRAIN Batch 0/800 loss 200.416138 loss_att 188.866440 loss_ctc 227.365387 lr 0.00012816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:28:57,123 DEBUG TRAIN Batch 0/1300 loss 188.498291 loss_att 177.159546 loss_ctc 214.955383 lr 0.00020816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:32:22,348 DEBUG TRAIN Batch 0/1400 loss 304.151733 loss_att 283.663605 loss_ctc 351.957336 lr 0.00022416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:35:49,257 DEBUG TRAIN Batch 0/1500 loss 49.704395 loss_att 47.181541 loss_ctc 55.591057 lr 0.00024016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:36:17,685 DEBUG TRAIN Batch 0/1600 loss 246.022095 loss_att 230.528915 loss_ctc 282.172852 lr 0.00025616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:39:40,437 DEBUG TRAIN Batch 0/1700 loss 280.884338 loss_att 263.957214 loss_ctc 320.380920 lr 0.00027216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:43:04,866 DEBUG TRAIN Batch 0/1800 loss 175.871094 loss_att 165.455200 loss_ctc 200.174805 lr 0.00028816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:46:25,284 DEBUG TRAIN Batch 0/1900 loss 262.134583 loss_att 250.340744 loss_ctc 289.653473 lr 0.00030416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:49:42,158 DEBUG TRAIN Batch 0/2000 loss 44.917046 loss_att 43.835602 loss_ctc 47.440411 lr 0.00032016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:50:10,771 DEBUG TRAIN Batch 0/2100 loss 240.127716 loss_att 236.339203 loss_ctc 248.967575 lr 0.00033616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:53:27,763 DEBUG TRAIN Batch 0/2200 loss 254.264221 loss_att 253.109253 loss_ctc 256.959167 lr 0.00035216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:56:39,645 DEBUG TRAIN Batch 0/2300 loss 149.233658 loss_att 150.055206 loss_ctc 147.316711 lr 0.00036816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 04:59:47,920 DEBUG TRAIN Batch 0/2400 loss 287.383942 loss_att 289.347870 loss_ctc 282.801483 lr 0.00038416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:03:11,852 DEBUG TRAIN Batch 0/2500 loss 40.383129 loss_att 40.702099 loss_ctc 39.638866 lr 0.00040016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:03:39,443 DEBUG TRAIN Batch 0/2600 loss 223.461578 loss_att 226.492279 loss_ctc 216.389984 lr 0.00041616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:07:06,010 DEBUG TRAIN Batch 0/2700 loss 238.493118 loss_att 244.661057 loss_ctc 224.101242 lr 0.00043216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:10:33,430 DEBUG TRAIN Batch 0/2800 loss 161.576294 loss_att 162.313660 loss_ctc 159.855804 lr 0.00044816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:14:02,261 DEBUG TRAIN Batch 0/2900 loss 221.220139 loss_att 235.113861 loss_ctc 188.801483 lr 0.00046416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:17:24,207 DEBUG TRAIN Batch 0/3000 loss 47.248943 loss_att 47.550835 loss_ctc 46.544533 lr 0.00048016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:17:51,985 DEBUG TRAIN Batch 0/3100 loss 198.230103 loss_att 204.226791 loss_ctc 184.237839 lr 0.00049616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:21:14,355 DEBUG TRAIN Batch 0/3200 loss 209.192932 loss_att 221.780136 loss_ctc 179.822800 lr 0.00051216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:24:41,296 DEBUG TRAIN Batch 0/3300 loss 153.609253 loss_att 161.823822 loss_ctc 134.441956 lr 0.00052816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:28:49,902 DEBUG TRAIN Batch 0/3400 loss 220.710205 loss_att 238.362427 loss_ctc 179.521698 lr 0.00054416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:32:12,858 DEBUG TRAIN Batch 0/3500 loss 48.654465 loss_att 48.509796 loss_ctc 48.992031 lr 0.00056016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:32:40,306 DEBUG TRAIN Batch 0/3600 loss 203.909622 loss_att 220.017914 loss_ctc 166.323578 lr 0.00057616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:36:04,407 DEBUG TRAIN Batch 0/3700 loss 208.453583 loss_att 229.981567 loss_ctc 158.221664 lr 0.00059216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:39:24,892 DEBUG TRAIN Batch 0/3800 loss 159.824814 loss_att 171.642334 loss_ctc 132.250580 lr 0.00060816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:42:40,052 DEBUG TRAIN Batch 0/3900 loss 214.458603 loss_att 235.814270 loss_ctc 164.628708 lr 0.00062416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:45:56,769 DEBUG TRAIN Batch 0/4000 loss 39.262115 loss_att 41.490589 loss_ctc 34.062340 lr 0.00064016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:46:25,141 DEBUG TRAIN Batch 0/4100 loss 201.076035 loss_att 220.037689 loss_ctc 156.832184 lr 0.00065616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:49:42,520 DEBUG TRAIN Batch 0/4200 loss 242.249924 loss_att 264.895844 loss_ctc 189.409424 lr 0.00067216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 05:53:01,977 DEBUG TRAIN Batch 0/4300 loss 136.597153 loss_att 145.310303 loss_ctc 116.266464 lr 0.00068816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:06:29,618 DEBUG TRAIN Batch 0/4800 loss 133.458603 loss_att 141.637604 loss_ctc 114.374268 lr 0.00076816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:09:57,933 DEBUG TRAIN Batch 0/4900 loss 198.482498 loss_att 222.161713 loss_ctc 143.231003 lr 0.00078416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:13:19,884 DEBUG TRAIN Batch 0/5000 loss 34.351307 loss_att 35.236332 loss_ctc 32.286247 lr 0.00080016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:13:48,304 DEBUG TRAIN Batch 0/5100 loss 186.564774 loss_att 202.211823 loss_ctc 150.054993 lr 0.00081616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:17:09,047 DEBUG TRAIN Batch 0/5200 loss 197.039597 loss_att 217.087875 loss_ctc 150.260254 lr 0.00083216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:20:32,064 DEBUG TRAIN Batch 0/5300 loss 144.549545 loss_att 154.890564 loss_ctc 120.420517 lr 0.00084816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:23:54,376 DEBUG TRAIN Batch 0/5400 loss 203.734222 loss_att 225.147125 loss_ctc 153.770828 lr 0.00086416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:27:18,770 DEBUG TRAIN Batch 0/5500 loss 36.271667 loss_att 36.709763 loss_ctc 35.249443 lr 0.00088016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:27:46,505 DEBUG TRAIN Batch 0/5600 loss 177.776886 loss_att 198.003159 loss_ctc 130.582260 lr 0.00089616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:31:08,549 DEBUG TRAIN Batch 0/5700 loss 198.098633 loss_att 222.213531 loss_ctc 141.830536 lr 0.00091216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:34:33,766 DEBUG TRAIN Batch 0/5800 loss 138.916687 loss_att 153.659241 loss_ctc 104.517410 lr 0.00092816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:37:56,085 DEBUG TRAIN Batch 0/5900 loss 192.806381 loss_att 213.627991 loss_ctc 144.222626 lr 0.00094416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:54:37,744 DEBUG TRAIN Batch 0/6500 loss 35.240295 loss_att 34.790775 loss_ctc 36.289169 lr 0.00104016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:55:05,551 DEBUG TRAIN Batch 0/6600 loss 167.427765 loss_att 187.477341 loss_ctc 120.645432 lr 0.00105616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 06:58:17,654 DEBUG TRAIN Batch 0/6700 loss 204.961502 loss_att 227.521362 loss_ctc 152.321838 lr 0.00107216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 07:01:29,650 DEBUG TRAIN Batch 0/6800 loss 116.733963 loss_att 123.173920 loss_ctc 101.707397 lr 0.00108816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 07:04:53,009 DEBUG TRAIN Batch 0/6900 loss 188.511017 loss_att 212.494415 loss_ctc 132.549728 lr 0.00110416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 07:08:19,227 DEBUG TRAIN Batch 0/7000 loss 34.409363 loss_att 34.844032 loss_ctc 33.395142 lr 0.00112016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 07:08:47,446 DEBUG TRAIN Batch 0/7100 loss 156.719254 loss_att 175.495300 loss_ctc 112.908470 lr 0.00113616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 07:12:13,969 DEBUG TRAIN Batch 0/7200 loss 177.435410 loss_att 198.865326 loss_ctc 127.432304 lr 0.00115216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 07:15:35,549 DEBUG TRAIN Batch 0/7300 loss 117.573303 loss_att 128.627930 loss_ctc 91.779175 lr 0.00116816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 07:18:58,629 DEBUG TRAIN Batch 0/7400 loss 172.490875 loss_att 191.734543 loss_ctc 127.589012 lr 0.00118416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 07:22:18,627 DEBUG TRAIN Batch 0/7500 loss 34.525120 loss_att 34.615578 loss_ctc 34.314049 lr 0.00120016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 07:22:46,523 DEBUG TRAIN Batch 0/7600 loss 158.027817 loss_att 173.820847 loss_ctc 121.177429 lr 0.00121616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 07:26:08,534 DEBUG TRAIN Batch 0/7700 loss 178.713242 loss_att 197.274445 loss_ctc 135.403809 lr 0.00123216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 07:29:21,466 DEBUG TRAIN Batch 0/7800 loss 93.450302 loss_att 100.167740 loss_ctc 77.776276 lr 0.00124816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 07:55:51,454 DEBUG TRAIN Batch 0/8800 loss 91.358543 loss_att 95.484177 loss_ctc 81.732063 lr 0.00140816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 07:58:53,174 DEBUG TRAIN Batch 0/8900 loss 132.693344 loss_att 142.420212 loss_ctc 109.997314 lr 0.00142416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 08:02:06,793 DEBUG TRAIN Batch 0/9000 loss 23.181480 loss_att 22.948492 loss_ctc 23.725121 lr 0.00144016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 08:02:34,326 DEBUG TRAIN Batch 0/9100 loss 122.407837 loss_att 132.315231 loss_ctc 99.290581 lr 0.00145616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 08:05:57,211 DEBUG TRAIN Batch 0/9200 loss 152.876587 loss_att 162.410339 loss_ctc 130.631195 lr 0.00147216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 08:09:19,830 DEBUG TRAIN Batch 0/9300 loss 85.478348 loss_att 90.107903 loss_ctc 74.676041 lr 0.00148816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 08:12:45,363 DEBUG TRAIN Batch 0/9400 loss 140.668167 loss_att 150.569626 loss_ctc 117.564781 lr 0.00150416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 08:16:06,862 DEBUG TRAIN Batch 0/9500 loss 29.342995 loss_att 27.312916 loss_ctc 34.079845 lr 0.00152016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 08:16:34,512 DEBUG TRAIN Batch 0/9600 loss 113.937523 loss_att 121.065147 loss_ctc 97.306396 lr 0.00153616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 08:19:53,800 DEBUG TRAIN Batch 0/9700 loss 124.672638 loss_att 132.316330 loss_ctc 106.837364 lr 0.00155216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 08:23:16,832 DEBUG TRAIN Batch 0/9800 loss 106.333420 loss_att 107.613609 loss_ctc 103.346306 lr 0.00156816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 09:36:27,137 DEBUG TRAIN Batch 0/12600 loss 140.413620 loss_att 143.784149 loss_ctc 132.549042 lr 0.00201616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 09:55:44,731 DEBUG TRAIN Batch 0/13300 loss 82.092682 loss_att 81.070366 loss_ctc 84.478088 lr 0.00212816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 09:58:52,294 DEBUG TRAIN Batch 0/13400 loss 113.286552 loss_att 111.464523 loss_ctc 117.537964 lr 0.00214416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 10:02:06,715 DEBUG TRAIN Batch 0/13500 loss 22.359081 loss_att 20.908691 loss_ctc 25.743322 lr 0.00216016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 10:02:35,225 DEBUG TRAIN Batch 0/13600 loss 106.368759 loss_att 103.854370 loss_ctc 112.235657 lr 0.00217616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 10:05:55,572 DEBUG TRAIN Batch 0/13700 loss 141.953415 loss_att 143.939804 loss_ctc 137.318527 lr 0.00219216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 10:09:08,012 DEBUG TRAIN Batch 0/13800 loss 81.388443 loss_att 77.255241 loss_ctc 91.032593 lr 0.00220816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 11:47:31,930 DEBUG TRAIN Batch 0/17500 loss 39.807285 loss_att 35.911331 loss_ctc 48.897842 lr 0.00280016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 11:47:59,411 DEBUG TRAIN Batch 0/17600 loss 256.928894 loss_att 226.515427 loss_ctc 327.893646 lr 0.00281616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 11:51:06,184 DEBUG TRAIN Batch 0/17700 loss 266.920807 loss_att 234.972092 loss_ctc 341.467743 lr 0.00283216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 11:54:10,055 DEBUG TRAIN Batch 0/17800 loss 170.849426 loss_att 151.237717 loss_ctc 216.610077 lr 0.00284816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 11:57:14,633 DEBUG TRAIN Batch 0/17900 loss 274.783478 loss_att 244.571960 loss_ctc 345.277069 lr 0.00286416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 12:00:18,047 DEBUG TRAIN Batch 0/18000 loss 45.001965 loss_att 41.405434 loss_ctc 53.393867 lr 0.00288016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 12:00:45,835 DEBUG TRAIN Batch 0/18100 loss 240.481537 loss_att 215.816452 loss_ctc 298.033386 lr 0.00289616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 12:04:01,875 DEBUG TRAIN Batch 0/18200 loss 267.509552 loss_att 237.525620 loss_ctc 337.472046 lr 0.00291216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 12:07:17,656 DEBUG TRAIN Batch 0/18300 loss 183.098328 loss_att 163.135284 loss_ctc 229.678741 lr 0.00292816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 12:10:28,659 DEBUG TRAIN Batch 0/18400 loss 264.885498 loss_att 235.067535 loss_ctc 334.460785 lr 0.00294416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 12:13:41,162 DEBUG TRAIN Batch 0/18500 loss 44.766861 loss_att 40.709595 loss_ctc 54.233807 lr 0.00296016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-30 12:14:08,862 DEBUG TRAIN Batch 0/18600 loss 253.709320 loss_att 228.313843 loss_ctc 312.965454 lr 0.00297616 rank 0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "job_name = est.fit({\"training\":training})\n",
    "#job_name = est.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07b7c9-acde-4d0a-b179-3134d28f6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = est.model_data\n",
    "print(\"Model artifact saved at:\\n\", model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55192e1-23f5-4c2f-8c0f-5d1c43655226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
