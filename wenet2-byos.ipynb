{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c12b63-64f8-4862-8c21-2d5abdd87a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip3 install -U sagemaker\n",
    "!pip3 install -U boto3\n",
    "\n",
    "#then restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecbc5ab-852b-4157-916d-8feb9207b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__:1.12.1+cpu\n",
      "boto3.__version__:1.24.84\n",
      "sagemaker.__version__:2.110.0\n",
      "bucket:sagemaker-us-east-1-348052051973\n",
      "role:arn:aws:iam::348052051973:role/service-role/AmazonSageMakerServiceCatalogProductsExecutionRole\n",
      "CPU times: user 1.84 s, sys: 1.67 s, total: 3.51 s\n",
      "Wall time: 2.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "import torch\n",
    "from sagemaker.utils import unique_name_from_base\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = get_execution_role()\n",
    "prefix = 'wenet'\n",
    "output_path = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "print(\"torch.__version__:{}\".format(torch.__version__))\n",
    "print(\"boto3.__version__:{}\".format(boto3.__version__))\n",
    "print(\"sagemaker.__version__:{}\".format(sagemaker.__version__))\n",
    "print(\"bucket:{}\".format(bucket))\n",
    "print(\"role:{}\".format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad794e-6139-4422-892b-936ccaaa3c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "Copy the wenet/examples/librispeech/s0/*.sh and wenet/examples/librispeech/s0/local to wenet/ as requested by Sagemaker\n",
    "Overwrite the wenet/wenet/bin/train.py with the given one\n",
    "Change the /root/wenet to /opt/ml/input in all data.list files (especially for train_960 and dev)\n",
    "\n",
    "If you are cloning from Github:\n",
    "The \"Librispeech\" in data.list file in Github has the wrong captalization because it's wrong when I upload it. Please change it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "922ce9af-f711-4702-b811-4379efbcc8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.3 ms, sys: 3.78 ms, total: 46.1 ms\n",
      "Wall time: 45.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "hyperparameters = {\n",
    "    'datadir':'/opt/ml/input/data/training',\n",
    "    'stage': '4',\n",
    "    'stop_stage': '5',\n",
    "    'train_config': 'examples/librispeech/s0/conf/train_conformer.yaml',\n",
    "    'model_dir': '/opt/ml/model',\n",
    "}\n",
    "\n",
    "est = PyTorch(\n",
    "    entry_point=\"run.sh\",\n",
    "    source_dir=\"./wenet\",\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=200,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=prefix,\n",
    "    hyperparameters = hyperparameters,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "991f505d-b0c4-4aa1-abae-fd3c3d4534d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-348052051973/wenet/export\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "prefix_dataset = \"wenet/export\"\n",
    "loc =f\"s3://{bucket}/{prefix_dataset}\"\n",
    "print(loc)\n",
    "training = TrainingInput(\n",
    "    s3_data_type='S3Prefix', # Available Options: S3Prefix | ManifestFile | AugmentedManifestFile\n",
    "    s3_data=loc,\n",
    "    distribution='FullyReplicated', # Available Options: FullyReplicated | ShardedByS3Key \n",
    "    input_mode='FastFile'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653e3f8-9feb-449b-9dad-a53b1735e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 15:29:24 Starting - Starting the training job...\n",
      "2022-11-27 15:29:51 Starting - Preparing the instances for training.........\n",
      "2022-11-27 15:31:04 Downloading - Downloading input data...\n",
      "2022-11-27 15:31:45 Training - Downloading the training image.......................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-11-27 15:35:34,125 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-11-27 15:35:34,156 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-11-27 15:35:34,163 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\n",
      "2022-11-27 15:35:31 Training - Training image download completed. Training in progress.\u001b[34m2022-11-27 15:35:46,104 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"datadir\": \"/opt/ml/input/data/training\",\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"stage\": \"4\",\n",
      "        \"stop_stage\": \"5\",\n",
      "        \"train_config\": \"examples/librispeech/s0/conf/train_conformer.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"wenet-2022-11-27-15-27-35-840\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-348052051973/wenet-2022-11-27-15-27-35-840/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"datadir\":\"/opt/ml/input/data/training\",\"model_dir\":\"/opt/ml/model\",\"stage\":\"4\",\"stop_stage\":\"5\",\"train_config\":\"examples/librispeech/s0/conf/train_conformer.yaml\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-348052051973/wenet-2022-11-27-15-27-35-840/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"datadir\":\"/opt/ml/input/data/training\",\"model_dir\":\"/opt/ml/model\",\"stage\":\"4\",\"stop_stage\":\"5\",\"train_config\":\"examples/librispeech/s0/conf/train_conformer.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"wenet-2022-11-27-15-27-35-840\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-348052051973/wenet-2022-11-27-15-27-35-840/source/sourcedir.tar.gz\",\"module_name\":\"run.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--datadir\",\"/opt/ml/input/data/training\",\"--model_dir\",\"/opt/ml/model\",\"--stage\",\"4\",\"--stop_stage\",\"5\",\"--train_config\",\"examples/librispeech/s0/conf/train_conformer.yaml\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATADIR=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_STAGE=4\u001b[0m\n",
      "\u001b[34mSM_HP_STOP_STAGE=5\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_CONFIG=examples/librispeech/s0/conf/train_conformer.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221003-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./run.sh --datadir /opt/ml/input/data/training --model_dir /opt/ml/model --stage 4 --stop_stage 5 --train_config examples/librispeech/s0/conf/train_conformer.yaml\"\u001b[0m\n",
      "\u001b[34m2022-11-27 15:35:46,105 sagemaker-training-toolkit INFO     Sagemaker Debugger is not enabled, debugger exceptions will not be imported.\u001b[0m\n",
      "\u001b[34mdictionary: data/lang_char/train_960_unigram5000_units.txt\u001b[0m\n",
      "\u001b[34m./run.sh: init method is file:///opt/ml/code/exp/sp_spec_aug/ddp_init\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (9.2.0)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (5.4.1)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 44.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece\u001b[0m\n",
      "\u001b[34mSuccessfully installed sentencepiece-0.1.97\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 75.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.50.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 1.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.14.1-py2.py3-none-any.whl (175 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175.4/175.4 kB 21.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.3.0-py3-none-any.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.6/124.6 kB 25.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 46.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (0.37.1)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 89.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (65.4.1)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.1-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 17.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (3.19.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 30.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2022.9.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 28.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tensorboard-plugin-wit, tensorboard-data-server, pyasn1-modules, oauthlib, grpcio, cachetools, absl-py, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.3.0 cachetools-5.2.0 google-auth-2.14.1 google-auth-oauthlib-0.4.6 grpcio-1.50.0 markdown-3.4.1 oauthlib-3.2.2 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting tensorboardX\u001b[0m\n",
      "\u001b[34mDownloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.4/125.4 kB 11.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from tensorboardX) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /opt/conda/lib/python3.8/site-packages (from tensorboardX) (3.19.6)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tensorboardX\u001b[0m\n",
      "\u001b[34mSuccessfully installed tensorboardX-2.5.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting typeguard\u001b[0m\n",
      "\u001b[34mDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: typeguard\u001b[0m\n",
      "\u001b[34mSuccessfully installed typeguard-2.13.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting textgrid\u001b[0m\n",
      "\u001b[34mDownloading TextGrid-1.5-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: textgrid\u001b[0m\n",
      "\u001b[34mSuccessfully installed textgrid-1.5\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting pytest\u001b[0m\n",
      "\u001b[34mDownloading pytest-7.2.0-py3-none-any.whl (316 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.8/316.8 kB 22.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tomli>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading tomli-2.0.1-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.8/site-packages (from pytest) (21.4.0)\u001b[0m\n",
      "\u001b[34mCollecting pluggy<2.0,>=0.12\u001b[0m\n",
      "\u001b[34mDownloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from pytest) (21.3)\u001b[0m\n",
      "\u001b[34mCollecting iniconfig\u001b[0m\n",
      "\u001b[34mDownloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting exceptiongroup>=1.0.0rc8\u001b[0m\n",
      "\u001b[34mDownloading exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->pytest) (3.0.9)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: iniconfig, tomli, pluggy, exceptiongroup, pytest\u001b[0m\n",
      "\u001b[34mSuccessfully installed exceptiongroup-1.0.4 iniconfig-1.1.1 pluggy-1.0.0 pytest-7.2.0 tomli-2.0.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting flake8\u001b[0m\n",
      "\u001b[34mDownloading flake8-6.0.0-py2.py3-none-any.whl (57 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.8/57.8 kB 8.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mccabe<0.8.0,>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyflakes<3.1.0,>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading pyflakes-3.0.1-py2.py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 15.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pycodestyle<2.11.0,>=2.10.0\u001b[0m\n",
      "\u001b[34mDownloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.3/41.3 kB 7.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyflakes, pycodestyle, mccabe, flake8\u001b[0m\n",
      "\u001b[34mSuccessfully installed flake8-6.0.0 mccabe-0.7.0 pycodestyle-2.10.0 pyflakes-3.0.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting flake8-bugbear\u001b[0m\n",
      "\u001b[34mDownloading flake8_bugbear-22.10.27-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.8/site-packages (from flake8-bugbear) (21.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: flake8>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from flake8-bugbear) (6.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyflakes<3.1.0,>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from flake8>=3.0.0->flake8-bugbear) (3.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycodestyle<2.11.0,>=2.10.0 in /opt/conda/lib/python3.8/site-packages (from flake8>=3.0.0->flake8-bugbear) (2.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mccabe<0.8.0,>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from flake8>=3.0.0->flake8-bugbear) (0.7.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: flake8-bugbear\u001b[0m\n",
      "\u001b[34mSuccessfully installed flake8-bugbear-22.10.27\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting flake8-comprehensions\u001b[0m\n",
      "\u001b[34mDownloading flake8_comprehensions-3.10.1-py3-none-any.whl (7.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: flake8!=3.2.0,>=3.0 in /opt/conda/lib/python3.8/site-packages (from flake8-comprehensions) (6.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyflakes<3.1.0,>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from flake8!=3.2.0,>=3.0->flake8-comprehensions) (3.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycodestyle<2.11.0,>=2.10.0 in /opt/conda/lib/python3.8/site-packages (from flake8!=3.2.0,>=3.0->flake8-comprehensions) (2.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mccabe<0.8.0,>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from flake8!=3.2.0,>=3.0->flake8-comprehensions) (0.7.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: flake8-comprehensions\u001b[0m\n",
      "\u001b[34mSuccessfully installed flake8-comprehensions-3.10.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting flake8-executable\u001b[0m\n",
      "\u001b[34mDownloading flake8_executable-2.1.2-py3-none-any.whl (35 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: flake8>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from flake8-executable) (6.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyflakes<3.1.0,>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from flake8>=3.0.0->flake8-executable) (3.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycodestyle<2.11.0,>=2.10.0 in /opt/conda/lib/python3.8/site-packages (from flake8>=3.0.0->flake8-executable) (2.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mccabe<0.8.0,>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from flake8>=3.0.0->flake8-executable) (0.7.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: flake8-executable\u001b[0m\n",
      "\u001b[34mSuccessfully installed flake8-executable-2.1.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mCollecting flake8-pyi\u001b[0m\n",
      "\u001b[34mDownloading flake8_pyi-22.11.0-py37-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyflakes>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from flake8-pyi) (3.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: flake8<7.0.0,>=3.2.1 in /opt/conda/lib/python3.8/site-packages (from flake8-pyi) (6.0.0)\u001b[0m\n",
      "\u001b[34mCollecting ast-decompiler<1.0,>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading ast_decompiler-0.7.0-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycodestyle<2.11.0,>=2.10.0 in /opt/conda/lib/python3.8/site-packages (from flake8<7.0.0,>=3.2.1->flake8-pyi) (2.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mccabe<0.8.0,>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from flake8<7.0.0,>=3.2.1->flake8-pyi) (0.7.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: ast-decompiler, flake8-pyi\u001b[0m\n",
      "\u001b[34mSuccessfully installed ast-decompiler-0.7.0 flake8-pyi-22.11.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mccabe in /opt/conda/lib/python3.8/site-packages (0.7.0)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycodestyle in /opt/conda/lib/python3.8/site-packages (2.10.0)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyflakes in /opt/conda/lib/python3.8/site-packages (3.0.1)\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34m2022-11-27 15:36:24,700 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-27 15:36:24,700 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34mASRModel(\n",
      "  (encoder): ConformerEncoder(\n",
      "    (global_cmvn): GlobalCMVN()\n",
      "    (embed): Conv2dSubsampling4(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (out): Sequential(\n",
      "        (0): Linear(in_features=4864, out_features=256, bias=True)\n",
      "      )\n",
      "      (pos_enc): RelPositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoders): ModuleList(\n",
      "      (0): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (1): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (2): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (3): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (4): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (5): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (6): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (7): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (8): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (9): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (10): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (11): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed): Sequential(\n",
      "      (0): Embedding(5001, 256)\n",
      "      (1): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_layer): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (decoders): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (4): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (5): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ctc): CTC(\n",
      "    (ctc_lo): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (ctc_loss): CTCLoss()\n",
      "  )\n",
      "  (criterion_att): LabelSmoothingLoss(\n",
      "    (criterion): KLDivLoss()\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34m2022-11-27 15:36:33,091 INFO Checkpoint: save to checkpoint /opt/ml/model/init.pt\u001b[0m\n",
      "\u001b[34m2022-11-27 15:36:33,425 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-11-27 15:36:33,427 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-27 15:39:21,725 DEBUG TRAIN Batch 0/0 loss 196.478836 loss_att 73.712662 loss_ctc 482.933228 lr 0.00000016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 15:39:51,274 DEBUG TRAIN Batch 0/100 loss 354.576538 loss_att 319.202332 loss_ctc 437.116333 lr 0.00001616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 15:42:32,523 DEBUG TRAIN Batch 0/200 loss 328.514130 loss_att 309.293457 loss_ctc 373.362396 lr 0.00003216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 15:45:13,159 DEBUG TRAIN Batch 0/300 loss 220.146271 loss_att 210.322311 loss_ctc 243.068848 lr 0.00004816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 15:47:51,796 DEBUG TRAIN Batch 0/400 loss 280.458374 loss_att 268.066711 loss_ctc 309.372223 lr 0.00006416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 15:50:34,849 DEBUG TRAIN Batch 0/500 loss 45.058098 loss_att 43.396587 loss_ctc 48.934948 lr 0.00008016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 15:51:03,561 DEBUG TRAIN Batch 0/600 loss 269.569977 loss_att 256.353821 loss_ctc 300.407623 lr 0.00009616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 15:53:42,834 DEBUG TRAIN Batch 0/700 loss 283.233032 loss_att 267.847046 loss_ctc 319.133606 lr 0.00011216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 15:56:21,879 DEBUG TRAIN Batch 0/800 loss 200.395813 loss_att 188.866196 loss_ctc 227.298248 lr 0.00012816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:24:46,380 DEBUG TRAIN Batch 0/7200 loss 185.951828 loss_att 210.968079 loss_ctc 127.580566 lr 0.00115216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:27:36,756 DEBUG TRAIN Batch 0/7300 loss 122.562340 loss_att 135.092697 loss_ctc 93.324829 lr 0.00116816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:30:29,090 DEBUG TRAIN Batch 0/7400 loss 182.145416 loss_att 202.765625 loss_ctc 134.031647 lr 0.00118416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:33:22,153 DEBUG TRAIN Batch 0/7500 loss 34.953163 loss_att 35.158566 loss_ctc 34.473892 lr 0.00120016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:33:51,180 DEBUG TRAIN Batch 0/7600 loss 163.558594 loss_att 182.366562 loss_ctc 119.673309 lr 0.00121616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:36:46,301 DEBUG TRAIN Batch 0/7700 loss 186.058228 loss_att 204.793427 loss_ctc 142.342804 lr 0.00123216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:39:37,787 DEBUG TRAIN Batch 0/7800 loss 99.501381 loss_att 107.507568 loss_ctc 80.820282 lr 0.00124816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:42:27,906 DEBUG TRAIN Batch 0/7900 loss 157.202469 loss_att 179.091980 loss_ctc 106.126953 lr 0.00126416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:45:18,470 DEBUG TRAIN Batch 0/8000 loss 29.994308 loss_att 29.291702 loss_ctc 31.633728 lr 0.00128016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:45:46,817 DEBUG TRAIN Batch 0/8100 loss 144.388885 loss_att 157.597336 loss_ctc 113.569176 lr 0.00129616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:48:36,074 DEBUG TRAIN Batch 0/8200 loss 157.351654 loss_att 175.615326 loss_ctc 114.736412 lr 0.00131216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:51:21,256 DEBUG TRAIN Batch 0/8300 loss 118.615860 loss_att 125.177826 loss_ctc 103.304619 lr 0.00132816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:54:09,646 DEBUG TRAIN Batch 0/8400 loss 156.356537 loss_att 174.460022 loss_ctc 114.115044 lr 0.00134416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:56:57,922 DEBUG TRAIN Batch 0/8500 loss 35.917332 loss_att 35.839928 loss_ctc 36.097939 lr 0.00136016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 18:57:26,561 DEBUG TRAIN Batch 0/8600 loss 154.885468 loss_att 167.247559 loss_ctc 126.040588 lr 0.00137616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:00:13,772 DEBUG TRAIN Batch 0/8700 loss 166.676315 loss_att 185.573105 loss_ctc 122.583801 lr 0.00139216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:03:06,327 DEBUG TRAIN Batch 0/8800 loss 97.715607 loss_att 104.734344 loss_ctc 81.338547 lr 0.00140816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:06:03,700 DEBUG TRAIN Batch 0/8900 loss 143.716568 loss_att 157.413559 loss_ctc 111.756912 lr 0.00142416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:08:55,927 DEBUG TRAIN Batch 0/9000 loss 26.106768 loss_att 26.756500 loss_ctc 24.590725 lr 0.00144016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:09:24,619 DEBUG TRAIN Batch 0/9100 loss 131.333618 loss_att 144.963135 loss_ctc 99.531403 lr 0.00145616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:12:20,374 DEBUG TRAIN Batch 0/9200 loss 165.687805 loss_att 179.377960 loss_ctc 133.744125 lr 0.00147216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:15:11,363 DEBUG TRAIN Batch 0/9300 loss 95.379890 loss_att 103.420868 loss_ctc 76.617599 lr 0.00148816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:18:02,402 DEBUG TRAIN Batch 0/9400 loss 152.433197 loss_att 167.392227 loss_ctc 117.528770 lr 0.00150416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:20:54,765 DEBUG TRAIN Batch 0/9500 loss 27.989231 loss_att 25.499838 loss_ctc 33.797813 lr 0.00152016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:21:23,432 DEBUG TRAIN Batch 0/9600 loss 126.001900 loss_att 138.744278 loss_ctc 96.269676 lr 0.00153616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:24:13,913 DEBUG TRAIN Batch 0/9700 loss 144.553223 loss_att 159.683990 loss_ctc 109.248077 lr 0.00155216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:27:05,413 DEBUG TRAIN Batch 0/9800 loss 122.666191 loss_att 128.127975 loss_ctc 109.922020 lr 0.00156816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:29:57,511 DEBUG TRAIN Batch 0/9900 loss 142.796051 loss_att 155.109329 loss_ctc 114.065079 lr 0.00158416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:32:49,866 DEBUG TRAIN Batch 0/10000 loss 24.495857 loss_att 21.817984 loss_ctc 30.744228 lr 0.00160016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:33:18,693 DEBUG TRAIN Batch 0/10100 loss 131.184052 loss_att 141.122971 loss_ctc 107.993217 lr 0.00161616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:36:09,505 DEBUG TRAIN Batch 0/10200 loss 177.068375 loss_att 191.217651 loss_ctc 144.053406 lr 0.00163216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:38:59,897 DEBUG TRAIN Batch 0/10300 loss 83.773224 loss_att 88.400902 loss_ctc 72.975304 lr 0.00164816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:41:48,594 DEBUG TRAIN Batch 0/10400 loss 130.809906 loss_att 139.882782 loss_ctc 109.639877 lr 0.00166416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:44:48,019 DEBUG TRAIN Batch 0/10500 loss 22.817766 loss_att 22.021919 loss_ctc 24.674744 lr 0.00168016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:45:16,672 DEBUG TRAIN Batch 0/10600 loss 125.444748 loss_att 133.922684 loss_ctc 105.662895 lr 0.00169616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:48:07,259 DEBUG TRAIN Batch 0/10700 loss 159.231171 loss_att 170.849899 loss_ctc 132.120789 lr 0.00171216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:50:53,217 DEBUG TRAIN Batch 0/10800 loss 105.704498 loss_att 111.409424 loss_ctc 92.393005 lr 0.00172816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:53:42,662 DEBUG TRAIN Batch 0/10900 loss 151.743225 loss_att 163.255615 loss_ctc 124.880997 lr 0.00174416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:56:29,024 DEBUG TRAIN Batch 0/11000 loss 25.482162 loss_att 24.081402 loss_ctc 28.750607 lr 0.00176016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:56:57,580 DEBUG TRAIN Batch 0/11100 loss 112.857727 loss_att 121.859520 loss_ctc 91.853546 lr 0.00177616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 19:59:38,809 DEBUG TRAIN Batch 0/11200 loss 140.454742 loss_att 150.894669 loss_ctc 116.094910 lr 0.00179216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:02:33,979 DEBUG TRAIN Batch 0/11300 loss 94.562874 loss_att 99.181870 loss_ctc 83.785225 lr 0.00180816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:05:27,991 DEBUG TRAIN Batch 0/11400 loss 151.848770 loss_att 160.894104 loss_ctc 130.743011 lr 0.00182416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:08:20,674 DEBUG TRAIN Batch 0/11500 loss 28.833241 loss_att 27.899612 loss_ctc 31.011703 lr 0.00184016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:08:49,206 DEBUG TRAIN Batch 0/11600 loss 140.608643 loss_att 147.562286 loss_ctc 124.383469 lr 0.00185616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:11:39,555 DEBUG TRAIN Batch 0/11700 loss 137.544495 loss_att 150.833176 loss_ctc 106.537590 lr 0.00187216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:14:35,121 DEBUG TRAIN Batch 0/11800 loss 86.611801 loss_att 90.729515 loss_ctc 77.003815 lr 0.00188816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:17:29,723 DEBUG TRAIN Batch 0/11900 loss 149.400314 loss_att 159.645203 loss_ctc 125.495590 lr 0.00190416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:20:23,684 DEBUG TRAIN Batch 0/12000 loss 32.102509 loss_att 30.780386 loss_ctc 35.187454 lr 0.00192016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:20:52,534 DEBUG TRAIN Batch 0/12100 loss 141.454025 loss_att 147.696472 loss_ctc 126.888306 lr 0.00193616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:23:50,857 DEBUG TRAIN Batch 0/12200 loss 131.375534 loss_att 140.366333 loss_ctc 110.396988 lr 0.00195216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:26:41,946 DEBUG TRAIN Batch 0/12300 loss 84.322525 loss_att 84.137367 loss_ctc 84.754555 lr 0.00196816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:29:36,486 DEBUG TRAIN Batch 0/12400 loss 139.347214 loss_att 148.254456 loss_ctc 118.563660 lr 0.00198416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:32:30,829 DEBUG TRAIN Batch 0/12500 loss 23.991112 loss_att 22.234890 loss_ctc 28.088963 lr 0.00200016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-27 20:32:59,344 DEBUG TRAIN Batch 0/12600 loss 148.265106 loss_att 154.902191 loss_ctc 132.778595 lr 0.00201616 rank 0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "job_name = est.fit({\"training\":training})\n",
    "#job_name = est.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07b7c9-acde-4d0a-b179-3134d28f6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = est.model_data\n",
    "print(\"Model artifact saved at:\\n\", model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60519dfe-3653-4321-a6ab-b125cb1d8e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchaudio==0.10.0\n",
      "Pillow\n",
      "pyyaml>=5.1\n",
      "sentencepiece\n",
      "tensorboard\n",
      "tensorboardX\n",
      "typeguard\n",
      "textgrid\n",
      "pytest\n",
      "flake8==3.8.2\n",
      "flake8-bugbear\n",
      "flake8-comprehensions\n",
      "flake8-executable\n",
      "flake8-pyi==20.5.0\n",
      "mccabe\n",
      "pycodestyle==2.6.0\n",
      "pyflakes==2.2.0"
     ]
    }
   ],
   "source": [
    "!cat wenet/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55192e1-23f5-4c2f-8c0f-5d1c43655226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.12-cpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
