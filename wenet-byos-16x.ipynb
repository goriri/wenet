{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fecbc5ab-852b-4157-916d-8feb9207b7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3.__version__:1.26.96\n",
      "sagemaker.__version__:2.140.1\n",
      "bucket:sagemaker-us-east-1-348052051973\n",
      "role:arn:aws:iam::348052051973:role/service-role/AmazonSageMakerServiceCatalogProductsExecutionRole\n",
      "CPU times: user 394 ms, sys: 59 ms, total: 453 ms\n",
      "Wall time: 871 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.utils import unique_name_from_base\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = get_execution_role()\n",
    "prefix = 'wenet220'\n",
    "output_path = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "print(\"boto3.__version__:{}\".format(boto3.__version__))\n",
    "print(\"sagemaker.__version__:{}\".format(sagemaker.__version__))\n",
    "print(\"bucket:{}\".format(bucket))\n",
    "print(\"role:{}\".format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aad794e-6139-4422-892b-936ccaaa3c39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Change the /root/wenet to /opt/ml/input in all data.list files (especially for train_960 and dev)\n",
       "The \"Librispeech\" in data.list file in Github has the wrong captalization because it's wrong when I upload it. Please change it yourself!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "Change the /root/wenet to /opt/ml/input in all data.list files (especially for train_960 and dev)\n",
    "The \"Librispeech\" in data.list file in Github has the wrong captalization because it's wrong when I upload it. Please change it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "991f505d-b0c4-4aa1-abae-fd3c3d4534d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "prefix_dataset = \"wenet220/export\"\n",
    "loc =f\"s3://{bucket}/{prefix_dataset}\"\n",
    "\n",
    "training = TrainingInput(\n",
    "    s3_data_type='S3Prefix', # Available Options: S3Prefix | ManifestFile | AugmentedManifestFile\n",
    "    s3_data=loc,\n",
    "    distribution='FullyReplicated', # Available Options: FullyReplicated | ShardedByS3Key \n",
    "    input_mode='FastFile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "922ce9af-f711-4702-b811-4379efbcc8fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "# instance_type='local'\n",
    "\n",
    "max_run = 432000\n",
    "checkpoint_s3_uri = f\"s3://{bucket}/{prefix}/checkpoints\"\n",
    "\n",
    "hyperparameters = {\n",
    "    'datadir':'/opt/ml/input/data/training',\n",
    "    'stage': '4',\n",
    "    'stop_stage': '4',\n",
    "    'train_config': 'conf/train_conformer.yaml',\n",
    "    'model_dir': '/opt/ml/model',\n",
    "    'checkpoint_dir': '/opt/ml/checkpoints',\n",
    "    'output_dir': '/opt/ml/output/data',\n",
    "}\n",
    "\n",
    "est = PyTorch(\n",
    "    entry_point=\"run-librispeech.sh\",\n",
    "    source_dir=\"/root/wenet-source/wenet\",\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=200,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=prefix,\n",
    "    hyperparameters = hyperparameters,\n",
    "    checkpoint_s3_uri = checkpoint_s3_uri,\n",
    "    output_path = f\"s3://{bucket}/{prefix}/\",\n",
    "    # keep_alive_period_in_seconds=1800,\n",
    "    max_run = max_run,\n",
    "    tags = [{\"Key\": \"team\", \"Value\": \"asr\"}, {\"Key\": \"person\", \"Value\": \"andrew\"}, {\"Key\": \"project\", \"Value\": \"abc\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653e3f8-9feb-449b-9dad-a53b1735e33d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: wenet220-2023-03-29-03-16-01-482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-29 03:17:23 Starting - Starting the training job...\n",
      "2023-03-29 03:18:00 Starting - Preparing the instances for training.........\n",
      "2023-03-29 03:19:30 Downloading - Downloading input data......\n",
      "2023-03-29 03:20:21 Training - Downloading the training image..................\n",
      "2023-03-29 03:23:27 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:13,504 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:13,566 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:13,575 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:13,577 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:19,523 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:19,596 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:19,666 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:19,675 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"checkpoint_dir\": \"/opt/ml/checkpoints\",\n",
      "        \"datadir\": \"/opt/ml/input/data/training\",\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"output_dir\": \"/opt/ml/output/data\",\n",
      "        \"stage\": \"4\",\n",
      "        \"stop_stage\": \"4\",\n",
      "        \"train_config\": \"conf/train_conformer.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"wenet220-2023-03-29-03-16-01-482\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-348052051973/wenet220-2023-03-29-03-16-01-482/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run-librispeech.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run-librispeech.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"datadir\":\"/opt/ml/input/data/training\",\"model_dir\":\"/opt/ml/model\",\"output_dir\":\"/opt/ml/output/data\",\"stage\":\"4\",\"stop_stage\":\"4\",\"train_config\":\"conf/train_conformer.yaml\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run-librispeech.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run-librispeech.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-348052051973/wenet220-2023-03-29-03-16-01-482/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"datadir\":\"/opt/ml/input/data/training\",\"model_dir\":\"/opt/ml/model\",\"output_dir\":\"/opt/ml/output/data\",\"stage\":\"4\",\"stop_stage\":\"4\",\"train_config\":\"conf/train_conformer.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"wenet220-2023-03-29-03-16-01-482\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-348052051973/wenet220-2023-03-29-03-16-01-482/source/sourcedir.tar.gz\",\"module_name\":\"run-librispeech.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run-librispeech.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--checkpoint_dir\",\"/opt/ml/checkpoints\",\"--datadir\",\"/opt/ml/input/data/training\",\"--model_dir\",\"/opt/ml/model\",\"--output_dir\",\"/opt/ml/output/data\",\"--stage\",\"4\",\"--stop_stage\",\"4\",\"--train_config\",\"conf/train_conformer.yaml\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINT_DIR=/opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mSM_HP_DATADIR=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_HP_STAGE=4\u001b[0m\n",
      "\u001b[34mSM_HP_STOP_STAGE=4\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_CONFIG=conf/train_conformer.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./run-librispeech.sh --checkpoint_dir /opt/ml/checkpoints --datadir /opt/ml/input/data/training --model_dir /opt/ml/model --output_dir /opt/ml/output/data --stage 4 --stop_stage 4 --train_config conf/train_conformer.yaml\"\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:20,649 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 33.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 106.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboardX\u001b[0m\n",
      "\u001b[34mDownloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 24.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting typeguard==2.13.3\u001b[0m\n",
      "\u001b[34mDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting textgrid\u001b[0m\n",
      "\u001b[34mDownloading TextGrid-1.5-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytest\u001b[0m\n",
      "\u001b[34mDownloading pytest-7.2.2-py3-none-any.whl (317 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.2/317.2 kB 51.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting flake8==3.8.2\u001b[0m\n",
      "\u001b[34mDownloading flake8-3.8.2-py2.py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.7/72.7 kB 21.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting flake8-bugbear\u001b[0m\n",
      "\u001b[34mDownloading flake8_bugbear-23.3.23-py3-none-any.whl (28 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-comprehensions\u001b[0m\n",
      "\u001b[34mDownloading flake8_comprehensions-3.11.1-py3-none-any.whl (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-executable\u001b[0m\n",
      "\u001b[34mDownloading flake8_executable-2.1.3-py3-none-any.whl (35 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-pyi==20.5.0\u001b[0m\n",
      "\u001b[34mDownloading flake8_pyi-20.5.0-py36-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting mccabe\u001b[0m\n",
      "\u001b[34mDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pycodestyle==2.6.0\u001b[0m\n",
      "\u001b[34mDownloading pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.4/41.4 kB 11.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyflakes==2.2.0\u001b[0m\n",
      "\u001b[34mDownloading pyflakes-2.2.0-py2.py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 19.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mccabe\u001b[0m\n",
      "\u001b[34mDownloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs in /opt/conda/lib/python3.8/site-packages (from flake8-pyi==20.5.0->-r requirements.txt (line 13)) (22.2.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 85.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (0.38.4)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/5.0 MB 115.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 33.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 96.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (2.2.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.3-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.9/93.9 kB 24.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (1.22.2)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.17.0-py2.py3-none-any.whl (178 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 178.1/178.1 kB 39.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from tensorboardX->-r requirements.txt (line 5)) (23.0)\u001b[0m\n",
      "\u001b[34mCollecting tomli>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading tomli-2.0.1-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting iniconfig\u001b[0m\n",
      "\u001b[34mDownloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.8/site-packages (from pytest->-r requirements.txt (line 8)) (1.0.0)\u001b[0m\n",
      "\u001b[34mCollecting exceptiongroup>=1.0.0rc8\u001b[0m\n",
      "\u001b[34mDownloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-bugbear\u001b[0m\n",
      "\u001b[34mDownloading flake8_bugbear-23.3.12-py3-none-any.whl (28 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 28.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 4)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 4)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 4)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 39.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: textgrid, tensorboard-plugin-wit, sentencepiece, mccabe, typeguard, tomli, tensorboardX, tensorboard-data-server, pyflakes, pycodestyle, pyasn1-modules, oauthlib, iniconfig, grpcio, exceptiongroup, cachetools, absl-py, requests-oauthlib, pytest, markdown, google-auth, flake8, google-auth-oauthlib, flake8-pyi, flake8-executable, flake8-comprehensions, flake8-bugbear, tensorboard\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.4.0 cachetools-5.3.0 exceptiongroup-1.1.1 flake8-3.8.2 flake8-bugbear-23.3.12 flake8-comprehensions-3.11.1 flake8-executable-2.1.3 flake8-pyi-20.5.0 google-auth-2.17.0 google-auth-oauthlib-0.4.6 grpcio-1.53.0 iniconfig-2.0.0 markdown-3.4.3 mccabe-0.6.1 oauthlib-3.2.2 pyasn1-modules-0.2.8 pycodestyle-2.6.0 pyflakes-2.2.0 pytest-7.2.2 requests-oauthlib-1.3.1 sentencepiece-0.1.97 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 tensorboardX-2.6 textgrid-1.5 tomli-2.0.1 typeguard-2.13.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mdictionary: data/lang_char/train_960_unigram5000_units.txt\u001b[0m\n",
      "\u001b[34mrun.sh: init method is file:///opt/ml/code/examples/librispeech/s0/exp/sp_spec_aug/ddp_init\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,772 INFO training on multiple gpus, this gpu 7\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,773 INFO training on multiple gpus, this gpu 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,786 INFO training on multiple gpus, this gpu 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,786 INFO training on multiple gpus, this gpu 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,794 INFO training on multiple gpus, this gpu 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,795 INFO training on multiple gpus, this gpu 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,796 INFO training on multiple gpus, this gpu 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,805 INFO training on multiple gpus, this gpu 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,866 INFO Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,876 INFO Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,887 INFO Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,898 INFO Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,908 INFO Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,918 INFO Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,928 INFO Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,928 INFO Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,928 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,928 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,929 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,929 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,929 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,938 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,938 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,938 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,283 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,283 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,283 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,283 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,730 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,731 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,792 INFO Checkpoint: save to checkpoint /opt/ml/checkpoints/init.pt\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,793 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,794 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,795 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,796 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,888 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,889 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,893 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,894 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,894 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,895 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,897 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,898 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.120 algo-1:217 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:33,121 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:33,122 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.183 algo-1:218 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.202 algo-1:214 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.275 algo-1:215 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.278 algo-1:216 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.287 algo-1:219 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.311 algo-1:213 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.498 algo-1:217 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.550 algo-1:212 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.564 algo-1:218 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.603 algo-1:214 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.666 algo-1:216 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.666 algo-1:215 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.683 algo-1:219 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.768 algo-1:213 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.927 algo-1:212 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,308 DEBUG TRAIN Batch 0/0 loss 204.432053 loss_att 81.260284 loss_ctc 491.832825 lr 0.00000016 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,310 DEBUG TRAIN Batch 0/0 loss 194.463852 loss_att 80.563660 loss_ctc 460.230927 lr 0.00000016 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,311 DEBUG TRAIN Batch 0/0 loss 181.749664 loss_att 73.846573 loss_ctc 433.523560 lr 0.00000016 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,312 DEBUG TRAIN Batch 0/0 loss 181.995453 loss_att 68.416588 loss_ctc 447.012787 lr 0.00000016 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,315 DEBUG TRAIN Batch 0/0 loss 162.911316 loss_att 56.654522 loss_ctc 410.843842 lr 0.00000016 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,317 DEBUG TRAIN Batch 0/0 loss 196.423889 loss_att 71.998436 loss_ctc 486.749939 lr 0.00000016 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,318 DEBUG TRAIN Batch 0/0 loss 188.555679 loss_att 67.069550 loss_ctc 472.023285 lr 0.00000016 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,324 DEBUG TRAIN Batch 0/0 loss 174.466858 loss_att 59.583115 loss_ctc 442.528900 lr 0.00000016 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,061 DEBUG TRAIN Batch 0/100 loss 401.154968 loss_att 366.788574 loss_ctc 481.343262 lr 0.00001616 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,062 DEBUG TRAIN Batch 0/100 loss 381.153015 loss_att 346.797546 loss_ctc 461.315674 lr 0.00001616 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,062 DEBUG TRAIN Batch 0/100 loss 402.054413 loss_att 369.423065 loss_ctc 478.194275 lr 0.00001616 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,069 DEBUG TRAIN Batch 0/100 loss 366.122437 loss_att 332.458496 loss_ctc 444.671631 lr 0.00001616 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,069 DEBUG TRAIN Batch 0/100 loss 380.871521 loss_att 346.682190 loss_ctc 460.646576 lr 0.00001616 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,070 DEBUG TRAIN Batch 0/100 loss 405.917328 loss_att 371.632629 loss_ctc 485.914978 lr 0.00001616 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,071 DEBUG TRAIN Batch 0/100 loss 401.492462 loss_att 367.003113 loss_ctc 481.967651 lr 0.00001616 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,074 DEBUG TRAIN Batch 0/100 loss 385.354675 loss_att 351.132507 loss_ctc 465.206390 lr 0.00001616 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,529 DEBUG TRAIN Batch 0/200 loss 342.505493 loss_att 320.458740 loss_ctc 393.947937 lr 0.00003216 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,530 DEBUG TRAIN Batch 0/200 loss 327.755890 loss_att 307.206360 loss_ctc 375.704803 lr 0.00003216 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,530 DEBUG TRAIN Batch 0/200 loss 340.735901 loss_att 319.379425 loss_ctc 390.567627 lr 0.00003216 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,534 DEBUG TRAIN Batch 0/200 loss 337.787292 loss_att 316.337616 loss_ctc 387.836548 lr 0.00003216 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,537 DEBUG TRAIN Batch 0/200 loss 314.421143 loss_att 294.174805 loss_ctc 361.662598 lr 0.00003216 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,539 DEBUG TRAIN Batch 0/200 loss 349.083435 loss_att 326.384796 loss_ctc 402.046844 lr 0.00003216 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,539 DEBUG TRAIN Batch 0/200 loss 333.304077 loss_att 312.207489 loss_ctc 382.529388 lr 0.00003216 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,540 DEBUG TRAIN Batch 0/200 loss 313.206909 loss_att 292.456665 loss_ctc 361.624115 lr 0.00003216 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,792 DEBUG TRAIN Batch 0/300 loss 212.184799 loss_att 202.527954 loss_ctc 234.717453 lr 0.00004816 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,794 DEBUG TRAIN Batch 0/300 loss 200.814972 loss_att 191.825485 loss_ctc 221.790451 lr 0.00004816 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,795 DEBUG TRAIN Batch 0/300 loss 210.030151 loss_att 200.721848 loss_ctc 231.749512 lr 0.00004816 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,795 DEBUG TRAIN Batch 0/300 loss 197.888428 loss_att 188.946869 loss_ctc 218.752106 lr 0.00004816 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,796 DEBUG TRAIN Batch 0/300 loss 211.922333 loss_att 202.549347 loss_ctc 233.792587 lr 0.00004816 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,799 DEBUG TRAIN Batch 0/300 loss 215.290100 loss_att 205.810791 loss_ctc 237.408493 lr 0.00004816 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,804 DEBUG TRAIN Batch 0/300 loss 220.808731 loss_att 210.459335 loss_ctc 244.957321 lr 0.00004816 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,805 DEBUG TRAIN Batch 0/300 loss 207.383881 loss_att 197.987350 loss_ctc 229.309128 lr 0.00004816 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:37:44,889 DEBUG TRAIN Batch 0/400 loss 281.756958 loss_att 267.301208 loss_ctc 315.487030 lr 0.00006416 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:37:44,889 DEBUG TRAIN Batch 0/400 loss 298.410156 loss_att 283.273682 loss_ctc 333.728516 lr 0.00006416 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:37:44,891 DEBUG TRAIN Batch 0/400 loss 298.762207 loss_att 282.927124 loss_ctc 335.710754 lr 0.00006416 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:37:44,894 DEBUG TRAIN Batch 0/400 loss 287.900787 loss_att 273.438171 loss_ctc 321.646881 lr 0.00006416 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:37:44,896 DEBUG TRAIN Batch 0/400 loss 283.257355 loss_att 268.799774 loss_ctc 316.991730 lr 0.00006416 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:37:44,896 DEBUG TRAIN Batch 0/400 loss 322.891815 loss_att 306.932434 loss_ctc 360.130310 lr 0.00006416 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:37:44,897 DEBUG TRAIN Batch 0/400 loss 302.488434 loss_att 286.992737 loss_ctc 338.645081 lr 0.00006416 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:37:44,898 DEBUG TRAIN Batch 0/400 loss 265.125122 loss_att 251.756073 loss_ctc 296.319550 lr 0.00006416 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:40:46,853 DEBUG TRAIN Batch 0/500 loss 47.924553 loss_att 45.989410 loss_ctc 52.439892 lr 0.00008016 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:40:46,855 DEBUG TRAIN Batch 0/500 loss 59.117188 loss_att 56.357292 loss_ctc 65.556946 lr 0.00008016 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:40:46,856 DEBUG TRAIN Batch 0/500 loss 47.573822 loss_att 45.367611 loss_ctc 52.721642 lr 0.00008016 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:40:46,858 DEBUG TRAIN Batch 0/500 loss 60.580498 loss_att 57.840240 loss_ctc 66.974426 lr 0.00008016 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:40:46,864 DEBUG TRAIN Batch 0/500 loss 54.423794 loss_att 51.994057 loss_ctc 60.093178 lr 0.00008016 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:40:46,865 DEBUG TRAIN Batch 0/500 loss 59.186050 loss_att 56.585842 loss_ctc 65.253204 lr 0.00008016 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:40:46,878 DEBUG TRAIN Batch 0/500 loss 53.573074 loss_att 50.849895 loss_ctc 59.927155 lr 0.00008016 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:40:46,891 DEBUG TRAIN Batch 0/500 loss 64.236084 loss_att 61.388336 loss_ctc 70.880829 lr 0.00008016 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:41:20,743 DEBUG TRAIN Batch 0/600 loss 253.811722 loss_att 239.739258 loss_ctc 286.647491 lr 0.00009616 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:41:20,746 DEBUG TRAIN Batch 0/600 loss 245.558746 loss_att 231.370773 loss_ctc 278.664032 lr 0.00009616 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:41:20,750 DEBUG TRAIN Batch 0/600 loss 253.883148 loss_att 239.145630 loss_ctc 288.270721 lr 0.00009616 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:41:20,752 DEBUG TRAIN Batch 0/600 loss 255.975983 loss_att 239.993866 loss_ctc 293.267578 lr 0.00009616 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:41:20,753 DEBUG TRAIN Batch 0/600 loss 236.700729 loss_att 223.002228 loss_ctc 268.663879 lr 0.00009616 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:41:20,753 DEBUG TRAIN Batch 0/600 loss 270.079834 loss_att 253.361496 loss_ctc 309.089355 lr 0.00009616 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:41:20,754 DEBUG TRAIN Batch 0/600 loss 248.792160 loss_att 234.318878 loss_ctc 282.563171 lr 0.00009616 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:41:20,756 DEBUG TRAIN Batch 0/600 loss 261.427429 loss_att 245.880844 loss_ctc 297.702789 lr 0.00009616 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:44:22,257 DEBUG TRAIN Batch 0/700 loss 268.876587 loss_att 253.172119 loss_ctc 305.520355 lr 0.00011216 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:44:22,257 DEBUG TRAIN Batch 0/700 loss 304.910889 loss_att 288.941010 loss_ctc 342.173920 lr 0.00011216 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:44:22,258 DEBUG TRAIN Batch 0/700 loss 288.274963 loss_att 273.412933 loss_ctc 322.952972 lr 0.00011216 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:44:22,258 DEBUG TRAIN Batch 0/700 loss 300.699249 loss_att 284.727417 loss_ctc 337.966888 lr 0.00011216 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:44:22,259 DEBUG TRAIN Batch 0/700 loss 283.432648 loss_att 266.856537 loss_ctc 322.110229 lr 0.00011216 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:44:22,259 DEBUG TRAIN Batch 0/700 loss 277.392548 loss_att 262.090210 loss_ctc 313.097961 lr 0.00011216 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:44:22,260 DEBUG TRAIN Batch 0/700 loss 285.809052 loss_att 270.636780 loss_ctc 321.211060 lr 0.00011216 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:44:22,264 DEBUG TRAIN Batch 0/700 loss 275.903534 loss_att 260.645386 loss_ctc 311.505859 lr 0.00011216 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:47:26,201 DEBUG TRAIN Batch 0/800 loss 192.350464 loss_att 185.638092 loss_ctc 208.012695 lr 0.00012816 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:47:26,203 DEBUG TRAIN Batch 0/800 loss 194.036438 loss_att 185.521179 loss_ctc 213.905380 lr 0.00012816 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:47:26,204 DEBUG TRAIN Batch 0/800 loss 204.138123 loss_att 194.860229 loss_ctc 225.786545 lr 0.00012816 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:47:26,204 DEBUG TRAIN Batch 0/800 loss 205.252258 loss_att 195.783737 loss_ctc 227.345505 lr 0.00012816 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:47:26,208 DEBUG TRAIN Batch 0/800 loss 185.744019 loss_att 177.771484 loss_ctc 204.346558 lr 0.00012816 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:47:26,209 DEBUG TRAIN Batch 0/800 loss 208.904800 loss_att 198.639282 loss_ctc 232.857651 lr 0.00012816 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:47:26,212 DEBUG TRAIN Batch 0/800 loss 191.522675 loss_att 183.586777 loss_ctc 210.039795 lr 0.00012816 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:47:26,234 DEBUG TRAIN Batch 0/800 loss 209.770203 loss_att 201.700012 loss_ctc 228.600632 lr 0.00012816 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:50:26,861 DEBUG TRAIN Batch 0/900 loss 281.175385 loss_att 277.686584 loss_ctc 289.315918 lr 0.00014416 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:50:26,862 DEBUG TRAIN Batch 0/900 loss 251.195190 loss_att 248.413177 loss_ctc 257.686554 lr 0.00014416 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:50:26,863 DEBUG TRAIN Batch 0/900 loss 238.393112 loss_att 237.569214 loss_ctc 240.315536 lr 0.00014416 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:50:26,868 DEBUG TRAIN Batch 0/900 loss 270.856628 loss_att 268.922791 loss_ctc 275.368835 lr 0.00014416 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:50:26,869 DEBUG TRAIN Batch 0/900 loss 257.874695 loss_att 255.438904 loss_ctc 263.558167 lr 0.00014416 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:50:26,870 DEBUG TRAIN Batch 0/900 loss 246.016235 loss_att 245.369232 loss_ctc 247.525940 lr 0.00014416 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:50:26,872 DEBUG TRAIN Batch 0/900 loss 293.068787 loss_att 288.608887 loss_ctc 303.475189 lr 0.00014416 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:50:26,873 DEBUG TRAIN Batch 0/900 loss 262.076965 loss_att 260.398315 loss_ctc 265.993774 lr 0.00014416 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:53:28,985 DEBUG TRAIN Batch 0/1000 loss 45.794754 loss_att 44.800842 loss_ctc 48.113884 lr 0.00016016 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:53:28,985 DEBUG TRAIN Batch 0/1000 loss 36.930779 loss_att 37.504143 loss_ctc 35.592922 lr 0.00016016 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:53:28,986 DEBUG TRAIN Batch 0/1000 loss 42.875904 loss_att 42.537933 loss_ctc 43.664505 lr 0.00016016 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:53:28,987 DEBUG TRAIN Batch 0/1000 loss 43.963303 loss_att 43.694191 loss_ctc 44.591232 lr 0.00016016 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:53:28,989 DEBUG TRAIN Batch 0/1000 loss 40.771362 loss_att 41.154572 loss_ctc 39.877209 lr 0.00016016 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:53:29,000 DEBUG TRAIN Batch 0/1000 loss 39.619354 loss_att 39.554928 loss_ctc 39.769680 lr 0.00016016 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:53:29,016 DEBUG TRAIN Batch 0/1000 loss 48.878883 loss_att 48.412304 loss_ctc 49.967567 lr 0.00016016 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:53:29,022 DEBUG TRAIN Batch 0/1000 loss 40.077583 loss_att 40.572800 loss_ctc 38.922077 lr 0.00016016 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:54:03,159 DEBUG TRAIN Batch 0/1100 loss 213.498825 loss_att 220.223679 loss_ctc 197.807495 lr 0.00017616 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:54:03,160 DEBUG TRAIN Batch 0/1100 loss 217.043854 loss_att 227.404099 loss_ctc 192.869965 lr 0.00017616 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:54:03,161 DEBUG TRAIN Batch 0/1100 loss 216.708771 loss_att 225.470428 loss_ctc 196.264893 lr 0.00017616 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:54:03,161 DEBUG TRAIN Batch 0/1100 loss 213.390320 loss_att 220.376511 loss_ctc 197.089218 lr 0.00017616 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:54:03,161 DEBUG TRAIN Batch 0/1100 loss 200.413437 loss_att 209.856201 loss_ctc 178.380341 lr 0.00017616 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:54:03,168 DEBUG TRAIN Batch 0/1100 loss 222.851166 loss_att 232.410934 loss_ctc 200.545044 lr 0.00017616 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:54:03,189 DEBUG TRAIN Batch 0/1100 loss 221.592926 loss_att 230.268906 loss_ctc 201.348984 lr 0.00017616 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:54:03,197 DEBUG TRAIN Batch 0/1100 loss 227.937714 loss_att 233.270920 loss_ctc 215.493530 lr 0.00017616 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:56:59,198 DEBUG TRAIN Batch 0/1200 loss 239.661606 loss_att 255.361801 loss_ctc 203.027802 lr 0.00019216 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:56:59,201 DEBUG TRAIN Batch 0/1200 loss 233.888657 loss_att 249.858795 loss_ctc 196.625000 lr 0.00019216 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:56:59,202 DEBUG TRAIN Batch 0/1200 loss 231.337036 loss_att 246.384399 loss_ctc 196.226532 lr 0.00019216 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:56:59,202 DEBUG TRAIN Batch 0/1200 loss 229.603973 loss_att 242.965912 loss_ctc 198.426086 lr 0.00019216 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:56:59,208 DEBUG TRAIN Batch 0/1200 loss 232.180862 loss_att 248.708984 loss_ctc 193.615265 lr 0.00019216 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:56:59,209 DEBUG TRAIN Batch 0/1200 loss 233.860245 loss_att 249.775009 loss_ctc 196.725800 lr 0.00019216 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:56:59,211 DEBUG TRAIN Batch 0/1200 loss 220.869690 loss_att 238.674622 loss_ctc 179.324829 lr 0.00019216 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:56:59,212 DEBUG TRAIN Batch 0/1200 loss 259.368652 loss_att 273.868896 loss_ctc 225.534790 lr 0.00019216 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:59:56,471 DEBUG TRAIN Batch 0/1300 loss 158.699417 loss_att 172.195251 loss_ctc 127.209129 lr 0.00020816 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:59:56,471 DEBUG TRAIN Batch 0/1300 loss 161.478470 loss_att 169.361877 loss_ctc 143.083832 lr 0.00020816 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:59:56,472 DEBUG TRAIN Batch 0/1300 loss 161.406235 loss_att 171.201569 loss_ctc 138.550476 lr 0.00020816 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:59:56,473 DEBUG TRAIN Batch 0/1300 loss 168.128601 loss_att 177.245285 loss_ctc 146.856323 lr 0.00020816 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:59:56,474 DEBUG TRAIN Batch 0/1300 loss 160.515442 loss_att 170.271149 loss_ctc 137.752106 lr 0.00020816 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:59:56,476 DEBUG TRAIN Batch 0/1300 loss 163.149597 loss_att 174.122849 loss_ctc 137.545349 lr 0.00020816 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:59:56,479 DEBUG TRAIN Batch 0/1300 loss 184.364182 loss_att 196.291260 loss_ctc 156.534332 lr 0.00020816 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:59:56,480 DEBUG TRAIN Batch 0/1300 loss 174.998993 loss_att 184.616989 loss_ctc 152.557007 lr 0.00020816 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 00:59:38,456 DEBUG TRAIN Batch 0/3700 loss 85.202576 loss_att 83.959015 loss_ctc 88.104210 lr 0.00059216 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 00:59:38,460 DEBUG TRAIN Batch 0/3700 loss 87.487259 loss_att 86.513245 loss_ctc 89.759972 lr 0.00059216 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 00:59:38,460 DEBUG TRAIN Batch 0/3700 loss 84.400009 loss_att 84.135315 loss_ctc 85.017624 lr 0.00059216 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 00:59:38,465 DEBUG TRAIN Batch 0/3700 loss 91.812248 loss_att 93.389732 loss_ctc 88.131454 lr 0.00059216 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 00:59:38,465 DEBUG TRAIN Batch 0/3700 loss 84.958221 loss_att 85.660400 loss_ctc 83.319794 lr 0.00059216 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 00:59:38,467 DEBUG TRAIN Batch 0/3700 loss 82.667152 loss_att 81.444778 loss_ctc 85.519348 lr 0.00059216 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 00:59:38,468 DEBUG TRAIN Batch 0/3700 loss 70.308243 loss_att 69.821220 loss_ctc 71.444626 lr 0.00059216 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 00:59:38,471 DEBUG TRAIN Batch 0/3700 loss 88.399139 loss_att 88.297379 loss_ctc 88.636581 lr 0.00059216 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:02:29,381 DEBUG TRAIN Batch 0/3800 loss 68.289307 loss_att 66.162773 loss_ctc 73.251228 lr 0.00060816 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:02:29,386 DEBUG TRAIN Batch 0/3800 loss 66.536987 loss_att 66.349892 loss_ctc 66.973557 lr 0.00060816 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:02:29,388 DEBUG TRAIN Batch 0/3800 loss 62.216656 loss_att 59.628109 loss_ctc 68.256592 lr 0.00060816 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:02:29,388 DEBUG TRAIN Batch 0/3800 loss 67.159264 loss_att 65.007835 loss_ctc 72.179253 lr 0.00060816 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:02:29,389 DEBUG TRAIN Batch 0/3800 loss 65.590881 loss_att 64.938431 loss_ctc 67.113266 lr 0.00060816 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:02:29,390 DEBUG TRAIN Batch 0/3800 loss 49.135502 loss_att 48.747337 loss_ctc 50.041225 lr 0.00060816 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:02:29,390 DEBUG TRAIN Batch 0/3800 loss 56.921661 loss_att 56.317810 loss_ctc 58.330650 lr 0.00060816 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:02:29,393 DEBUG TRAIN Batch 0/3800 loss 65.007935 loss_att 64.192642 loss_ctc 66.910271 lr 0.00060816 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:05:24,820 DEBUG TRAIN Batch 0/3900 loss 88.926514 loss_att 86.935547 loss_ctc 93.572098 lr 0.00062416 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:05:24,823 DEBUG TRAIN Batch 0/3900 loss 75.331619 loss_att 73.246880 loss_ctc 80.196007 lr 0.00062416 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:05:24,825 DEBUG TRAIN Batch 0/3900 loss 78.526947 loss_att 77.215622 loss_ctc 81.586708 lr 0.00062416 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:05:24,827 DEBUG TRAIN Batch 0/3900 loss 79.797493 loss_att 77.121414 loss_ctc 86.041679 lr 0.00062416 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:05:24,828 DEBUG TRAIN Batch 0/3900 loss 76.043755 loss_att 75.575249 loss_ctc 77.136940 lr 0.00062416 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:05:24,830 DEBUG TRAIN Batch 0/3900 loss 67.063324 loss_att 65.672516 loss_ctc 70.308533 lr 0.00062416 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:05:24,832 DEBUG TRAIN Batch 0/3900 loss 77.407440 loss_att 75.455765 loss_ctc 81.961357 lr 0.00062416 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:05:24,833 DEBUG TRAIN Batch 0/3900 loss 77.540344 loss_att 77.156555 loss_ctc 78.435844 lr 0.00062416 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:18,169 DEBUG TRAIN Batch 0/4000 loss 18.686584 loss_att 18.133556 loss_ctc 19.976988 lr 0.00064016 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:18,172 DEBUG TRAIN Batch 0/4000 loss 22.503050 loss_att 22.710270 loss_ctc 22.019537 lr 0.00064016 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:18,173 DEBUG TRAIN Batch 0/4000 loss 25.171696 loss_att 22.959438 loss_ctc 30.333633 lr 0.00064016 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:18,176 DEBUG TRAIN Batch 0/4000 loss 17.547684 loss_att 16.118853 loss_ctc 20.881622 lr 0.00064016 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:18,176 DEBUG TRAIN Batch 0/4000 loss 21.104662 loss_att 21.004492 loss_ctc 21.338390 lr 0.00064016 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:18,177 DEBUG TRAIN Batch 0/4000 loss 20.985374 loss_att 19.855722 loss_ctc 23.621231 lr 0.00064016 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:18,194 DEBUG TRAIN Batch 0/4000 loss 26.410557 loss_att 24.769855 loss_ctc 30.238861 lr 0.00064016 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:18,196 DEBUG TRAIN Batch 0/4000 loss 27.158249 loss_att 27.092571 loss_ctc 27.311499 lr 0.00064016 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:51,188 DEBUG TRAIN Batch 0/4100 loss 60.099247 loss_att 59.213173 loss_ctc 62.166756 lr 0.00065616 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:51,189 DEBUG TRAIN Batch 0/4100 loss 75.119270 loss_att 74.548004 loss_ctc 76.452225 lr 0.00065616 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:51,189 DEBUG TRAIN Batch 0/4100 loss 65.354607 loss_att 63.913116 loss_ctc 68.718079 lr 0.00065616 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:51,192 DEBUG TRAIN Batch 0/4100 loss 59.901279 loss_att 58.478424 loss_ctc 63.221283 lr 0.00065616 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:51,192 DEBUG TRAIN Batch 0/4100 loss 73.713104 loss_att 71.218765 loss_ctc 79.533226 lr 0.00065616 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:51,194 DEBUG TRAIN Batch 0/4100 loss 50.399445 loss_att 49.926678 loss_ctc 51.502563 lr 0.00065616 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:51,196 DEBUG TRAIN Batch 0/4100 loss 63.615669 loss_att 61.896553 loss_ctc 67.626945 lr 0.00065616 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:08:51,197 DEBUG TRAIN Batch 0/4100 loss 73.673096 loss_att 70.917755 loss_ctc 80.102226 lr 0.00065616 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:11:44,563 DEBUG TRAIN Batch 0/4200 loss 58.808167 loss_att 58.290039 loss_ctc 60.017128 lr 0.00067216 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:11:44,563 DEBUG TRAIN Batch 0/4200 loss 58.866295 loss_att 57.230377 loss_ctc 62.683434 lr 0.00067216 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:11:44,563 DEBUG TRAIN Batch 0/4200 loss 59.705139 loss_att 58.502487 loss_ctc 62.511330 lr 0.00067216 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:11:44,564 DEBUG TRAIN Batch 0/4200 loss 69.103630 loss_att 67.703552 loss_ctc 72.370483 lr 0.00067216 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:11:44,565 DEBUG TRAIN Batch 0/4200 loss 65.695435 loss_att 63.641724 loss_ctc 70.487434 lr 0.00067216 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:11:44,570 DEBUG TRAIN Batch 0/4200 loss 65.635590 loss_att 65.478378 loss_ctc 66.002411 lr 0.00067216 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:11:44,573 DEBUG TRAIN Batch 0/4200 loss 75.723587 loss_att 74.297668 loss_ctc 79.050743 lr 0.00067216 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:11:44,603 DEBUG TRAIN Batch 0/4200 loss 66.879166 loss_att 63.984070 loss_ctc 73.634392 lr 0.00067216 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:14:38,666 DEBUG TRAIN Batch 0/4300 loss 50.575012 loss_att 47.891399 loss_ctc 56.836773 lr 0.00068816 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:14:38,666 DEBUG TRAIN Batch 0/4300 loss 52.882690 loss_att 49.481045 loss_ctc 60.819866 lr 0.00068816 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:14:38,667 DEBUG TRAIN Batch 0/4300 loss 51.319618 loss_att 48.963745 loss_ctc 56.816650 lr 0.00068816 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:14:38,667 DEBUG TRAIN Batch 0/4300 loss 47.976570 loss_att 45.395737 loss_ctc 53.998505 lr 0.00068816 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:14:38,672 DEBUG TRAIN Batch 0/4300 loss 70.322540 loss_att 66.891594 loss_ctc 78.328064 lr 0.00068816 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:14:38,675 DEBUG TRAIN Batch 0/4300 loss 61.103310 loss_att 56.977489 loss_ctc 70.730225 lr 0.00068816 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:14:38,676 DEBUG TRAIN Batch 0/4300 loss 43.272713 loss_att 41.655773 loss_ctc 47.045578 lr 0.00068816 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:14:38,676 DEBUG TRAIN Batch 0/4300 loss 48.794559 loss_att 47.408195 loss_ctc 52.029404 lr 0.00068816 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:17:31,363 DEBUG TRAIN Batch 0/4400 loss 55.264046 loss_att 52.735634 loss_ctc 61.163662 lr 0.00070416 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:17:31,366 DEBUG TRAIN Batch 0/4400 loss 76.174332 loss_att 70.728760 loss_ctc 88.880676 lr 0.00070416 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:17:31,366 DEBUG TRAIN Batch 0/4400 loss 62.887039 loss_att 59.856091 loss_ctc 69.959259 lr 0.00070416 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:17:31,368 DEBUG TRAIN Batch 0/4400 loss 65.284241 loss_att 61.860008 loss_ctc 73.274124 lr 0.00070416 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:17:31,368 DEBUG TRAIN Batch 0/4400 loss 51.753506 loss_att 51.440117 loss_ctc 52.484749 lr 0.00070416 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:17:31,371 DEBUG TRAIN Batch 0/4400 loss 81.637413 loss_att 76.343689 loss_ctc 93.989441 lr 0.00070416 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:17:31,371 DEBUG TRAIN Batch 0/4400 loss 68.611412 loss_att 66.240349 loss_ctc 74.143890 lr 0.00070416 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:17:31,377 DEBUG TRAIN Batch 0/4400 loss 87.902702 loss_att 83.739616 loss_ctc 97.616577 lr 0.00070416 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:27,127 DEBUG TRAIN Batch 0/4500 loss 23.631073 loss_att 22.337585 loss_ctc 26.649212 lr 0.00072016 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:27,129 DEBUG TRAIN Batch 0/4500 loss 15.146259 loss_att 14.736422 loss_ctc 16.102547 lr 0.00072016 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:27,129 DEBUG TRAIN Batch 0/4500 loss 21.317886 loss_att 20.598591 loss_ctc 22.996244 lr 0.00072016 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:27,131 DEBUG TRAIN Batch 0/4500 loss 15.778534 loss_att 13.990550 loss_ctc 19.950497 lr 0.00072016 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:27,135 DEBUG TRAIN Batch 0/4500 loss 15.883402 loss_att 15.283768 loss_ctc 17.282547 lr 0.00072016 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:27,140 DEBUG TRAIN Batch 0/4500 loss 30.950768 loss_att 27.853771 loss_ctc 38.177097 lr 0.00072016 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:27,146 DEBUG TRAIN Batch 0/4500 loss 18.556610 loss_att 17.104467 loss_ctc 21.944942 lr 0.00072016 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:27,150 DEBUG TRAIN Batch 0/4500 loss 20.544401 loss_att 20.205505 loss_ctc 21.335159 lr 0.00072016 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:58,596 DEBUG TRAIN Batch 0/4600 loss 62.006416 loss_att 57.564278 loss_ctc 72.371399 lr 0.00073616 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:58,598 DEBUG TRAIN Batch 0/4600 loss 44.403130 loss_att 41.299820 loss_ctc 51.644188 lr 0.00073616 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:58,598 DEBUG TRAIN Batch 0/4600 loss 66.246025 loss_att 63.375011 loss_ctc 72.945061 lr 0.00073616 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:58,598 DEBUG TRAIN Batch 0/4600 loss 64.170944 loss_att 60.778503 loss_ctc 72.086639 lr 0.00073616 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:58,599 DEBUG TRAIN Batch 0/4600 loss 65.304306 loss_att 62.742104 loss_ctc 71.282776 lr 0.00073616 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:58,599 DEBUG TRAIN Batch 0/4600 loss 52.536739 loss_att 48.472435 loss_ctc 62.020119 lr 0.00073616 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:58,600 DEBUG TRAIN Batch 0/4600 loss 61.110489 loss_att 57.036438 loss_ctc 70.616600 lr 0.00073616 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:20:58,603 DEBUG TRAIN Batch 0/4600 loss 67.151703 loss_att 63.152649 loss_ctc 76.482834 lr 0.00073616 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:23:56,057 DEBUG TRAIN Batch 0/4700 loss 75.283852 loss_att 70.212112 loss_ctc 87.117905 lr 0.00075216 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:23:56,058 DEBUG TRAIN Batch 0/4700 loss 59.045410 loss_att 56.528435 loss_ctc 64.918350 lr 0.00075216 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:23:56,058 DEBUG TRAIN Batch 0/4700 loss 52.946182 loss_att 49.889400 loss_ctc 60.078674 lr 0.00075216 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:23:56,059 DEBUG TRAIN Batch 0/4700 loss 57.999489 loss_att 54.981243 loss_ctc 65.042068 lr 0.00075216 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:23:56,060 DEBUG TRAIN Batch 0/4700 loss 54.788696 loss_att 51.930351 loss_ctc 61.458168 lr 0.00075216 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:23:56,062 DEBUG TRAIN Batch 0/4700 loss 57.914566 loss_att 55.916168 loss_ctc 62.577507 lr 0.00075216 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:23:56,063 DEBUG TRAIN Batch 0/4700 loss 60.308136 loss_att 56.256691 loss_ctc 69.761497 lr 0.00075216 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:23:56,064 DEBUG TRAIN Batch 0/4700 loss 66.969528 loss_att 63.730286 loss_ctc 74.527756 lr 0.00075216 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:26:55,558 DEBUG TRAIN Batch 0/4800 loss 55.594963 loss_att 50.842316 loss_ctc 66.684486 lr 0.00076816 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:26:55,558 DEBUG TRAIN Batch 0/4800 loss 48.888351 loss_att 45.815735 loss_ctc 56.057793 lr 0.00076816 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:26:55,559 DEBUG TRAIN Batch 0/4800 loss 49.148552 loss_att 46.658371 loss_ctc 54.958969 lr 0.00076816 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:26:55,559 DEBUG TRAIN Batch 0/4800 loss 46.267906 loss_att 43.298134 loss_ctc 53.197380 lr 0.00076816 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:26:55,568 DEBUG TRAIN Batch 0/4800 loss 33.034626 loss_att 30.242874 loss_ctc 39.548714 lr 0.00076816 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:26:55,568 DEBUG TRAIN Batch 0/4800 loss 60.094933 loss_att 54.960457 loss_ctc 72.075371 lr 0.00076816 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:26:55,569 DEBUG TRAIN Batch 0/4800 loss 46.736313 loss_att 44.305401 loss_ctc 52.408443 lr 0.00076816 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:26:55,574 DEBUG TRAIN Batch 0/4800 loss 53.997314 loss_att 49.613583 loss_ctc 64.226013 lr 0.00076816 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:29:51,536 DEBUG TRAIN Batch 0/4900 loss 60.118622 loss_att 55.702251 loss_ctc 70.423485 lr 0.00078416 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:29:51,537 DEBUG TRAIN Batch 0/4900 loss 52.019257 loss_att 48.665459 loss_ctc 59.844788 lr 0.00078416 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:29:51,537 DEBUG TRAIN Batch 0/4900 loss 66.451523 loss_att 63.665298 loss_ctc 72.952713 lr 0.00078416 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:29:51,537 DEBUG TRAIN Batch 0/4900 loss 56.909180 loss_att 53.885967 loss_ctc 63.963348 lr 0.00078416 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:29:51,539 DEBUG TRAIN Batch 0/4900 loss 45.959251 loss_att 43.576286 loss_ctc 51.519497 lr 0.00078416 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:29:51,541 DEBUG TRAIN Batch 0/4900 loss 63.472466 loss_att 59.904568 loss_ctc 71.797562 lr 0.00078416 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:29:51,543 DEBUG TRAIN Batch 0/4900 loss 51.897842 loss_att 49.954964 loss_ctc 56.431225 lr 0.00078416 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:29:51,572 DEBUG TRAIN Batch 0/4900 loss 62.488632 loss_att 58.275467 loss_ctc 72.319351 lr 0.00078416 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:32:47,282 DEBUG TRAIN Batch 0/5000 loss 18.932983 loss_att 17.632055 loss_ctc 21.968483 lr 0.00080016 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:32:47,283 DEBUG TRAIN Batch 0/5000 loss 19.679874 loss_att 18.961990 loss_ctc 21.354940 lr 0.00080016 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:32:47,287 DEBUG TRAIN Batch 0/5000 loss 23.002205 loss_att 20.924170 loss_ctc 27.850950 lr 0.00080016 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:32:47,288 DEBUG TRAIN Batch 0/5000 loss 17.329868 loss_att 14.747306 loss_ctc 23.355846 lr 0.00080016 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:32:47,288 DEBUG TRAIN Batch 0/5000 loss 19.725134 loss_att 17.928238 loss_ctc 23.917891 lr 0.00080016 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:32:47,289 DEBUG TRAIN Batch 0/5000 loss 16.572660 loss_att 15.334219 loss_ctc 19.462355 lr 0.00080016 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:32:47,296 DEBUG TRAIN Batch 0/5000 loss 16.445187 loss_att 15.182235 loss_ctc 19.392075 lr 0.00080016 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:32:47,316 DEBUG TRAIN Batch 0/5000 loss 18.562887 loss_att 16.760117 loss_ctc 22.769354 lr 0.00080016 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:33:19,149 DEBUG TRAIN Batch 0/5100 loss 41.991692 loss_att 38.837204 loss_ctc 49.352158 lr 0.00081616 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:33:19,150 DEBUG TRAIN Batch 0/5100 loss 50.031101 loss_att 46.987610 loss_ctc 57.132576 lr 0.00081616 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:33:19,152 DEBUG TRAIN Batch 0/5100 loss 60.803391 loss_att 56.094734 loss_ctc 71.790253 lr 0.00081616 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:33:19,154 DEBUG TRAIN Batch 0/5100 loss 47.357536 loss_att 44.208038 loss_ctc 54.706360 lr 0.00081616 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:33:19,158 DEBUG TRAIN Batch 0/5100 loss 67.639679 loss_att 63.816326 loss_ctc 76.560822 lr 0.00081616 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:33:19,159 DEBUG TRAIN Batch 0/5100 loss 62.274330 loss_att 58.401337 loss_ctc 71.311310 lr 0.00081616 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:33:19,159 DEBUG TRAIN Batch 0/5100 loss 44.578194 loss_att 41.526455 loss_ctc 51.698914 lr 0.00081616 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:33:19,174 DEBUG TRAIN Batch 0/5100 loss 55.365257 loss_att 52.077469 loss_ctc 63.036758 lr 0.00081616 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:36:12,231 DEBUG TRAIN Batch 0/5200 loss 41.842995 loss_att 39.037727 loss_ctc 48.388622 lr 0.00083216 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:36:12,233 DEBUG TRAIN Batch 0/5200 loss 55.689629 loss_att 52.362530 loss_ctc 63.452850 lr 0.00083216 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:36:12,233 DEBUG TRAIN Batch 0/5200 loss 50.994892 loss_att 48.264328 loss_ctc 57.366203 lr 0.00083216 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:36:12,235 DEBUG TRAIN Batch 0/5200 loss 61.019379 loss_att 56.167542 loss_ctc 72.340324 lr 0.00083216 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:36:12,236 DEBUG TRAIN Batch 0/5200 loss 52.899586 loss_att 50.715359 loss_ctc 57.996109 lr 0.00083216 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:36:12,241 DEBUG TRAIN Batch 0/5200 loss 53.994911 loss_att 51.291229 loss_ctc 60.303509 lr 0.00083216 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:36:12,242 DEBUG TRAIN Batch 0/5200 loss 67.851646 loss_att 63.082623 loss_ctc 78.979355 lr 0.00083216 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:36:12,243 DEBUG TRAIN Batch 0/5200 loss 59.454758 loss_att 56.822659 loss_ctc 65.596321 lr 0.00083216 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:39:05,808 DEBUG TRAIN Batch 0/5300 loss 38.442352 loss_att 35.284084 loss_ctc 45.811642 lr 0.00084816 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:39:05,810 DEBUG TRAIN Batch 0/5300 loss 47.443871 loss_att 44.554909 loss_ctc 54.184769 lr 0.00084816 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:39:05,813 DEBUG TRAIN Batch 0/5300 loss 41.164871 loss_att 39.280602 loss_ctc 45.561504 lr 0.00084816 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:39:05,813 DEBUG TRAIN Batch 0/5300 loss 50.989605 loss_att 45.841072 loss_ctc 63.002850 lr 0.00084816 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:39:05,813 DEBUG TRAIN Batch 0/5300 loss 30.798233 loss_att 28.817226 loss_ctc 35.420578 lr 0.00084816 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:39:05,825 DEBUG TRAIN Batch 0/5300 loss 46.898296 loss_att 43.499451 loss_ctc 54.828934 lr 0.00084816 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:39:05,857 DEBUG TRAIN Batch 0/5300 loss 39.266579 loss_att 35.679565 loss_ctc 47.636276 lr 0.00084816 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:39:05,861 DEBUG TRAIN Batch 0/5300 loss 43.087360 loss_att 38.245872 loss_ctc 54.384167 lr 0.00084816 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:48:01,722 DEBUG TRAIN Batch 0/5700 loss 53.584740 loss_att 49.647293 loss_ctc 62.772110 lr 0.00091216 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:48:01,724 DEBUG TRAIN Batch 0/5700 loss 53.236458 loss_att 49.427715 loss_ctc 62.123516 lr 0.00091216 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:48:01,724 DEBUG TRAIN Batch 0/5700 loss 69.591759 loss_att 64.282158 loss_ctc 81.980835 lr 0.00091216 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:48:01,731 DEBUG TRAIN Batch 0/5700 loss 53.959866 loss_att 52.144695 loss_ctc 58.195267 lr 0.00091216 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:48:01,734 DEBUG TRAIN Batch 0/5700 loss 53.987514 loss_att 50.652416 loss_ctc 61.769409 lr 0.00091216 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:48:01,734 DEBUG TRAIN Batch 0/5700 loss 59.575268 loss_att 57.256344 loss_ctc 64.986084 lr 0.00091216 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:48:01,735 DEBUG TRAIN Batch 0/5700 loss 51.360008 loss_att 46.654633 loss_ctc 62.339214 lr 0.00091216 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:48:01,735 DEBUG TRAIN Batch 0/5700 loss 47.540863 loss_att 45.144127 loss_ctc 53.133251 lr 0.00091216 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:50:56,950 DEBUG TRAIN Batch 0/5800 loss 38.220909 loss_att 35.597572 loss_ctc 44.342026 lr 0.00092816 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:50:56,955 DEBUG TRAIN Batch 0/5800 loss 55.236801 loss_att 51.015060 loss_ctc 65.087540 lr 0.00092816 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:50:56,959 DEBUG TRAIN Batch 0/5800 loss 29.965710 loss_att 28.482840 loss_ctc 33.425735 lr 0.00092816 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:50:56,959 DEBUG TRAIN Batch 0/5800 loss 34.972549 loss_att 32.846100 loss_ctc 39.934273 lr 0.00092816 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:50:56,959 DEBUG TRAIN Batch 0/5800 loss 37.837318 loss_att 34.513168 loss_ctc 45.593670 lr 0.00092816 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:50:56,959 DEBUG TRAIN Batch 0/5800 loss 45.550129 loss_att 41.221054 loss_ctc 55.651302 lr 0.00092816 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:50:56,962 DEBUG TRAIN Batch 0/5800 loss 37.640800 loss_att 34.603294 loss_ctc 44.728317 lr 0.00092816 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:50:56,977 DEBUG TRAIN Batch 0/5800 loss 36.622604 loss_att 33.374619 loss_ctc 44.201244 lr 0.00092816 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:52:58,945 DEBUG TRAIN Batch 0/5900 loss 50.910229 loss_att 47.761227 loss_ctc 58.257904 lr 0.00094416 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:52:58,946 DEBUG TRAIN Batch 0/5900 loss 48.975060 loss_att 47.441051 loss_ctc 52.554413 lr 0.00094416 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:52:58,947 DEBUG TRAIN Batch 0/5900 loss 72.887543 loss_att 65.942917 loss_ctc 89.091675 lr 0.00094416 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:52:58,949 DEBUG TRAIN Batch 0/5900 loss 58.745411 loss_att 54.938488 loss_ctc 67.628235 lr 0.00094416 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:52:58,951 DEBUG TRAIN Batch 0/5900 loss 49.054573 loss_att 45.462093 loss_ctc 57.437023 lr 0.00094416 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:52:58,953 DEBUG TRAIN Batch 0/5900 loss 41.159916 loss_att 38.062355 loss_ctc 48.387554 lr 0.00094416 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:52:58,955 DEBUG TRAIN Batch 0/5900 loss 51.184181 loss_att 47.069267 loss_ctc 60.785656 lr 0.00094416 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:52:58,957 DEBUG TRAIN Batch 0/5900 loss 60.068691 loss_att 54.607193 loss_ctc 72.812195 lr 0.00094416 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:54:08,071 DEBUG CV Batch 0/0 loss 6.553925 loss_att 5.570512 loss_ctc 8.848553 history loss 6.049777 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:54:08,072 DEBUG CV Batch 0/0 loss 6.553925 loss_att 5.570512 loss_ctc 8.848553 history loss 6.049777 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:54:08,073 DEBUG CV Batch 0/0 loss 6.553925 loss_att 5.570512 loss_ctc 8.848553 history loss 6.049777 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:54:08,075 DEBUG CV Batch 0/0 loss 6.553925 loss_att 5.570512 loss_ctc 8.848553 history loss 6.049777 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:54:08,076 DEBUG CV Batch 0/0 loss 6.553925 loss_att 5.570512 loss_ctc 8.848553 history loss 6.049777 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:54:08,076 DEBUG CV Batch 0/0 loss 6.553925 loss_att 5.570512 loss_ctc 8.848553 history loss 6.049777 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:54:08,076 DEBUG CV Batch 0/0 loss 6.553925 loss_att 5.570512 loss_ctc 8.848553 history loss 6.049777 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:54:08,078 DEBUG CV Batch 0/0 loss 6.553925 loss_att 5.570512 loss_ctc 8.848553 history loss 6.049777 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:55:42,843 DEBUG CV Batch 0/100 loss 12.549555 loss_att 11.359236 loss_ctc 15.326965 history loss 16.437942 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:55:42,849 DEBUG CV Batch 0/100 loss 12.549555 loss_att 11.359236 loss_ctc 15.326965 history loss 16.437942 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:55:42,853 DEBUG CV Batch 0/100 loss 12.549555 loss_att 11.359236 loss_ctc 15.326965 history loss 16.437942 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:55:42,870 DEBUG CV Batch 0/100 loss 12.549555 loss_att 11.359236 loss_ctc 15.326965 history loss 16.437942 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:55:42,880 DEBUG CV Batch 0/100 loss 12.549555 loss_att 11.359236 loss_ctc 15.326965 history loss 16.437942 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:55:42,921 DEBUG CV Batch 0/100 loss 12.549555 loss_att 11.359236 loss_ctc 15.326965 history loss 16.437942 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:55:42,950 DEBUG CV Batch 0/100 loss 12.549555 loss_att 11.359236 loss_ctc 15.326965 history loss 16.437942 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:55:42,998 DEBUG CV Batch 0/100 loss 12.549555 loss_att 11.359236 loss_ctc 15.326965 history loss 16.437942 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:57:09,434 DEBUG CV Batch 0/200 loss 28.305174 loss_att 26.016098 loss_ctc 33.646355 history loss 17.591745 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:57:09,442 DEBUG CV Batch 0/200 loss 28.305174 loss_att 26.016098 loss_ctc 33.646355 history loss 17.591745 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:57:09,450 DEBUG CV Batch 0/200 loss 28.305174 loss_att 26.016098 loss_ctc 33.646355 history loss 17.591745 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:57:09,489 DEBUG CV Batch 0/200 loss 28.305174 loss_att 26.016098 loss_ctc 33.646355 history loss 17.591745 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:57:09,499 DEBUG CV Batch 0/200 loss 28.305174 loss_att 26.016098 loss_ctc 33.646355 history loss 17.591745 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 01:57:09,534 DEBUG CV Batch 0/200 loss 28.305174 loss_att 26.016098 loss_ctc 33.646355 history loss 17.591745 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:57:09,596 DEBUG CV Batch 0/200 loss 28.305174 loss_att 26.016098 loss_ctc 33.646355 history loss 17.591745 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:57:09,630 DEBUG CV Batch 0/200 loss 28.305174 loss_att 26.016098 loss_ctc 33.646355 history loss 17.591745 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:59:18,812 DEBUG CV Batch 0/300 loss 12.842491 loss_att 11.399943 loss_ctc 16.208437 history loss 20.692681 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 01:59:18,816 DEBUG CV Batch 0/300 loss 12.842491 loss_att 11.399943 loss_ctc 16.208437 history loss 20.692681 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 01:59:18,818 DEBUG CV Batch 0/300 loss 12.842491 loss_att 11.399943 loss_ctc 16.208437 history loss 20.692681 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 01:59:18,819 DEBUG CV Batch 0/300 loss 12.842491 loss_att 11.399943 loss_ctc 16.208437 history loss 20.692681 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 01:59:18,846 DEBUG CV Batch 0/300 loss 12.842491 loss_att 11.399943 loss_ctc 16.208437 history loss 20.692681 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 01:59:18,850 DEBUG CV Batch 0/300 loss 12.842491 loss_att 11.399943 loss_ctc 16.208437 history loss 20.692681 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 01:59:18,873 DEBUG CV Batch 0/300 loss 12.842491 loss_att 11.399943 loss_ctc 16.208437 history loss 20.692681 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 01:59:18,874 DEBUG CV Batch 0/300 loss 12.842491 loss_att 11.399943 loss_ctc 16.208437 history loss 20.692681 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:00:44,457 DEBUG CV Batch 0/400 loss 30.698444 loss_att 26.641218 loss_ctc 40.165298 history loss 22.309042 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:00:44,461 DEBUG CV Batch 0/400 loss 30.698444 loss_att 26.641218 loss_ctc 40.165298 history loss 22.309042 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:00:44,467 DEBUG CV Batch 0/400 loss 30.698444 loss_att 26.641218 loss_ctc 40.165298 history loss 22.309042 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:00:44,484 DEBUG CV Batch 0/400 loss 30.698444 loss_att 26.641218 loss_ctc 40.165298 history loss 22.309042 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:00:44,486 DEBUG CV Batch 0/400 loss 30.698444 loss_att 26.641218 loss_ctc 40.165298 history loss 22.309042 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:00:44,507 DEBUG CV Batch 0/400 loss 30.698444 loss_att 26.641218 loss_ctc 40.165298 history loss 22.309042 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:00:44,520 DEBUG CV Batch 0/400 loss 30.698444 loss_att 26.641218 loss_ctc 40.165298 history loss 22.309042 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:00:44,578 DEBUG CV Batch 0/400 loss 30.698444 loss_att 26.641218 loss_ctc 40.165298 history loss 22.309042 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:02:49,443 DEBUG CV Batch 0/500 loss 5.961809 loss_att 5.129793 loss_ctc 7.903181 history loss 22.914710 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:02:49,446 DEBUG CV Batch 0/500 loss 5.961809 loss_att 5.129793 loss_ctc 7.903181 history loss 22.914710 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:02:49,448 DEBUG CV Batch 0/500 loss 5.961809 loss_att 5.129793 loss_ctc 7.903181 history loss 22.914710 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:02:49,448 DEBUG CV Batch 0/500 loss 5.961809 loss_att 5.129793 loss_ctc 7.903181 history loss 22.914710 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:02:49,448 DEBUG CV Batch 0/500 loss 5.961809 loss_att 5.129793 loss_ctc 7.903181 history loss 22.914710 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:02:49,453 DEBUG CV Batch 0/500 loss 5.961809 loss_att 5.129793 loss_ctc 7.903181 history loss 22.914710 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:02:49,468 DEBUG CV Batch 0/500 loss 5.961809 loss_att 5.129793 loss_ctc 7.903181 history loss 22.914710 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:02:49,472 DEBUG CV Batch 0/500 loss 5.961809 loss_att 5.129793 loss_ctc 7.903181 history loss 22.914710 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:04:09,778 DEBUG CV Batch 0/600 loss 16.536215 loss_att 15.192932 loss_ctc 19.670538 history loss 21.825364 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:04:09,779 DEBUG CV Batch 0/600 loss 16.536215 loss_att 15.192932 loss_ctc 19.670538 history loss 21.825364 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:04:09,782 DEBUG CV Batch 0/600 loss 16.536215 loss_att 15.192932 loss_ctc 19.670538 history loss 21.825364 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:04:09,786 DEBUG CV Batch 0/600 loss 16.536215 loss_att 15.192932 loss_ctc 19.670538 history loss 21.825364 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:04:09,787 DEBUG CV Batch 0/600 loss 16.536215 loss_att 15.192932 loss_ctc 19.670538 history loss 21.825364 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:04:09,787 DEBUG CV Batch 0/600 loss 16.536215 loss_att 15.192932 loss_ctc 19.670538 history loss 21.825364 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:04:09,805 DEBUG CV Batch 0/600 loss 16.536215 loss_att 15.192932 loss_ctc 19.670538 history loss 21.825364 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:04:09,820 DEBUG CV Batch 0/600 loss 16.536215 loss_att 15.192932 loss_ctc 19.670538 history loss 21.825364 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:05:29,144 DEBUG CV Batch 0/700 loss 37.551132 loss_att 34.462795 loss_ctc 44.757256 history loss 21.692938 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:05:29,145 DEBUG CV Batch 0/700 loss 37.551132 loss_att 34.462795 loss_ctc 44.757256 history loss 21.692938 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:05:29,172 DEBUG CV Batch 0/700 loss 37.551132 loss_att 34.462795 loss_ctc 44.757256 history loss 21.692938 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:05:29,174 DEBUG CV Batch 0/700 loss 37.551132 loss_att 34.462795 loss_ctc 44.757256 history loss 21.692938 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:05:29,211 DEBUG CV Batch 0/700 loss 37.551132 loss_att 34.462795 loss_ctc 44.757256 history loss 21.692938 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:05:29,216 DEBUG CV Batch 0/700 loss 37.551132 loss_att 34.462795 loss_ctc 44.757256 history loss 21.692938 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:05:29,218 DEBUG CV Batch 0/700 loss 37.551132 loss_att 34.462795 loss_ctc 44.757256 history loss 21.692938 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:05:29,261 DEBUG CV Batch 0/700 loss 37.551132 loss_att 34.462795 loss_ctc 44.757256 history loss 21.692938 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:07:24,218 DEBUG CV Batch 0/800 loss 14.914280 loss_att 13.464831 loss_ctc 18.296326 history loss 22.446241 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:07:24,245 DEBUG CV Batch 0/800 loss 14.914280 loss_att 13.464831 loss_ctc 18.296326 history loss 22.446241 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:07:24,262 DEBUG CV Batch 0/800 loss 14.914280 loss_att 13.464831 loss_ctc 18.296326 history loss 22.446241 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:07:24,264 DEBUG CV Batch 0/800 loss 14.914280 loss_att 13.464831 loss_ctc 18.296326 history loss 22.446241 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:07:24,267 DEBUG CV Batch 0/800 loss 14.914280 loss_att 13.464831 loss_ctc 18.296326 history loss 22.446241 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:07:24,269 DEBUG CV Batch 0/800 loss 14.914280 loss_att 13.464831 loss_ctc 18.296326 history loss 22.446241 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:07:24,270 DEBUG CV Batch 0/800 loss 14.914280 loss_att 13.464831 loss_ctc 18.296326 history loss 22.446241 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:07:24,277 DEBUG CV Batch 0/800 loss 14.914280 loss_att 13.464831 loss_ctc 18.296326 history loss 22.446241 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:08:38,297 DEBUG CV Batch 0/900 loss 20.310806 loss_att 18.165159 loss_ctc 25.317318 history loss 23.034220 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:08:38,322 DEBUG CV Batch 0/900 loss 20.310806 loss_att 18.165159 loss_ctc 25.317318 history loss 23.034220 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:08:38,336 DEBUG CV Batch 0/900 loss 20.310806 loss_att 18.165159 loss_ctc 25.317318 history loss 23.034220 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:08:38,384 DEBUG CV Batch 0/900 loss 20.310806 loss_att 18.165159 loss_ctc 25.317318 history loss 23.034220 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:08:38,390 DEBUG CV Batch 0/900 loss 20.310806 loss_att 18.165159 loss_ctc 25.317318 history loss 23.034220 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:08:38,405 DEBUG CV Batch 0/900 loss 20.310806 loss_att 18.165159 loss_ctc 25.317318 history loss 23.034220 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:08:38,406 DEBUG CV Batch 0/900 loss 20.310806 loss_att 18.165159 loss_ctc 25.317318 history loss 23.034220 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:08:38,542 DEBUG CV Batch 0/900 loss 20.310806 loss_att 18.165159 loss_ctc 25.317318 history loss 23.034220 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:10:35,390 DEBUG CV Batch 0/1000 loss 5.005470 loss_att 4.543194 loss_ctc 6.084115 history loss 22.869289 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:10:35,391 DEBUG CV Batch 0/1000 loss 5.005470 loss_att 4.543194 loss_ctc 6.084115 history loss 22.869289 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:10:35,391 DEBUG CV Batch 0/1000 loss 5.005470 loss_att 4.543194 loss_ctc 6.084115 history loss 22.869289 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:10:35,391 DEBUG CV Batch 0/1000 loss 5.005470 loss_att 4.543194 loss_ctc 6.084115 history loss 22.869289 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:10:35,392 DEBUG CV Batch 0/1000 loss 5.005470 loss_att 4.543194 loss_ctc 6.084115 history loss 22.869289 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:10:35,395 DEBUG CV Batch 0/1000 loss 5.005470 loss_att 4.543194 loss_ctc 6.084115 history loss 22.869289 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:10:35,400 DEBUG CV Batch 0/1000 loss 5.005470 loss_att 4.543194 loss_ctc 6.084115 history loss 22.869289 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:10:35,411 DEBUG CV Batch 0/1000 loss 5.005470 loss_att 4.543194 loss_ctc 6.084115 history loss 22.869289 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:11:51,540 DEBUG CV Batch 0/1100 loss 14.425034 loss_att 12.922576 loss_ctc 17.930769 history loss 22.305702 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:11:51,542 DEBUG CV Batch 0/1100 loss 14.425034 loss_att 12.922576 loss_ctc 17.930769 history loss 22.305702 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:11:51,562 DEBUG CV Batch 0/1100 loss 14.425034 loss_att 12.922576 loss_ctc 17.930769 history loss 22.305702 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:11:51,572 DEBUG CV Batch 0/1100 loss 14.425034 loss_att 12.922576 loss_ctc 17.930769 history loss 22.305702 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:11:51,573 DEBUG CV Batch 0/1100 loss 14.425034 loss_att 12.922576 loss_ctc 17.930769 history loss 22.305702 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:11:51,585 DEBUG CV Batch 0/1100 loss 14.425034 loss_att 12.922576 loss_ctc 17.930769 history loss 22.305702 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:11:51,598 DEBUG CV Batch 0/1100 loss 14.425034 loss_att 12.922576 loss_ctc 17.930769 history loss 22.305702 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:11:51,615 DEBUG CV Batch 0/1100 loss 14.425034 loss_att 12.922576 loss_ctc 17.930769 history loss 22.305702 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:13:06,569 DEBUG CV Batch 0/1200 loss 40.204021 loss_att 36.657677 loss_ctc 48.478821 history loss 22.582430 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:13:06,570 DEBUG CV Batch 0/1200 loss 40.204021 loss_att 36.657677 loss_ctc 48.478821 history loss 22.582430 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:13:06,598 DEBUG CV Batch 0/1200 loss 40.204021 loss_att 36.657677 loss_ctc 48.478821 history loss 22.582430 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:13:06,644 DEBUG CV Batch 0/1200 loss 40.204021 loss_att 36.657677 loss_ctc 48.478821 history loss 22.582430 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:13:06,652 DEBUG CV Batch 0/1200 loss 40.204021 loss_att 36.657677 loss_ctc 48.478821 history loss 22.582430 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:13:06,660 DEBUG CV Batch 0/1200 loss 40.204021 loss_att 36.657677 loss_ctc 48.478821 history loss 22.582430 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:13:06,676 DEBUG CV Batch 0/1200 loss 40.204021 loss_att 36.657677 loss_ctc 48.478821 history loss 22.582430 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:13:06,719 DEBUG CV Batch 0/1200 loss 40.204021 loss_att 36.657677 loss_ctc 48.478821 history loss 22.582430 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:14:54,167 DEBUG CV Batch 0/1300 loss 19.369061 loss_att 16.365158 loss_ctc 26.378164 history loss 23.040918 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:14:54,198 DEBUG CV Batch 0/1300 loss 19.369061 loss_att 16.365158 loss_ctc 26.378164 history loss 23.040918 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:14:54,201 DEBUG CV Batch 0/1300 loss 19.369061 loss_att 16.365158 loss_ctc 26.378164 history loss 23.040918 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:14:54,206 DEBUG CV Batch 0/1300 loss 19.369061 loss_att 16.365158 loss_ctc 26.378164 history loss 23.040918 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:14:54,207 DEBUG CV Batch 0/1300 loss 19.369061 loss_att 16.365158 loss_ctc 26.378164 history loss 23.040918 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:14:54,207 DEBUG CV Batch 0/1300 loss 19.369061 loss_att 16.365158 loss_ctc 26.378164 history loss 23.040918 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:14:54,209 DEBUG CV Batch 0/1300 loss 19.369061 loss_att 16.365158 loss_ctc 26.378164 history loss 23.040918 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:14:54,232 DEBUG CV Batch 0/1300 loss 19.369061 loss_att 16.365158 loss_ctc 26.378164 history loss 23.040918 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:22,991 INFO Epoch 0 CV info cv_loss 23.317426207880473\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:22,991 INFO Epoch 1 TRAIN info lr 0.00095488\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:22,993 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,028 INFO Epoch 0 CV info cv_loss 23.317426207880473\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,028 INFO Epoch 1 TRAIN info lr 0.00095504\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,030 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,057 INFO Epoch 0 CV info cv_loss 23.317426207880473\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,057 INFO Checkpoint: save to checkpoint /opt/ml/checkpoints/0.pt\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,059 INFO Epoch 0 CV info cv_loss 23.317426207880473\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,059 INFO Epoch 1 TRAIN info lr 0.00095504\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,060 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,070 INFO Epoch 0 CV info cv_loss 23.317426207880473\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,070 INFO Epoch 1 TRAIN info lr 0.00095504\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,072 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,074 INFO Epoch 0 CV info cv_loss 23.317426207880473\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,074 INFO Epoch 1 TRAIN info lr 0.00095504\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,074 INFO Epoch 0 CV info cv_loss 23.317426207880473\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,075 INFO Epoch 1 TRAIN info lr 0.00095504\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,076 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,076 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,148 INFO Epoch 0 CV info cv_loss 23.317426207880473\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,148 INFO Epoch 1 TRAIN info lr 0.00095488\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,150 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,401 INFO Epoch 1 TRAIN info lr 0.00095488\u001b[0m\n",
      "\u001b[34m2023-03-29 02:15:23,403 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:12,141 DEBUG TRAIN Batch 1/0 loss 14.290369 loss_att 12.961279 loss_ctc 17.391579 lr 0.00095504 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:12,144 DEBUG TRAIN Batch 1/0 loss 15.847467 loss_att 13.858310 loss_ctc 20.488834 lr 0.00095520 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:12,147 DEBUG TRAIN Batch 1/0 loss 18.286484 loss_att 16.538166 loss_ctc 22.365887 lr 0.00095504 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:12,149 DEBUG TRAIN Batch 1/0 loss 21.023687 loss_att 19.191563 loss_ctc 25.298643 lr 0.00095520 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:12,150 DEBUG TRAIN Batch 1/0 loss 17.620323 loss_att 16.416935 loss_ctc 20.428230 lr 0.00095520 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:12,150 DEBUG TRAIN Batch 1/0 loss 19.273827 loss_att 17.888149 loss_ctc 22.507071 lr 0.00095520 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:12,156 DEBUG TRAIN Batch 1/0 loss 19.570108 loss_att 18.202347 loss_ctc 22.761551 lr 0.00095504 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:12,162 DEBUG TRAIN Batch 1/0 loss 18.387104 loss_att 16.982706 loss_ctc 21.664032 lr 0.00095520 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:46,650 DEBUG TRAIN Batch 1/100 loss 44.056496 loss_att 39.921154 loss_ctc 53.705627 lr 0.00097120 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:46,653 DEBUG TRAIN Batch 1/100 loss 47.959198 loss_att 43.271904 loss_ctc 58.896206 lr 0.00097120 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:46,653 DEBUG TRAIN Batch 1/100 loss 51.653473 loss_att 48.131927 loss_ctc 59.870403 lr 0.00097104 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:46,654 DEBUG TRAIN Batch 1/100 loss 48.387447 loss_att 44.886120 loss_ctc 56.557205 lr 0.00097120 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:46,654 DEBUG TRAIN Batch 1/100 loss 43.071564 loss_att 39.156460 loss_ctc 52.206814 lr 0.00097120 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:46,655 DEBUG TRAIN Batch 1/100 loss 57.885544 loss_att 53.499584 loss_ctc 68.119446 lr 0.00097104 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:46,655 DEBUG TRAIN Batch 1/100 loss 40.391880 loss_att 37.462200 loss_ctc 47.227798 lr 0.00097120 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:18:46,660 DEBUG TRAIN Batch 1/100 loss 48.581264 loss_att 43.565331 loss_ctc 60.285110 lr 0.00097104 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:21:35,080 DEBUG TRAIN Batch 1/200 loss 54.830006 loss_att 50.756786 loss_ctc 64.334183 lr 0.00098720 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:21:35,080 DEBUG TRAIN Batch 1/200 loss 48.377571 loss_att 43.323509 loss_ctc 60.170380 lr 0.00098720 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:21:35,081 DEBUG TRAIN Batch 1/200 loss 52.728180 loss_att 48.025055 loss_ctc 63.702133 lr 0.00098720 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:21:35,082 DEBUG TRAIN Batch 1/200 loss 44.642456 loss_att 40.854774 loss_ctc 53.480385 lr 0.00098704 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:21:35,090 DEBUG TRAIN Batch 1/200 loss 51.993248 loss_att 47.119415 loss_ctc 63.365524 lr 0.00098704 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:21:35,090 DEBUG TRAIN Batch 1/200 loss 47.432934 loss_att 43.532967 loss_ctc 56.532860 lr 0.00098704 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:21:35,090 DEBUG TRAIN Batch 1/200 loss 52.830498 loss_att 49.437416 loss_ctc 60.747692 lr 0.00098720 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:21:35,092 DEBUG TRAIN Batch 1/200 loss 54.591408 loss_att 52.139919 loss_ctc 60.311535 lr 0.00098720 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:24:28,377 DEBUG TRAIN Batch 1/300 loss 38.829300 loss_att 34.553810 loss_ctc 48.805450 lr 0.00100320 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:24:28,378 DEBUG TRAIN Batch 1/300 loss 46.706795 loss_att 44.020550 loss_ctc 52.974701 lr 0.00100320 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:24:28,378 DEBUG TRAIN Batch 1/300 loss 49.817738 loss_att 45.904217 loss_ctc 58.949291 lr 0.00100304 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:24:28,378 DEBUG TRAIN Batch 1/300 loss 34.031090 loss_att 31.357006 loss_ctc 40.270615 lr 0.00100320 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:24:28,380 DEBUG TRAIN Batch 1/300 loss 47.569664 loss_att 43.027275 loss_ctc 58.168571 lr 0.00100320 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:24:28,383 DEBUG TRAIN Batch 1/300 loss 37.004204 loss_att 33.470566 loss_ctc 45.249359 lr 0.00100304 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:24:28,384 DEBUG TRAIN Batch 1/300 loss 40.451500 loss_att 37.849060 loss_ctc 46.523861 lr 0.00100320 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:24:28,386 DEBUG TRAIN Batch 1/300 loss 38.663101 loss_att 35.268997 loss_ctc 46.582672 lr 0.00100304 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:27:18,305 DEBUG TRAIN Batch 1/400 loss 58.479984 loss_att 53.247917 loss_ctc 70.688141 lr 0.00101904 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:27:18,307 DEBUG TRAIN Batch 1/400 loss 42.950287 loss_att 39.471405 loss_ctc 51.067680 lr 0.00101920 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:27:18,308 DEBUG TRAIN Batch 1/400 loss 54.639328 loss_att 49.265244 loss_ctc 67.178856 lr 0.00101920 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:27:18,309 DEBUG TRAIN Batch 1/400 loss 45.630535 loss_att 41.608833 loss_ctc 55.014503 lr 0.00101920 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:27:18,314 DEBUG TRAIN Batch 1/400 loss 50.575737 loss_att 46.011806 loss_ctc 61.224915 lr 0.00101920 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:27:18,314 DEBUG TRAIN Batch 1/400 loss 39.734970 loss_att 36.412239 loss_ctc 47.488007 lr 0.00101920 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:27:18,315 DEBUG TRAIN Batch 1/400 loss 45.817272 loss_att 41.806908 loss_ctc 55.174786 lr 0.00101904 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:27:18,316 DEBUG TRAIN Batch 1/400 loss 41.737801 loss_att 37.973763 loss_ctc 50.520561 lr 0.00101904 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:05,764 DEBUG TRAIN Batch 1/500 loss 14.318577 loss_att 12.702484 loss_ctc 18.089460 lr 0.00103520 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:05,765 DEBUG TRAIN Batch 1/500 loss 14.601987 loss_att 13.046341 loss_ctc 18.231827 lr 0.00103520 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:05,766 DEBUG TRAIN Batch 1/500 loss 22.751961 loss_att 19.925940 loss_ctc 29.346008 lr 0.00103504 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:05,767 DEBUG TRAIN Batch 1/500 loss 16.214884 loss_att 14.958602 loss_ctc 19.146208 lr 0.00103520 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:05,769 DEBUG TRAIN Batch 1/500 loss 20.961128 loss_att 18.354458 loss_ctc 27.043356 lr 0.00103504 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:05,770 DEBUG TRAIN Batch 1/500 loss 18.224632 loss_att 16.421371 loss_ctc 22.432243 lr 0.00103520 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:05,772 DEBUG TRAIN Batch 1/500 loss 18.155642 loss_att 15.706537 loss_ctc 23.870220 lr 0.00103504 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:05,786 DEBUG TRAIN Batch 1/500 loss 15.682791 loss_att 13.175287 loss_ctc 21.533630 lr 0.00103520 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:39,491 DEBUG TRAIN Batch 1/600 loss 35.069695 loss_att 32.304981 loss_ctc 41.520683 lr 0.00105120 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:39,491 DEBUG TRAIN Batch 1/600 loss 48.818798 loss_att 43.714760 loss_ctc 60.728218 lr 0.00105120 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:39,492 DEBUG TRAIN Batch 1/600 loss 39.979145 loss_att 36.464035 loss_ctc 48.181072 lr 0.00105120 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:39,497 DEBUG TRAIN Batch 1/600 loss 48.619690 loss_att 43.618896 loss_ctc 60.288208 lr 0.00105104 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:39,498 DEBUG TRAIN Batch 1/600 loss 44.010777 loss_att 40.085632 loss_ctc 53.169449 lr 0.00105104 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:39,501 DEBUG TRAIN Batch 1/600 loss 39.287109 loss_att 36.660465 loss_ctc 45.415951 lr 0.00105120 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:39,501 DEBUG TRAIN Batch 1/600 loss 59.452457 loss_att 54.447411 loss_ctc 71.130905 lr 0.00105104 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:30:39,521 DEBUG TRAIN Batch 1/600 loss 51.804688 loss_att 49.045498 loss_ctc 58.242794 lr 0.00105120 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:33:26,966 DEBUG TRAIN Batch 1/700 loss 49.044456 loss_att 45.711987 loss_ctc 56.820213 lr 0.00106720 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:33:26,967 DEBUG TRAIN Batch 1/700 loss 49.568989 loss_att 46.366650 loss_ctc 57.041115 lr 0.00106704 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:33:26,972 DEBUG TRAIN Batch 1/700 loss 46.512505 loss_att 43.517998 loss_ctc 53.499676 lr 0.00106704 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:33:26,975 DEBUG TRAIN Batch 1/700 loss 37.793327 loss_att 34.676998 loss_ctc 45.064766 lr 0.00106720 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:33:26,976 DEBUG TRAIN Batch 1/700 loss 42.739700 loss_att 40.648842 loss_ctc 47.618374 lr 0.00106720 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:33:26,978 DEBUG TRAIN Batch 1/700 loss 40.858547 loss_att 36.133801 loss_ctc 51.882957 lr 0.00106720 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:33:26,979 DEBUG TRAIN Batch 1/700 loss 46.518028 loss_att 43.783752 loss_ctc 52.898003 lr 0.00106704 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:33:26,992 DEBUG TRAIN Batch 1/700 loss 43.596905 loss_att 40.323303 loss_ctc 51.235302 lr 0.00106720 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:36:14,187 DEBUG TRAIN Batch 1/800 loss 37.962784 loss_att 34.139309 loss_ctc 46.884216 lr 0.00108304 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:36:14,188 DEBUG TRAIN Batch 1/800 loss 37.628616 loss_att 34.582989 loss_ctc 44.735077 lr 0.00108320 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:36:14,188 DEBUG TRAIN Batch 1/800 loss 38.085815 loss_att 33.564529 loss_ctc 48.635475 lr 0.00108320 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:36:14,188 DEBUG TRAIN Batch 1/800 loss 32.429314 loss_att 29.676888 loss_ctc 38.851646 lr 0.00108320 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:36:14,189 DEBUG TRAIN Batch 1/800 loss 37.551960 loss_att 35.180569 loss_ctc 43.085205 lr 0.00108320 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:36:14,191 DEBUG TRAIN Batch 1/800 loss 41.839737 loss_att 37.175201 loss_ctc 52.723660 lr 0.00108304 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:36:14,194 DEBUG TRAIN Batch 1/800 loss 37.122166 loss_att 33.718956 loss_ctc 45.062981 lr 0.00108320 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:36:14,202 DEBUG TRAIN Batch 1/800 loss 50.850849 loss_att 46.434460 loss_ctc 61.155762 lr 0.00108304 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:39:02,901 DEBUG TRAIN Batch 1/900 loss 50.779274 loss_att 45.135983 loss_ctc 63.946953 lr 0.00109904 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:39:02,901 DEBUG TRAIN Batch 1/900 loss 62.014160 loss_att 54.200844 loss_ctc 80.245239 lr 0.00109920 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:39:02,901 DEBUG TRAIN Batch 1/900 loss 39.101189 loss_att 36.776039 loss_ctc 44.526543 lr 0.00109920 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:39:02,903 DEBUG TRAIN Batch 1/900 loss 47.710415 loss_att 43.505341 loss_ctc 57.522255 lr 0.00109904 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:39:02,908 DEBUG TRAIN Batch 1/900 loss 49.663322 loss_att 45.694611 loss_ctc 58.923641 lr 0.00109920 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:39:02,926 DEBUG TRAIN Batch 1/900 loss 39.196373 loss_att 36.086216 loss_ctc 46.453400 lr 0.00109904 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:39:02,945 DEBUG TRAIN Batch 1/900 loss 41.618546 loss_att 37.394436 loss_ctc 51.474804 lr 0.00109920 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:39:02,946 DEBUG TRAIN Batch 1/900 loss 51.314137 loss_att 45.316647 loss_ctc 65.308273 lr 0.00109920 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:41:50,791 DEBUG TRAIN Batch 1/1000 loss 11.591534 loss_att 10.841674 loss_ctc 13.341208 lr 0.00111520 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:41:50,791 DEBUG TRAIN Batch 1/1000 loss 14.340860 loss_att 12.305956 loss_ctc 19.088970 lr 0.00111520 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:41:50,794 DEBUG TRAIN Batch 1/1000 loss 12.321629 loss_att 11.340518 loss_ctc 14.610886 lr 0.00111520 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:41:50,797 DEBUG TRAIN Batch 1/1000 loss 17.766191 loss_att 16.075497 loss_ctc 21.711147 lr 0.00111520 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:41:50,798 DEBUG TRAIN Batch 1/1000 loss 21.377800 loss_att 19.807039 loss_ctc 25.042908 lr 0.00111520 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:41:50,799 DEBUG TRAIN Batch 1/1000 loss 20.954973 loss_att 19.341587 loss_ctc 24.719540 lr 0.00111504 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:41:50,805 DEBUG TRAIN Batch 1/1000 loss 19.858135 loss_att 18.008305 loss_ctc 24.174408 lr 0.00111504 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:41:50,808 DEBUG TRAIN Batch 1/1000 loss 19.814987 loss_att 17.432905 loss_ctc 25.373175 lr 0.00111504 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:42:25,133 DEBUG TRAIN Batch 1/1100 loss 61.890129 loss_att 56.289150 loss_ctc 74.959076 lr 0.00113120 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:42:25,134 DEBUG TRAIN Batch 1/1100 loss 37.506828 loss_att 34.131805 loss_ctc 45.381882 lr 0.00113120 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:42:25,141 DEBUG TRAIN Batch 1/1100 loss 39.755642 loss_att 36.681046 loss_ctc 46.929699 lr 0.00113104 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:42:25,142 DEBUG TRAIN Batch 1/1100 loss 48.360619 loss_att 42.626911 loss_ctc 61.739269 lr 0.00113104 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:42:25,143 DEBUG TRAIN Batch 1/1100 loss 46.660809 loss_att 42.690063 loss_ctc 55.925884 lr 0.00113120 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:42:25,144 DEBUG TRAIN Batch 1/1100 loss 48.343994 loss_att 45.159576 loss_ctc 55.774303 lr 0.00113120 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:42:25,144 DEBUG TRAIN Batch 1/1100 loss 34.505199 loss_att 32.148849 loss_ctc 40.003345 lr 0.00113104 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:42:25,160 DEBUG TRAIN Batch 1/1100 loss 49.552784 loss_att 46.944725 loss_ctc 55.638260 lr 0.00113120 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:45:10,411 DEBUG TRAIN Batch 1/1200 loss 45.885605 loss_att 40.382534 loss_ctc 58.726097 lr 0.00114720 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:45:10,410 DEBUG TRAIN Batch 1/1200 loss 44.879265 loss_att 40.441086 loss_ctc 55.235012 lr 0.00114720 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:45:10,411 DEBUG TRAIN Batch 1/1200 loss 52.180222 loss_att 46.914104 loss_ctc 64.467827 lr 0.00114704 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:45:10,416 DEBUG TRAIN Batch 1/1200 loss 45.088615 loss_att 40.818493 loss_ctc 55.052227 lr 0.00114704 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:45:10,419 DEBUG TRAIN Batch 1/1200 loss 35.425610 loss_att 32.747120 loss_ctc 41.675419 lr 0.00114720 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:45:10,419 DEBUG TRAIN Batch 1/1200 loss 46.164421 loss_att 41.940834 loss_ctc 56.019463 lr 0.00114720 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:45:10,421 DEBUG TRAIN Batch 1/1200 loss 41.644157 loss_att 38.212227 loss_ctc 49.652000 lr 0.00114720 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:45:10,434 DEBUG TRAIN Batch 1/1200 loss 42.458332 loss_att 39.362293 loss_ctc 49.682426 lr 0.00114704 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:48:02,823 DEBUG TRAIN Batch 1/1300 loss 43.026745 loss_att 39.159439 loss_ctc 52.050457 lr 0.00116304 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:48:02,826 DEBUG TRAIN Batch 1/1300 loss 45.482029 loss_att 40.390678 loss_ctc 57.361847 lr 0.00116320 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:48:02,827 DEBUG TRAIN Batch 1/1300 loss 29.808094 loss_att 27.460272 loss_ctc 35.286339 lr 0.00116320 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:48:02,832 DEBUG TRAIN Batch 1/1300 loss 33.642784 loss_att 31.095898 loss_ctc 39.585522 lr 0.00116304 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:48:02,833 DEBUG TRAIN Batch 1/1300 loss 40.773640 loss_att 35.722286 loss_ctc 52.560127 lr 0.00116320 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:48:02,835 DEBUG TRAIN Batch 1/1300 loss 35.327950 loss_att 32.707165 loss_ctc 41.443108 lr 0.00116304 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:48:02,838 DEBUG TRAIN Batch 1/1300 loss 39.400848 loss_att 35.740952 loss_ctc 47.940613 lr 0.00116320 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:48:02,838 DEBUG TRAIN Batch 1/1300 loss 35.568825 loss_att 31.645987 loss_ctc 44.722115 lr 0.00116320 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:50:48,903 DEBUG TRAIN Batch 1/1400 loss 42.192688 loss_att 38.779648 loss_ctc 50.156441 lr 0.00117920 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:50:48,903 DEBUG TRAIN Batch 1/1400 loss 36.741856 loss_att 33.866692 loss_ctc 43.450573 lr 0.00117920 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:50:48,909 DEBUG TRAIN Batch 1/1400 loss 41.881878 loss_att 37.926723 loss_ctc 51.110573 lr 0.00117904 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:50:48,911 DEBUG TRAIN Batch 1/1400 loss 46.002480 loss_att 41.075184 loss_ctc 57.499504 lr 0.00117920 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:50:48,912 DEBUG TRAIN Batch 1/1400 loss 45.205444 loss_att 40.841011 loss_ctc 55.389122 lr 0.00117920 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:50:48,913 DEBUG TRAIN Batch 1/1400 loss 48.698166 loss_att 43.178780 loss_ctc 61.576729 lr 0.00117920 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:50:48,914 DEBUG TRAIN Batch 1/1400 loss 49.257786 loss_att 45.673840 loss_ctc 57.620323 lr 0.00117904 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:50:48,926 DEBUG TRAIN Batch 1/1400 loss 38.711975 loss_att 34.828133 loss_ctc 47.774273 lr 0.00117904 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:53:33,542 DEBUG TRAIN Batch 1/1500 loss 12.641335 loss_att 11.434608 loss_ctc 15.457026 lr 0.00119520 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:53:33,542 DEBUG TRAIN Batch 1/1500 loss 17.962008 loss_att 16.016668 loss_ctc 22.501129 lr 0.00119520 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:53:33,543 DEBUG TRAIN Batch 1/1500 loss 17.515471 loss_att 16.079906 loss_ctc 20.865120 lr 0.00119520 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:53:33,548 DEBUG TRAIN Batch 1/1500 loss 11.997353 loss_att 10.377136 loss_ctc 15.777858 lr 0.00119504 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:53:33,549 DEBUG TRAIN Batch 1/1500 loss 17.167507 loss_att 15.244555 loss_ctc 21.654392 lr 0.00119520 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:53:33,551 DEBUG TRAIN Batch 1/1500 loss 21.292809 loss_att 18.693726 loss_ctc 27.357336 lr 0.00119520 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:53:33,558 DEBUG TRAIN Batch 1/1500 loss 13.494314 loss_att 11.754330 loss_ctc 17.554277 lr 0.00119504 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:53:33,568 DEBUG TRAIN Batch 1/1500 loss 11.929605 loss_att 10.798800 loss_ctc 14.568149 lr 0.00119504 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:54:06,630 DEBUG TRAIN Batch 1/1600 loss 37.212570 loss_att 33.239040 loss_ctc 46.484142 lr 0.00121104 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:54:06,631 DEBUG TRAIN Batch 1/1600 loss 38.627529 loss_att 34.982742 loss_ctc 47.132030 lr 0.00121120 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:54:06,631 DEBUG TRAIN Batch 1/1600 loss 42.856274 loss_att 37.714611 loss_ctc 54.853485 lr 0.00121120 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:54:06,632 DEBUG TRAIN Batch 1/1600 loss 37.047840 loss_att 33.459656 loss_ctc 45.420273 lr 0.00121120 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:54:06,634 DEBUG TRAIN Batch 1/1600 loss 41.489059 loss_att 37.765640 loss_ctc 50.177044 lr 0.00121120 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:54:06,639 DEBUG TRAIN Batch 1/1600 loss 46.647942 loss_att 42.870998 loss_ctc 55.460808 lr 0.00121120 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:54:06,639 DEBUG TRAIN Batch 1/1600 loss 60.966103 loss_att 55.527336 loss_ctc 73.656570 lr 0.00121104 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:54:06,641 DEBUG TRAIN Batch 1/1600 loss 49.691536 loss_att 45.333176 loss_ctc 59.861038 lr 0.00121104 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:56:48,749 DEBUG TRAIN Batch 1/1700 loss 50.868088 loss_att 47.013607 loss_ctc 59.861885 lr 0.00122720 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:56:48,749 DEBUG TRAIN Batch 1/1700 loss 44.409111 loss_att 39.959206 loss_ctc 54.792229 lr 0.00122720 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:56:48,749 DEBUG TRAIN Batch 1/1700 loss 39.938099 loss_att 37.093933 loss_ctc 46.574486 lr 0.00122720 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:56:48,751 DEBUG TRAIN Batch 1/1700 loss 44.375969 loss_att 40.548275 loss_ctc 53.307251 lr 0.00122704 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 02:56:48,751 DEBUG TRAIN Batch 1/1700 loss 43.340218 loss_att 40.145321 loss_ctc 50.794975 lr 0.00122704 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:56:48,751 DEBUG TRAIN Batch 1/1700 loss 48.696976 loss_att 44.237560 loss_ctc 59.102272 lr 0.00122720 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:56:48,754 DEBUG TRAIN Batch 1/1700 loss 42.278534 loss_att 39.142776 loss_ctc 49.595299 lr 0.00122720 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:56:48,756 DEBUG TRAIN Batch 1/1700 loss 49.397453 loss_att 42.792965 loss_ctc 64.807922 lr 0.00122704 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:59:30,570 DEBUG TRAIN Batch 1/1800 loss 29.163925 loss_att 26.172132 loss_ctc 36.144779 lr 0.00124320 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 02:59:30,570 DEBUG TRAIN Batch 1/1800 loss 32.032619 loss_att 28.583015 loss_ctc 40.081696 lr 0.00124320 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 02:59:30,570 DEBUG TRAIN Batch 1/1800 loss 30.659731 loss_att 27.254261 loss_ctc 38.605827 lr 0.00124320 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 02:59:30,571 DEBUG TRAIN Batch 1/1800 loss 41.946201 loss_att 38.200279 loss_ctc 50.686687 lr 0.00124320 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 02:59:30,575 DEBUG TRAIN Batch 1/1800 loss 32.021439 loss_att 28.156279 loss_ctc 41.040138 lr 0.00124304 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 02:59:30,576 DEBUG TRAIN Batch 1/1800 loss 41.752247 loss_att 37.471695 loss_ctc 51.740204 lr 0.00124320 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 02:59:30,578 DEBUG TRAIN Batch 1/1800 loss 35.933208 loss_att 32.060394 loss_ctc 44.969769 lr 0.00124304 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 02:59:30,579 DEBUG TRAIN Batch 1/1800 loss 28.052803 loss_att 24.906124 loss_ctc 35.395058 lr 0.00124304 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:02:17,973 DEBUG TRAIN Batch 1/1900 loss 48.485157 loss_att 43.443817 loss_ctc 60.248283 lr 0.00125904 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:02:17,974 DEBUG TRAIN Batch 1/1900 loss 49.950878 loss_att 46.712490 loss_ctc 57.507118 lr 0.00125920 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:02:17,975 DEBUG TRAIN Batch 1/1900 loss 35.386055 loss_att 31.812611 loss_ctc 43.724091 lr 0.00125920 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:02:17,975 DEBUG TRAIN Batch 1/1900 loss 49.410629 loss_att 43.940079 loss_ctc 62.175243 lr 0.00125920 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:02:17,976 DEBUG TRAIN Batch 1/1900 loss 49.682457 loss_att 45.474651 loss_ctc 59.500664 lr 0.00125920 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:02:17,980 DEBUG TRAIN Batch 1/1900 loss 49.539871 loss_att 44.766052 loss_ctc 60.678780 lr 0.00125920 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:02:17,982 DEBUG TRAIN Batch 1/1900 loss 51.592846 loss_att 46.114891 loss_ctc 64.374741 lr 0.00125904 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:02:17,985 DEBUG TRAIN Batch 1/1900 loss 51.606949 loss_att 45.824856 loss_ctc 65.098495 lr 0.00125904 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:08,437 DEBUG TRAIN Batch 1/2000 loss 13.040698 loss_att 11.517281 loss_ctc 16.595337 lr 0.00127520 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:08,437 DEBUG TRAIN Batch 1/2000 loss 16.406403 loss_att 13.963184 loss_ctc 22.107246 lr 0.00127520 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:08,438 DEBUG TRAIN Batch 1/2000 loss 14.948541 loss_att 13.078995 loss_ctc 19.310814 lr 0.00127520 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:08,439 DEBUG TRAIN Batch 1/2000 loss 17.055161 loss_att 15.575686 loss_ctc 20.507261 lr 0.00127504 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:08,442 DEBUG TRAIN Batch 1/2000 loss 18.155731 loss_att 16.494505 loss_ctc 22.031927 lr 0.00127520 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:08,444 DEBUG TRAIN Batch 1/2000 loss 14.932793 loss_att 12.703316 loss_ctc 20.134905 lr 0.00127520 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:08,453 DEBUG TRAIN Batch 1/2000 loss 12.295670 loss_att 10.803000 loss_ctc 15.778564 lr 0.00127504 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:08,491 DEBUG TRAIN Batch 1/2000 loss 13.652170 loss_att 11.562275 loss_ctc 18.528591 lr 0.00127504 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:42,510 DEBUG TRAIN Batch 1/2100 loss 44.179001 loss_att 38.863186 loss_ctc 56.582565 lr 0.00129120 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:42,515 DEBUG TRAIN Batch 1/2100 loss 31.660393 loss_att 29.900183 loss_ctc 35.767548 lr 0.00129104 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:42,517 DEBUG TRAIN Batch 1/2100 loss 29.298656 loss_att 26.872328 loss_ctc 34.960094 lr 0.00129120 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:42,518 DEBUG TRAIN Batch 1/2100 loss 40.887192 loss_att 36.589672 loss_ctc 50.914742 lr 0.00129104 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:42,519 DEBUG TRAIN Batch 1/2100 loss 66.652512 loss_att 61.788231 loss_ctc 78.002510 lr 0.00129120 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:42,519 DEBUG TRAIN Batch 1/2100 loss 29.983143 loss_att 26.779898 loss_ctc 37.457378 lr 0.00129104 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:42,521 DEBUG TRAIN Batch 1/2100 loss 33.755573 loss_att 31.630713 loss_ctc 38.713577 lr 0.00129120 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:05:42,537 DEBUG TRAIN Batch 1/2100 loss 44.406757 loss_att 40.418159 loss_ctc 53.713486 lr 0.00129120 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:08:36,746 DEBUG TRAIN Batch 1/2200 loss 34.100147 loss_att 31.262594 loss_ctc 40.721107 lr 0.00130720 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:08:36,747 DEBUG TRAIN Batch 1/2200 loss 48.306686 loss_att 45.557678 loss_ctc 54.721035 lr 0.00130720 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:08:36,747 DEBUG TRAIN Batch 1/2200 loss 33.117882 loss_att 29.876791 loss_ctc 40.680431 lr 0.00130720 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:08:36,749 DEBUG TRAIN Batch 1/2200 loss 39.688164 loss_att 35.712120 loss_ctc 48.965599 lr 0.00130704 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:08:36,752 DEBUG TRAIN Batch 1/2200 loss 46.322361 loss_att 42.967781 loss_ctc 54.149712 lr 0.00130720 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:08:36,753 DEBUG TRAIN Batch 1/2200 loss 43.629128 loss_att 38.821651 loss_ctc 54.846569 lr 0.00130720 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:08:36,754 DEBUG TRAIN Batch 1/2200 loss 38.957710 loss_att 35.805931 loss_ctc 46.311859 lr 0.00130704 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:08:36,787 DEBUG TRAIN Batch 1/2200 loss 39.802002 loss_att 34.665398 loss_ctc 51.787415 lr 0.00130704 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:11:29,269 DEBUG TRAIN Batch 1/2300 loss 43.758827 loss_att 39.594673 loss_ctc 53.475182 lr 0.00132320 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:11:29,269 DEBUG TRAIN Batch 1/2300 loss 43.717583 loss_att 40.293560 loss_ctc 51.706963 lr 0.00132320 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:11:29,274 DEBUG TRAIN Batch 1/2300 loss 36.283947 loss_att 32.791832 loss_ctc 44.432217 lr 0.00132304 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:11:29,276 DEBUG TRAIN Batch 1/2300 loss 37.036491 loss_att 33.044514 loss_ctc 46.351112 lr 0.00132320 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:11:29,278 DEBUG TRAIN Batch 1/2300 loss 27.420547 loss_att 24.333776 loss_ctc 34.623009 lr 0.00132320 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:11:29,279 DEBUG TRAIN Batch 1/2300 loss 30.659548 loss_att 27.116398 loss_ctc 38.926899 lr 0.00132304 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:11:29,278 DEBUG TRAIN Batch 1/2300 loss 36.014107 loss_att 31.423462 loss_ctc 46.725609 lr 0.00132304 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:11:29,279 DEBUG TRAIN Batch 1/2300 loss 26.384399 loss_att 23.584793 loss_ctc 32.916809 lr 0.00132320 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:14:22,575 DEBUG TRAIN Batch 1/2400 loss 45.414486 loss_att 40.066185 loss_ctc 57.893852 lr 0.00133920 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:14:22,577 DEBUG TRAIN Batch 1/2400 loss 50.091209 loss_att 43.832573 loss_ctc 64.694702 lr 0.00133920 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:14:22,577 DEBUG TRAIN Batch 1/2400 loss 42.020344 loss_att 38.016396 loss_ctc 51.362892 lr 0.00133904 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:14:22,581 DEBUG TRAIN Batch 1/2400 loss 50.212048 loss_att 44.925201 loss_ctc 62.548016 lr 0.00133920 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:14:22,583 DEBUG TRAIN Batch 1/2400 loss 43.319019 loss_att 39.400642 loss_ctc 52.461895 lr 0.00133920 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:14:22,584 DEBUG TRAIN Batch 1/2400 loss 50.013176 loss_att 45.026093 loss_ctc 61.649700 lr 0.00133904 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:14:22,584 DEBUG TRAIN Batch 1/2400 loss 55.674522 loss_att 50.023354 loss_ctc 68.860588 lr 0.00133904 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:14:22,584 DEBUG TRAIN Batch 1/2400 loss 28.803337 loss_att 27.570847 loss_ctc 31.679146 lr 0.00133920 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:20,940 DEBUG TRAIN Batch 1/2500 loss 17.616249 loss_att 15.949018 loss_ctc 21.506458 lr 0.00135504 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:20,941 DEBUG TRAIN Batch 1/2500 loss 20.375214 loss_att 18.210958 loss_ctc 25.425144 lr 0.00135520 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:20,942 DEBUG TRAIN Batch 1/2500 loss 10.027310 loss_att 9.544925 loss_ctc 11.152877 lr 0.00135520 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:20,943 DEBUG TRAIN Batch 1/2500 loss 14.769064 loss_att 13.486214 loss_ctc 17.762379 lr 0.00135504 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:20,944 DEBUG TRAIN Batch 1/2500 loss 19.259327 loss_att 16.965069 loss_ctc 24.612593 lr 0.00135520 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:20,947 DEBUG TRAIN Batch 1/2500 loss 12.492695 loss_att 11.867808 loss_ctc 13.950764 lr 0.00135520 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:20,948 DEBUG TRAIN Batch 1/2500 loss 14.610132 loss_att 12.610931 loss_ctc 19.274933 lr 0.00135504 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:20,969 DEBUG TRAIN Batch 1/2500 loss 13.426954 loss_att 11.769141 loss_ctc 17.295187 lr 0.00135520 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:54,335 DEBUG TRAIN Batch 1/2600 loss 29.150967 loss_att 26.777184 loss_ctc 34.689796 lr 0.00137120 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:54,337 DEBUG TRAIN Batch 1/2600 loss 48.016617 loss_att 41.920513 loss_ctc 62.240852 lr 0.00137104 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:54,343 DEBUG TRAIN Batch 1/2600 loss 34.165371 loss_att 31.474560 loss_ctc 40.443928 lr 0.00137120 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:54,344 DEBUG TRAIN Batch 1/2600 loss 41.176239 loss_att 37.749199 loss_ctc 49.172668 lr 0.00137104 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:54,347 DEBUG TRAIN Batch 1/2600 loss 47.418461 loss_att 43.096848 loss_ctc 57.502220 lr 0.00137104 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:54,348 DEBUG TRAIN Batch 1/2600 loss 33.233833 loss_att 30.602688 loss_ctc 39.373169 lr 0.00137120 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:54,350 DEBUG TRAIN Batch 1/2600 loss 40.115089 loss_att 36.576950 loss_ctc 48.370750 lr 0.00137120 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:17:54,362 DEBUG TRAIN Batch 1/2600 loss 35.185619 loss_att 30.813610 loss_ctc 45.386971 lr 0.00137120 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:20:47,965 DEBUG TRAIN Batch 1/2700 loss 40.229576 loss_att 35.643219 loss_ctc 50.931072 lr 0.00138720 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:20:47,966 DEBUG TRAIN Batch 1/2700 loss 37.256859 loss_att 34.403107 loss_ctc 43.915615 lr 0.00138720 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:20:47,966 DEBUG TRAIN Batch 1/2700 loss 39.202053 loss_att 34.639076 loss_ctc 49.848995 lr 0.00138720 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:20:47,972 DEBUG TRAIN Batch 1/2700 loss 38.840584 loss_att 35.186371 loss_ctc 47.367077 lr 0.00138704 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:20:47,973 DEBUG TRAIN Batch 1/2700 loss 30.142754 loss_att 27.845335 loss_ctc 35.503395 lr 0.00138704 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:20:47,973 DEBUG TRAIN Batch 1/2700 loss 40.532562 loss_att 35.413712 loss_ctc 52.476543 lr 0.00138720 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:20:47,976 DEBUG TRAIN Batch 1/2700 loss 39.313339 loss_att 35.173271 loss_ctc 48.973488 lr 0.00138704 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:20:47,994 DEBUG TRAIN Batch 1/2700 loss 43.108654 loss_att 39.446747 loss_ctc 51.653107 lr 0.00138720 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:23:41,367 DEBUG TRAIN Batch 1/2800 loss 32.887245 loss_att 28.763191 loss_ctc 42.510033 lr 0.00140320 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:23:41,368 DEBUG TRAIN Batch 1/2800 loss 36.198807 loss_att 31.768055 loss_ctc 46.537231 lr 0.00140320 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:23:41,371 DEBUG TRAIN Batch 1/2800 loss 28.374374 loss_att 26.175024 loss_ctc 33.506187 lr 0.00140320 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:23:41,371 DEBUG TRAIN Batch 1/2800 loss 31.913717 loss_att 27.501556 loss_ctc 42.208759 lr 0.00140320 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:23:41,372 DEBUG TRAIN Batch 1/2800 loss 34.048668 loss_att 30.593201 loss_ctc 42.111431 lr 0.00140304 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:23:41,373 DEBUG TRAIN Batch 1/2800 loss 35.562119 loss_att 32.321320 loss_ctc 43.123974 lr 0.00140304 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:23:41,376 DEBUG TRAIN Batch 1/2800 loss 24.830132 loss_att 22.610027 loss_ctc 30.010376 lr 0.00140304 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:23:41,378 DEBUG TRAIN Batch 1/2800 loss 27.438417 loss_att 24.289928 loss_ctc 34.784893 lr 0.00140320 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:26:32,993 DEBUG TRAIN Batch 1/2900 loss 47.782372 loss_att 44.088661 loss_ctc 56.401031 lr 0.00141904 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:26:32,996 DEBUG TRAIN Batch 1/2900 loss 40.398193 loss_att 36.512779 loss_ctc 49.464153 lr 0.00141920 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:26:32,997 DEBUG TRAIN Batch 1/2900 loss 36.273655 loss_att 33.956573 loss_ctc 41.680176 lr 0.00141920 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:26:32,997 DEBUG TRAIN Batch 1/2900 loss 38.727051 loss_att 33.774799 loss_ctc 50.282310 lr 0.00141920 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:26:32,998 DEBUG TRAIN Batch 1/2900 loss 37.780533 loss_att 33.206802 loss_ctc 48.452568 lr 0.00141904 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:26:33,001 DEBUG TRAIN Batch 1/2900 loss 38.977020 loss_att 35.885101 loss_ctc 46.191498 lr 0.00141904 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:26:33,007 DEBUG TRAIN Batch 1/2900 loss 37.034691 loss_att 33.742889 loss_ctc 44.715569 lr 0.00141920 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:26:33,020 DEBUG TRAIN Batch 1/2900 loss 34.214706 loss_att 31.852865 loss_ctc 39.725674 lr 0.00141920 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:25,093 DEBUG TRAIN Batch 1/3000 loss 13.536911 loss_att 12.266706 loss_ctc 16.500721 lr 0.00143504 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:25,096 DEBUG TRAIN Batch 1/3000 loss 13.286500 loss_att 11.860545 loss_ctc 16.613728 lr 0.00143520 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:25,098 DEBUG TRAIN Batch 1/3000 loss 12.699650 loss_att 11.466553 loss_ctc 15.576877 lr 0.00143504 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:25,101 DEBUG TRAIN Batch 1/3000 loss 18.750839 loss_att 16.856312 loss_ctc 23.171400 lr 0.00143520 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:25,102 DEBUG TRAIN Batch 1/3000 loss 28.496889 loss_att 25.268454 loss_ctc 36.029903 lr 0.00143520 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:25,104 DEBUG TRAIN Batch 1/3000 loss 15.167603 loss_att 13.110428 loss_ctc 19.967678 lr 0.00143520 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:25,115 DEBUG TRAIN Batch 1/3000 loss 13.987106 loss_att 11.783802 loss_ctc 19.128151 lr 0.00143520 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:25,118 DEBUG TRAIN Batch 1/3000 loss 19.450287 loss_att 16.553978 loss_ctc 26.208340 lr 0.00143504 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:57,590 DEBUG TRAIN Batch 1/3100 loss 31.646973 loss_att 28.265301 loss_ctc 39.537544 lr 0.00145120 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:57,591 DEBUG TRAIN Batch 1/3100 loss 44.407120 loss_att 38.664902 loss_ctc 57.805626 lr 0.00145120 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:57,592 DEBUG TRAIN Batch 1/3100 loss 32.639668 loss_att 28.249355 loss_ctc 42.883728 lr 0.00145120 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:57,592 DEBUG TRAIN Batch 1/3100 loss 32.610535 loss_att 30.218267 loss_ctc 38.192497 lr 0.00145104 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:57,597 DEBUG TRAIN Batch 1/3100 loss 36.863632 loss_att 33.762974 loss_ctc 44.098495 lr 0.00145104 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:57,599 DEBUG TRAIN Batch 1/3100 loss 38.698166 loss_att 35.055542 loss_ctc 47.197613 lr 0.00145120 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:57,600 DEBUG TRAIN Batch 1/3100 loss 38.863159 loss_att 34.058556 loss_ctc 50.073895 lr 0.00145120 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:29:57,614 DEBUG TRAIN Batch 1/3100 loss 38.308517 loss_att 35.271049 loss_ctc 45.395947 lr 0.00145104 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:32:50,870 DEBUG TRAIN Batch 1/3200 loss 34.479805 loss_att 31.714275 loss_ctc 40.932701 lr 0.00146720 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:32:50,870 DEBUG TRAIN Batch 1/3200 loss 41.037220 loss_att 36.749069 loss_ctc 51.042900 lr 0.00146720 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:32:50,871 DEBUG TRAIN Batch 1/3200 loss 28.490128 loss_att 25.946774 loss_ctc 34.424622 lr 0.00146720 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:32:50,871 DEBUG TRAIN Batch 1/3200 loss 50.425392 loss_att 45.993134 loss_ctc 60.767326 lr 0.00146720 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:32:50,874 DEBUG TRAIN Batch 1/3200 loss 35.506245 loss_att 31.070715 loss_ctc 45.855812 lr 0.00146704 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:32:50,876 DEBUG TRAIN Batch 1/3200 loss 39.595379 loss_att 36.463799 loss_ctc 46.902405 lr 0.00146720 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:32:50,896 DEBUG TRAIN Batch 1/3200 loss 42.997807 loss_att 36.398949 loss_ctc 58.395142 lr 0.00146704 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:32:50,898 DEBUG TRAIN Batch 1/3200 loss 41.407352 loss_att 36.250008 loss_ctc 53.441154 lr 0.00146704 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:35:41,859 DEBUG TRAIN Batch 1/3300 loss 26.456228 loss_att 23.350409 loss_ctc 33.703140 lr 0.00148320 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:35:41,860 DEBUG TRAIN Batch 1/3300 loss 37.568138 loss_att 34.408459 loss_ctc 44.940727 lr 0.00148320 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:35:41,864 DEBUG TRAIN Batch 1/3300 loss 34.201366 loss_att 29.651136 loss_ctc 44.818573 lr 0.00148320 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:35:41,864 DEBUG TRAIN Batch 1/3300 loss 25.334251 loss_att 22.747910 loss_ctc 31.369047 lr 0.00148320 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:35:41,866 DEBUG TRAIN Batch 1/3300 loss 29.196779 loss_att 26.185780 loss_ctc 36.222443 lr 0.00148320 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:35:41,866 DEBUG TRAIN Batch 1/3300 loss 33.699409 loss_att 29.800871 loss_ctc 42.796001 lr 0.00148304 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:35:41,871 DEBUG TRAIN Batch 1/3300 loss 29.326365 loss_att 25.446463 loss_ctc 38.379471 lr 0.00148304 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:35:41,889 DEBUG TRAIN Batch 1/3300 loss 33.579327 loss_att 29.363882 loss_ctc 43.415356 lr 0.00148304 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:38:31,349 DEBUG TRAIN Batch 1/3400 loss 40.746059 loss_att 35.458221 loss_ctc 53.084351 lr 0.00149920 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:38:31,350 DEBUG TRAIN Batch 1/3400 loss 46.855896 loss_att 42.396641 loss_ctc 57.260818 lr 0.00149920 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:38:31,350 DEBUG TRAIN Batch 1/3400 loss 41.615196 loss_att 35.568958 loss_ctc 55.723083 lr 0.00149920 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:38:31,350 DEBUG TRAIN Batch 1/3400 loss 37.195995 loss_att 33.164856 loss_ctc 46.601986 lr 0.00149920 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:38:31,352 DEBUG TRAIN Batch 1/3400 loss 31.354340 loss_att 27.784279 loss_ctc 39.684479 lr 0.00149920 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:38:31,354 DEBUG TRAIN Batch 1/3400 loss 38.295662 loss_att 33.470512 loss_ctc 49.554344 lr 0.00149904 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:38:31,354 DEBUG TRAIN Batch 1/3400 loss 33.928730 loss_att 31.369202 loss_ctc 39.900967 lr 0.00149904 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:38:31,373 DEBUG TRAIN Batch 1/3400 loss 33.255531 loss_att 30.015179 loss_ctc 40.816360 lr 0.00149904 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:23,699 DEBUG TRAIN Batch 1/3500 loss 14.476334 loss_att 12.052475 loss_ctc 20.132004 lr 0.00151504 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:23,701 DEBUG TRAIN Batch 1/3500 loss 14.332685 loss_att 12.804688 loss_ctc 17.898014 lr 0.00151504 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:23,701 DEBUG TRAIN Batch 1/3500 loss 9.391493 loss_att 8.390359 loss_ctc 11.727473 lr 0.00151520 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:23,701 DEBUG TRAIN Batch 1/3500 loss 10.690580 loss_att 9.419277 loss_ctc 13.656956 lr 0.00151520 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:23,701 DEBUG TRAIN Batch 1/3500 loss 12.592731 loss_att 11.796793 loss_ctc 14.449919 lr 0.00151520 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:23,702 DEBUG TRAIN Batch 1/3500 loss 11.289576 loss_att 10.071882 loss_ctc 14.130858 lr 0.00151520 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:23,703 DEBUG TRAIN Batch 1/3500 loss 17.288935 loss_att 14.614722 loss_ctc 23.528761 lr 0.00151504 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:23,720 DEBUG TRAIN Batch 1/3500 loss 12.204659 loss_att 10.891685 loss_ctc 15.268265 lr 0.00151520 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:55,671 DEBUG TRAIN Batch 1/3600 loss 29.655016 loss_att 27.461269 loss_ctc 34.773754 lr 0.00153104 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:55,674 DEBUG TRAIN Batch 1/3600 loss 36.072865 loss_att 31.990515 loss_ctc 45.598343 lr 0.00153120 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:55,674 DEBUG TRAIN Batch 1/3600 loss 36.464642 loss_att 33.173363 loss_ctc 44.144287 lr 0.00153120 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:55,674 DEBUG TRAIN Batch 1/3600 loss 32.823833 loss_att 29.884766 loss_ctc 39.681652 lr 0.00153120 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:55,675 DEBUG TRAIN Batch 1/3600 loss 35.735546 loss_att 31.772198 loss_ctc 44.983364 lr 0.00153104 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:55,681 DEBUG TRAIN Batch 1/3600 loss 51.188217 loss_att 46.098732 loss_ctc 63.063686 lr 0.00153120 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:55,684 DEBUG TRAIN Batch 1/3600 loss 34.763100 loss_att 31.676300 loss_ctc 41.965630 lr 0.00153104 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:41:55,685 DEBUG TRAIN Batch 1/3600 loss 29.215927 loss_att 26.105276 loss_ctc 36.474110 lr 0.00153120 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:44:46,534 DEBUG TRAIN Batch 1/3700 loss 44.951927 loss_att 40.690678 loss_ctc 54.894836 lr 0.00154704 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:44:46,535 DEBUG TRAIN Batch 1/3700 loss 29.409979 loss_att 27.130812 loss_ctc 34.728035 lr 0.00154720 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:44:46,535 DEBUG TRAIN Batch 1/3700 loss 32.269356 loss_att 28.923584 loss_ctc 40.076164 lr 0.00154720 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:44:46,536 DEBUG TRAIN Batch 1/3700 loss 34.814320 loss_att 30.518896 loss_ctc 44.836975 lr 0.00154720 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:44:46,543 DEBUG TRAIN Batch 1/3700 loss 45.191498 loss_att 40.228436 loss_ctc 56.771984 lr 0.00154704 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:44:46,544 DEBUG TRAIN Batch 1/3700 loss 42.629658 loss_att 38.732788 loss_ctc 51.722351 lr 0.00154704 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:44:46,544 DEBUG TRAIN Batch 1/3700 loss 39.062332 loss_att 33.159004 loss_ctc 52.836769 lr 0.00154720 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:44:46,545 DEBUG TRAIN Batch 1/3700 loss 45.203220 loss_att 41.264114 loss_ctc 54.394463 lr 0.00154720 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:47:34,740 DEBUG TRAIN Batch 1/3800 loss 32.502548 loss_att 29.367796 loss_ctc 39.816971 lr 0.00156320 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:47:34,739 DEBUG TRAIN Batch 1/3800 loss 28.672453 loss_att 25.312475 loss_ctc 36.512398 lr 0.00156320 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:47:34,741 DEBUG TRAIN Batch 1/3800 loss 23.820568 loss_att 21.224640 loss_ctc 29.877737 lr 0.00156320 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:47:34,741 DEBUG TRAIN Batch 1/3800 loss 24.217018 loss_att 21.175413 loss_ctc 31.314095 lr 0.00156320 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:47:34,745 DEBUG TRAIN Batch 1/3800 loss 31.952711 loss_att 27.767376 loss_ctc 41.718494 lr 0.00156304 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:47:34,747 DEBUG TRAIN Batch 1/3800 loss 37.773510 loss_att 33.328285 loss_ctc 48.145702 lr 0.00156320 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:47:34,748 DEBUG TRAIN Batch 1/3800 loss 28.884792 loss_att 25.939848 loss_ctc 35.756329 lr 0.00156304 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:47:34,754 DEBUG TRAIN Batch 1/3800 loss 22.877178 loss_att 20.338356 loss_ctc 28.801092 lr 0.00156304 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:50:19,144 DEBUG TRAIN Batch 1/3900 loss 39.676056 loss_att 34.613178 loss_ctc 51.489433 lr 0.00157904 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:50:19,146 DEBUG TRAIN Batch 1/3900 loss 37.177826 loss_att 32.960869 loss_ctc 47.017395 lr 0.00157920 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:50:19,147 DEBUG TRAIN Batch 1/3900 loss 29.626514 loss_att 26.520308 loss_ctc 36.874329 lr 0.00157920 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:50:19,147 DEBUG TRAIN Batch 1/3900 loss 34.372826 loss_att 31.542267 loss_ctc 40.977459 lr 0.00157920 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:50:19,149 DEBUG TRAIN Batch 1/3900 loss 43.607430 loss_att 37.629158 loss_ctc 57.556725 lr 0.00157920 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:50:19,149 DEBUG TRAIN Batch 1/3900 loss 33.279472 loss_att 30.124968 loss_ctc 40.639977 lr 0.00157904 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:50:19,157 DEBUG TRAIN Batch 1/3900 loss 37.331268 loss_att 34.355286 loss_ctc 44.275223 lr 0.00157904 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:50:19,175 DEBUG TRAIN Batch 1/3900 loss 46.656815 loss_att 41.198116 loss_ctc 59.393776 lr 0.00157920 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:05,357 DEBUG TRAIN Batch 1/4000 loss 10.707023 loss_att 9.484677 loss_ctc 13.559161 lr 0.00159520 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:05,357 DEBUG TRAIN Batch 1/4000 loss 13.655568 loss_att 11.637932 loss_ctc 18.363386 lr 0.00159520 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:05,359 DEBUG TRAIN Batch 1/4000 loss 18.690926 loss_att 15.678595 loss_ctc 25.719700 lr 0.00159504 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:05,361 DEBUG TRAIN Batch 1/4000 loss 16.204775 loss_att 14.190996 loss_ctc 20.903593 lr 0.00159520 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:05,362 DEBUG TRAIN Batch 1/4000 loss 11.428357 loss_att 9.811860 loss_ctc 15.200183 lr 0.00159520 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:05,362 DEBUG TRAIN Batch 1/4000 loss 15.410405 loss_att 13.479017 loss_ctc 19.916975 lr 0.00159504 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:05,366 DEBUG TRAIN Batch 1/4000 loss 16.386255 loss_att 14.481806 loss_ctc 20.829967 lr 0.00159504 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:05,377 DEBUG TRAIN Batch 1/4000 loss 13.236719 loss_att 10.411716 loss_ctc 19.828392 lr 0.00159520 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:38,912 DEBUG TRAIN Batch 1/4100 loss 31.139023 loss_att 26.615753 loss_ctc 41.693317 lr 0.00161104 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:38,913 DEBUG TRAIN Batch 1/4100 loss 39.687675 loss_att 34.426910 loss_ctc 51.962791 lr 0.00161120 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:38,914 DEBUG TRAIN Batch 1/4100 loss 34.875919 loss_att 29.995331 loss_ctc 46.263958 lr 0.00161120 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:38,914 DEBUG TRAIN Batch 1/4100 loss 36.126190 loss_att 31.471619 loss_ctc 46.986858 lr 0.00161120 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:38,914 DEBUG TRAIN Batch 1/4100 loss 32.818031 loss_att 29.835629 loss_ctc 39.776974 lr 0.00161120 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:38,916 DEBUG TRAIN Batch 1/4100 loss 44.488113 loss_att 38.541985 loss_ctc 58.362419 lr 0.00161104 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:38,922 DEBUG TRAIN Batch 1/4100 loss 33.079945 loss_att 28.732199 loss_ctc 43.224678 lr 0.00161104 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:53:38,921 DEBUG TRAIN Batch 1/4100 loss 32.212379 loss_att 28.713161 loss_ctc 40.377228 lr 0.00161120 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:56:24,157 DEBUG TRAIN Batch 1/4200 loss 30.636656 loss_att 28.276485 loss_ctc 36.143723 lr 0.00162704 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:56:24,158 DEBUG TRAIN Batch 1/4200 loss 31.763424 loss_att 28.443604 loss_ctc 39.509674 lr 0.00162720 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:56:24,160 DEBUG TRAIN Batch 1/4200 loss 31.094601 loss_att 27.656090 loss_ctc 39.117790 lr 0.00162720 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:56:24,162 DEBUG TRAIN Batch 1/4200 loss 37.420666 loss_att 33.083473 loss_ctc 47.540783 lr 0.00162720 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 03:56:24,167 DEBUG TRAIN Batch 1/4200 loss 35.813290 loss_att 30.970566 loss_ctc 47.112976 lr 0.00162720 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:56:24,169 DEBUG TRAIN Batch 1/4200 loss 34.820297 loss_att 30.810934 loss_ctc 44.175472 lr 0.00162720 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:56:24,170 DEBUG TRAIN Batch 1/4200 loss 31.233713 loss_att 28.101858 loss_ctc 38.541374 lr 0.00162704 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:56:24,188 DEBUG TRAIN Batch 1/4200 loss 45.761433 loss_att 41.365562 loss_ctc 56.018463 lr 0.00162704 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:59:04,384 DEBUG TRAIN Batch 1/4300 loss 30.788254 loss_att 26.675686 loss_ctc 40.384247 lr 0.00164304 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 03:59:04,384 DEBUG TRAIN Batch 1/4300 loss 22.288944 loss_att 19.367920 loss_ctc 29.104668 lr 0.00164320 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 03:59:04,386 DEBUG TRAIN Batch 1/4300 loss 30.639679 loss_att 27.553633 loss_ctc 37.840454 lr 0.00164304 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 03:59:04,389 DEBUG TRAIN Batch 1/4300 loss 24.758991 loss_att 21.898987 loss_ctc 31.432339 lr 0.00164320 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 03:59:04,390 DEBUG TRAIN Batch 1/4300 loss 28.170513 loss_att 25.327377 loss_ctc 34.804497 lr 0.00164320 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 03:59:04,391 DEBUG TRAIN Batch 1/4300 loss 35.932503 loss_att 32.163021 loss_ctc 44.727959 lr 0.00164320 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 03:59:04,392 DEBUG TRAIN Batch 1/4300 loss 21.965115 loss_att 19.478271 loss_ctc 27.767746 lr 0.00164304 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 03:59:04,412 DEBUG TRAIN Batch 1/4300 loss 24.228905 loss_att 21.440701 loss_ctc 30.734715 lr 0.00164320 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 04:04:40,517 DEBUG TRAIN Batch 1/4500 loss 11.676691 loss_att 9.831341 loss_ctc 15.982509 lr 0.00167504 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 04:04:40,521 DEBUG TRAIN Batch 1/4500 loss 13.437765 loss_att 11.808050 loss_ctc 17.240437 lr 0.00167520 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 04:04:40,521 DEBUG TRAIN Batch 1/4500 loss 18.134962 loss_att 15.281893 loss_ctc 24.792122 lr 0.00167504 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 04:04:40,525 DEBUG TRAIN Batch 1/4500 loss 18.677317 loss_att 16.097874 loss_ctc 24.696014 lr 0.00167520 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 04:04:40,525 DEBUG TRAIN Batch 1/4500 loss 11.072668 loss_att 10.092867 loss_ctc 13.358870 lr 0.00167520 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 04:04:40,527 DEBUG TRAIN Batch 1/4500 loss 16.348049 loss_att 15.613616 loss_ctc 18.061726 lr 0.00167520 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 04:04:40,529 DEBUG TRAIN Batch 1/4500 loss 12.305737 loss_att 10.553234 loss_ctc 16.394911 lr 0.00167504 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 04:04:40,551 DEBUG TRAIN Batch 1/4500 loss 14.700639 loss_att 12.838710 loss_ctc 19.045139 lr 0.00167520 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 04:05:13,418 DEBUG TRAIN Batch 1/4600 loss 33.752926 loss_att 29.544537 loss_ctc 43.572502 lr 0.00169120 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 04:05:13,418 DEBUG TRAIN Batch 1/4600 loss 32.633213 loss_att 29.060669 loss_ctc 40.969143 lr 0.00169120 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 04:05:13,419 DEBUG TRAIN Batch 1/4600 loss 28.067852 loss_att 24.252956 loss_ctc 36.969276 lr 0.00169104 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 04:05:13,419 DEBUG TRAIN Batch 1/4600 loss 22.623951 loss_att 19.709805 loss_ctc 29.423626 lr 0.00169120 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 04:05:13,425 DEBUG TRAIN Batch 1/4600 loss 36.497093 loss_att 32.971786 loss_ctc 44.722816 lr 0.00169120 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 04:05:13,426 DEBUG TRAIN Batch 1/4600 loss 31.307825 loss_att 27.492292 loss_ctc 40.210735 lr 0.00169104 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 04:05:13,427 DEBUG TRAIN Batch 1/4600 loss 41.418617 loss_att 35.616463 loss_ctc 54.956982 lr 0.00169104 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 04:05:13,431 DEBUG TRAIN Batch 1/4600 loss 30.386478 loss_att 27.146116 loss_ctc 37.947319 lr 0.00169120 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 04:08:03,946 DEBUG TRAIN Batch 1/4700 loss 34.823467 loss_att 31.584080 loss_ctc 42.382030 lr 0.00170704 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 04:08:03,946 DEBUG TRAIN Batch 1/4700 loss 27.779394 loss_att 24.189693 loss_ctc 36.155361 lr 0.00170720 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 04:08:03,947 DEBUG TRAIN Batch 1/4700 loss 39.725952 loss_att 34.894588 loss_ctc 50.999138 lr 0.00170720 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 04:08:03,949 DEBUG TRAIN Batch 1/4700 loss 34.523010 loss_att 30.109512 loss_ctc 44.821178 lr 0.00170720 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 04:08:03,951 DEBUG TRAIN Batch 1/4700 loss 35.131882 loss_att 31.761183 loss_ctc 42.996841 lr 0.00170704 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 04:08:03,955 DEBUG TRAIN Batch 1/4700 loss 42.096863 loss_att 36.299568 loss_ctc 55.623882 lr 0.00170720 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 04:08:03,956 DEBUG TRAIN Batch 1/4700 loss 28.445700 loss_att 25.033903 loss_ctc 36.406559 lr 0.00170704 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 04:08:03,956 DEBUG TRAIN Batch 1/4700 loss 37.437576 loss_att 33.153419 loss_ctc 47.433945 lr 0.00170720 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:15:23,195 DEBUG TRAIN Batch 2/700 loss 27.977152 loss_att 25.390301 loss_ctc 34.013138 lr 0.00202224 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:15:23,195 DEBUG TRAIN Batch 2/700 loss 27.225933 loss_att 22.748642 loss_ctc 37.672943 lr 0.00202224 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:15:23,195 DEBUG TRAIN Batch 2/700 loss 35.419823 loss_att 30.871344 loss_ctc 46.032940 lr 0.00202224 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:15:23,196 DEBUG TRAIN Batch 2/700 loss 40.871670 loss_att 36.919456 loss_ctc 50.093502 lr 0.00202208 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:15:23,201 DEBUG TRAIN Batch 2/700 loss 40.080341 loss_att 36.633488 loss_ctc 48.123001 lr 0.00202192 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:15:23,203 DEBUG TRAIN Batch 2/700 loss 28.771406 loss_att 25.569485 loss_ctc 36.242554 lr 0.00202208 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:15:23,204 DEBUG TRAIN Batch 2/700 loss 37.983353 loss_att 33.871529 loss_ctc 47.577606 lr 0.00202208 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:15:23,205 DEBUG TRAIN Batch 2/700 loss 39.962006 loss_att 33.700310 loss_ctc 54.572639 lr 0.00202192 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:18:13,455 DEBUG TRAIN Batch 2/800 loss 26.606909 loss_att 22.705770 loss_ctc 35.709564 lr 0.00203792 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:18:13,457 DEBUG TRAIN Batch 2/800 loss 28.221788 loss_att 24.860128 loss_ctc 36.065659 lr 0.00203808 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:18:13,459 DEBUG TRAIN Batch 2/800 loss 21.938725 loss_att 19.093359 loss_ctc 28.577908 lr 0.00203824 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:18:13,466 DEBUG TRAIN Batch 2/800 loss 24.478241 loss_att 22.411993 loss_ctc 29.299490 lr 0.00203792 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:18:13,466 DEBUG TRAIN Batch 2/800 loss 21.306530 loss_att 18.301430 loss_ctc 28.318432 lr 0.00203808 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:18:13,467 DEBUG TRAIN Batch 2/800 loss 34.479420 loss_att 31.065075 loss_ctc 42.446224 lr 0.00203824 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:18:13,468 DEBUG TRAIN Batch 2/800 loss 31.277069 loss_att 28.423203 loss_ctc 37.936089 lr 0.00203808 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:18:13,485 DEBUG TRAIN Batch 2/800 loss 36.671768 loss_att 33.344448 loss_ctc 44.435509 lr 0.00203824 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:20:59,311 DEBUG TRAIN Batch 2/900 loss 30.120104 loss_att 26.322411 loss_ctc 38.981384 lr 0.00205392 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:20:59,312 DEBUG TRAIN Batch 2/900 loss 27.082247 loss_att 23.883720 loss_ctc 34.545479 lr 0.00205408 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:20:59,314 DEBUG TRAIN Batch 2/900 loss 27.991940 loss_att 24.633139 loss_ctc 35.829140 lr 0.00205424 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:20:59,314 DEBUG TRAIN Batch 2/900 loss 30.278872 loss_att 27.380659 loss_ctc 37.041363 lr 0.00205424 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:20:59,314 DEBUG TRAIN Batch 2/900 loss 37.193359 loss_att 32.609356 loss_ctc 47.889370 lr 0.00205392 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:20:59,315 DEBUG TRAIN Batch 2/900 loss 31.386677 loss_att 27.808731 loss_ctc 39.735218 lr 0.00205424 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:20:59,320 DEBUG TRAIN Batch 2/900 loss 38.961311 loss_att 33.693069 loss_ctc 51.253872 lr 0.00205408 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:20:59,321 DEBUG TRAIN Batch 2/900 loss 28.960907 loss_att 26.001135 loss_ctc 35.867043 lr 0.00205408 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:23:45,285 DEBUG TRAIN Batch 2/1000 loss 16.205223 loss_att 14.544455 loss_ctc 20.080347 lr 0.00206992 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:23:45,288 DEBUG TRAIN Batch 2/1000 loss 15.407787 loss_att 13.009527 loss_ctc 21.003727 lr 0.00207024 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:23:45,288 DEBUG TRAIN Batch 2/1000 loss 7.805149 loss_att 6.446247 loss_ctc 10.975920 lr 0.00207024 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:23:45,289 DEBUG TRAIN Batch 2/1000 loss 9.179246 loss_att 8.090296 loss_ctc 11.720129 lr 0.00206992 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:23:45,296 DEBUG TRAIN Batch 2/1000 loss 11.884489 loss_att 10.813628 loss_ctc 14.383165 lr 0.00207008 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:23:45,301 DEBUG TRAIN Batch 2/1000 loss 10.358054 loss_att 8.240385 loss_ctc 15.299282 lr 0.00207008 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:23:45,345 DEBUG TRAIN Batch 2/1000 loss 11.087709 loss_att 9.841824 loss_ctc 13.994774 lr 0.00207024 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:23:45,346 DEBUG TRAIN Batch 2/1000 loss 10.577957 loss_att 9.188770 loss_ctc 13.819393 lr 0.00207008 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:24:20,267 DEBUG TRAIN Batch 2/1100 loss 30.947330 loss_att 27.873037 loss_ctc 38.120682 lr 0.00208608 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:24:20,268 DEBUG TRAIN Batch 2/1100 loss 39.522381 loss_att 35.253490 loss_ctc 49.483124 lr 0.00208624 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:24:20,268 DEBUG TRAIN Batch 2/1100 loss 41.690338 loss_att 37.738464 loss_ctc 50.911373 lr 0.00208624 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:24:20,268 DEBUG TRAIN Batch 2/1100 loss 27.090612 loss_att 24.278082 loss_ctc 33.653183 lr 0.00208592 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:24:20,274 DEBUG TRAIN Batch 2/1100 loss 40.426601 loss_att 35.574718 loss_ctc 51.747662 lr 0.00208592 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:24:20,274 DEBUG TRAIN Batch 2/1100 loss 26.509512 loss_att 23.199249 loss_ctc 34.233459 lr 0.00208608 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:24:20,277 DEBUG TRAIN Batch 2/1100 loss 26.875687 loss_att 23.665564 loss_ctc 34.365967 lr 0.00208608 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:24:20,279 DEBUG TRAIN Batch 2/1100 loss 27.151646 loss_att 23.686749 loss_ctc 35.236404 lr 0.00208624 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:27:06,958 DEBUG TRAIN Batch 2/1200 loss 30.689331 loss_att 26.353806 loss_ctc 40.805557 lr 0.00210192 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:27:06,960 DEBUG TRAIN Batch 2/1200 loss 32.135746 loss_att 28.839245 loss_ctc 39.827579 lr 0.00210208 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:27:06,960 DEBUG TRAIN Batch 2/1200 loss 28.809483 loss_att 26.038441 loss_ctc 35.275246 lr 0.00210224 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:27:06,960 DEBUG TRAIN Batch 2/1200 loss 32.913712 loss_att 29.477627 loss_ctc 40.931244 lr 0.00210224 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:27:06,961 DEBUG TRAIN Batch 2/1200 loss 24.034069 loss_att 21.202127 loss_ctc 30.641937 lr 0.00210224 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:27:06,967 DEBUG TRAIN Batch 2/1200 loss 25.782175 loss_att 21.976309 loss_ctc 34.662529 lr 0.00210208 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:27:06,968 DEBUG TRAIN Batch 2/1200 loss 28.539574 loss_att 24.579052 loss_ctc 37.780792 lr 0.00210208 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:27:06,989 DEBUG TRAIN Batch 2/1200 loss 33.130951 loss_att 29.614487 loss_ctc 41.336025 lr 0.00210192 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:29:51,821 DEBUG TRAIN Batch 2/1300 loss 25.787785 loss_att 23.701473 loss_ctc 30.655851 lr 0.00211792 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:29:51,823 DEBUG TRAIN Batch 2/1300 loss 27.441896 loss_att 23.874180 loss_ctc 35.766567 lr 0.00211808 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:29:51,830 DEBUG TRAIN Batch 2/1300 loss 31.093863 loss_att 27.203970 loss_ctc 40.170280 lr 0.00211808 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:29:51,831 DEBUG TRAIN Batch 2/1300 loss 32.219204 loss_att 28.073898 loss_ctc 41.891586 lr 0.00211792 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:29:51,832 DEBUG TRAIN Batch 2/1300 loss 31.010355 loss_att 27.609875 loss_ctc 38.944809 lr 0.00211808 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:29:51,835 DEBUG TRAIN Batch 2/1300 loss 21.295601 loss_att 19.457764 loss_ctc 25.583889 lr 0.00211824 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:29:51,835 DEBUG TRAIN Batch 2/1300 loss 28.240360 loss_att 23.482861 loss_ctc 39.341194 lr 0.00211824 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:29:51,836 DEBUG TRAIN Batch 2/1300 loss 28.401730 loss_att 24.910641 loss_ctc 36.547604 lr 0.00211824 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:32:41,445 DEBUG TRAIN Batch 2/1400 loss 36.467342 loss_att 31.366385 loss_ctc 48.369579 lr 0.00213424 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:32:41,446 DEBUG TRAIN Batch 2/1400 loss 29.820518 loss_att 27.456997 loss_ctc 35.335396 lr 0.00213424 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:32:41,446 DEBUG TRAIN Batch 2/1400 loss 31.700367 loss_att 27.375105 loss_ctc 41.792645 lr 0.00213408 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:32:41,447 DEBUG TRAIN Batch 2/1400 loss 33.207111 loss_att 28.804180 loss_ctc 43.480621 lr 0.00213392 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:32:41,448 DEBUG TRAIN Batch 2/1400 loss 26.937202 loss_att 24.008514 loss_ctc 33.770809 lr 0.00213408 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:32:41,450 DEBUG TRAIN Batch 2/1400 loss 44.770676 loss_att 39.638309 loss_ctc 56.746197 lr 0.00213424 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:32:41,451 DEBUG TRAIN Batch 2/1400 loss 35.681801 loss_att 32.462536 loss_ctc 43.193428 lr 0.00213408 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:32:41,455 DEBUG TRAIN Batch 2/1400 loss 30.015617 loss_att 26.761908 loss_ctc 37.607601 lr 0.00213392 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:35:28,034 DEBUG TRAIN Batch 2/1500 loss 16.825251 loss_att 14.785360 loss_ctc 21.584995 lr 0.00215008 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:35:28,035 DEBUG TRAIN Batch 2/1500 loss 13.936208 loss_att 12.112006 loss_ctc 18.192678 lr 0.00215024 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:35:28,036 DEBUG TRAIN Batch 2/1500 loss 15.758052 loss_att 14.260969 loss_ctc 19.251244 lr 0.00214992 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:35:28,042 DEBUG TRAIN Batch 2/1500 loss 16.492218 loss_att 14.974597 loss_ctc 20.033337 lr 0.00215008 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:35:28,049 DEBUG TRAIN Batch 2/1500 loss 11.074587 loss_att 10.633375 loss_ctc 12.104080 lr 0.00215024 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:35:28,067 DEBUG TRAIN Batch 2/1500 loss 12.928698 loss_att 11.145338 loss_ctc 17.089869 lr 0.00215008 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:35:28,077 DEBUG TRAIN Batch 2/1500 loss 11.848846 loss_att 10.172851 loss_ctc 15.759501 lr 0.00214992 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:35:28,080 DEBUG TRAIN Batch 2/1500 loss 11.152504 loss_att 9.226707 loss_ctc 15.646029 lr 0.00215024 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:36:01,262 DEBUG TRAIN Batch 2/1600 loss 35.958099 loss_att 31.045683 loss_ctc 47.420403 lr 0.00216592 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:36:01,270 DEBUG TRAIN Batch 2/1600 loss 24.469965 loss_att 22.457243 loss_ctc 29.166317 lr 0.00216608 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:36:01,270 DEBUG TRAIN Batch 2/1600 loss 30.220924 loss_att 27.239407 loss_ctc 37.177795 lr 0.00216624 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:36:01,271 DEBUG TRAIN Batch 2/1600 loss 34.060905 loss_att 28.703491 loss_ctc 46.561535 lr 0.00216608 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:36:01,271 DEBUG TRAIN Batch 2/1600 loss 18.561863 loss_att 16.295132 loss_ctc 23.850903 lr 0.00216624 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:36:01,271 DEBUG TRAIN Batch 2/1600 loss 33.161922 loss_att 28.426769 loss_ctc 44.210613 lr 0.00216592 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:36:01,271 DEBUG TRAIN Batch 2/1600 loss 38.936321 loss_att 34.921028 loss_ctc 48.305340 lr 0.00216624 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:36:01,273 DEBUG TRAIN Batch 2/1600 loss 27.172935 loss_att 24.255051 loss_ctc 33.981331 lr 0.00216608 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:39:07,037 DEBUG TRAIN Batch 2/1700 loss 29.701164 loss_att 25.802612 loss_ctc 38.797783 lr 0.00218224 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:39:07,037 DEBUG TRAIN Batch 2/1700 loss 33.509338 loss_att 28.783155 loss_ctc 44.537094 lr 0.00218208 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:39:07,038 DEBUG TRAIN Batch 2/1700 loss 30.531912 loss_att 27.318981 loss_ctc 38.028748 lr 0.00218192 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:39:07,038 DEBUG TRAIN Batch 2/1700 loss 31.309429 loss_att 28.398224 loss_ctc 38.102245 lr 0.00218224 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:39:07,041 DEBUG TRAIN Batch 2/1700 loss 29.571426 loss_att 24.807463 loss_ctc 40.687340 lr 0.00218192 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:39:07,044 DEBUG TRAIN Batch 2/1700 loss 28.120399 loss_att 23.948080 loss_ctc 37.855812 lr 0.00218224 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:39:07,047 DEBUG TRAIN Batch 2/1700 loss 37.839012 loss_att 32.651058 loss_ctc 49.944237 lr 0.00218208 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:39:07,051 DEBUG TRAIN Batch 2/1700 loss 29.024578 loss_att 24.668325 loss_ctc 39.189163 lr 0.00218208 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:41:51,904 DEBUG TRAIN Batch 2/1800 loss 26.451313 loss_att 23.237843 loss_ctc 33.949409 lr 0.00219808 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:41:51,906 DEBUG TRAIN Batch 2/1800 loss 28.259659 loss_att 24.078327 loss_ctc 38.016098 lr 0.00219824 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:41:51,906 DEBUG TRAIN Batch 2/1800 loss 27.972565 loss_att 24.369747 loss_ctc 36.379135 lr 0.00219792 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:41:51,908 DEBUG TRAIN Batch 2/1800 loss 28.410282 loss_att 25.210232 loss_ctc 35.877068 lr 0.00219824 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:41:51,913 DEBUG TRAIN Batch 2/1800 loss 23.604015 loss_att 21.155779 loss_ctc 29.316566 lr 0.00219808 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:41:51,913 DEBUG TRAIN Batch 2/1800 loss 34.094669 loss_att 29.480030 loss_ctc 44.862164 lr 0.00219808 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:41:51,914 DEBUG TRAIN Batch 2/1800 loss 28.870285 loss_att 25.007435 loss_ctc 37.883606 lr 0.00219792 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:41:51,917 DEBUG TRAIN Batch 2/1800 loss 28.792843 loss_att 25.088242 loss_ctc 37.436913 lr 0.00219824 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:44:35,148 DEBUG TRAIN Batch 2/1900 loss 22.285290 loss_att 20.739765 loss_ctc 25.891514 lr 0.00221392 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:44:35,149 DEBUG TRAIN Batch 2/1900 loss 29.015118 loss_att 25.602734 loss_ctc 36.977348 lr 0.00221408 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:44:35,151 DEBUG TRAIN Batch 2/1900 loss 27.041241 loss_att 23.853653 loss_ctc 34.478939 lr 0.00221424 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:44:35,153 DEBUG TRAIN Batch 2/1900 loss 29.902058 loss_att 25.468197 loss_ctc 40.247726 lr 0.00221408 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:44:35,157 DEBUG TRAIN Batch 2/1900 loss 30.529823 loss_att 27.077770 loss_ctc 38.584614 lr 0.00221408 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:44:35,157 DEBUG TRAIN Batch 2/1900 loss 29.625063 loss_att 25.759069 loss_ctc 38.645710 lr 0.00221424 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:44:35,158 DEBUG TRAIN Batch 2/1900 loss 32.426952 loss_att 27.232983 loss_ctc 44.546215 lr 0.00221424 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:44:35,159 DEBUG TRAIN Batch 2/1900 loss 29.239979 loss_att 26.981945 loss_ctc 34.508720 lr 0.00221392 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:18,671 DEBUG TRAIN Batch 2/2000 loss 13.607908 loss_att 11.718222 loss_ctc 18.017178 lr 0.00223008 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:18,672 DEBUG TRAIN Batch 2/2000 loss 14.130601 loss_att 12.238914 loss_ctc 18.544537 lr 0.00223024 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:18,673 DEBUG TRAIN Batch 2/2000 loss 18.012110 loss_att 15.082470 loss_ctc 24.847937 lr 0.00223024 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:18,680 DEBUG TRAIN Batch 2/2000 loss 12.590401 loss_att 10.168639 loss_ctc 18.241177 lr 0.00223008 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:18,681 DEBUG TRAIN Batch 2/2000 loss 10.749496 loss_att 9.243766 loss_ctc 14.262870 lr 0.00223008 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:18,682 DEBUG TRAIN Batch 2/2000 loss 15.900702 loss_att 13.418564 loss_ctc 21.692352 lr 0.00223024 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:18,690 DEBUG TRAIN Batch 2/2000 loss 13.326693 loss_att 11.364085 loss_ctc 17.906109 lr 0.00222992 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:18,699 DEBUG TRAIN Batch 2/2000 loss 19.034603 loss_att 16.844206 loss_ctc 24.145529 lr 0.00222992 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:51,610 DEBUG TRAIN Batch 2/2100 loss 31.707850 loss_att 28.670467 loss_ctc 38.795074 lr 0.00224608 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:51,610 DEBUG TRAIN Batch 2/2100 loss 36.097084 loss_att 32.061172 loss_ctc 45.514214 lr 0.00224624 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:51,611 DEBUG TRAIN Batch 2/2100 loss 28.157028 loss_att 25.548309 loss_ctc 34.244041 lr 0.00224624 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:51,616 DEBUG TRAIN Batch 2/2100 loss 34.716923 loss_att 30.533278 loss_ctc 44.478756 lr 0.00224592 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:51,620 DEBUG TRAIN Batch 2/2100 loss 27.053017 loss_att 22.487442 loss_ctc 37.706024 lr 0.00224608 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:51,619 DEBUG TRAIN Batch 2/2100 loss 27.554153 loss_att 24.079992 loss_ctc 35.660522 lr 0.00224624 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:51,621 DEBUG TRAIN Batch 2/2100 loss 33.438660 loss_att 29.991943 loss_ctc 41.480995 lr 0.00224592 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:47:51,623 DEBUG TRAIN Batch 2/2100 loss 26.892151 loss_att 23.152243 loss_ctc 35.618603 lr 0.00224608 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:50:34,722 DEBUG TRAIN Batch 2/2200 loss 37.723431 loss_att 32.780838 loss_ctc 49.256149 lr 0.00226192 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:50:34,728 DEBUG TRAIN Batch 2/2200 loss 29.221947 loss_att 25.992239 loss_ctc 36.757931 lr 0.00226192 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:50:34,728 DEBUG TRAIN Batch 2/2200 loss 26.255463 loss_att 23.058378 loss_ctc 33.715321 lr 0.00226224 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:50:34,728 DEBUG TRAIN Batch 2/2200 loss 30.892647 loss_att 26.457844 loss_ctc 41.240517 lr 0.00226208 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:50:34,730 DEBUG TRAIN Batch 2/2200 loss 29.525299 loss_att 26.065319 loss_ctc 37.598587 lr 0.00226208 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:50:34,730 DEBUG TRAIN Batch 2/2200 loss 26.695438 loss_att 26.037140 loss_ctc 28.231466 lr 0.00226208 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:50:34,731 DEBUG TRAIN Batch 2/2200 loss 28.159203 loss_att 24.949142 loss_ctc 35.649345 lr 0.00226224 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:50:34,737 DEBUG TRAIN Batch 2/2200 loss 29.854481 loss_att 25.811249 loss_ctc 39.288692 lr 0.00226224 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:53:15,827 DEBUG TRAIN Batch 2/2300 loss 25.618334 loss_att 22.084814 loss_ctc 33.863213 lr 0.00227824 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:53:15,828 DEBUG TRAIN Batch 2/2300 loss 35.425140 loss_att 31.798803 loss_ctc 43.886589 lr 0.00227808 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:53:15,829 DEBUG TRAIN Batch 2/2300 loss 31.075733 loss_att 27.506409 loss_ctc 39.404156 lr 0.00227824 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:53:15,829 DEBUG TRAIN Batch 2/2300 loss 26.027790 loss_att 22.774162 loss_ctc 33.619583 lr 0.00227792 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:53:15,836 DEBUG TRAIN Batch 2/2300 loss 18.352434 loss_att 15.884593 loss_ctc 24.110731 lr 0.00227824 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:53:15,838 DEBUG TRAIN Batch 2/2300 loss 30.779154 loss_att 26.647099 loss_ctc 40.420612 lr 0.00227808 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:53:15,839 DEBUG TRAIN Batch 2/2300 loss 29.600925 loss_att 25.924919 loss_ctc 38.178272 lr 0.00227808 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:53:15,847 DEBUG TRAIN Batch 2/2300 loss 22.584753 loss_att 19.563934 loss_ctc 29.633331 lr 0.00227792 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:55:55,361 DEBUG TRAIN Batch 2/2400 loss 38.640633 loss_att 33.470627 loss_ctc 50.703979 lr 0.00229392 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:55:55,365 DEBUG TRAIN Batch 2/2400 loss 35.019474 loss_att 31.606268 loss_ctc 42.983620 lr 0.00229392 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:55:55,366 DEBUG TRAIN Batch 2/2400 loss 33.279144 loss_att 31.183147 loss_ctc 38.169807 lr 0.00229424 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:55:55,371 DEBUG TRAIN Batch 2/2400 loss 31.226593 loss_att 27.138313 loss_ctc 40.765915 lr 0.00229408 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:55:55,371 DEBUG TRAIN Batch 2/2400 loss 27.586958 loss_att 25.153320 loss_ctc 33.265442 lr 0.00229424 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:55:55,373 DEBUG TRAIN Batch 2/2400 loss 28.628241 loss_att 25.345755 loss_ctc 36.287376 lr 0.00229408 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:55:55,374 DEBUG TRAIN Batch 2/2400 loss 36.182755 loss_att 30.745590 loss_ctc 48.869473 lr 0.00229408 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:55:55,391 DEBUG TRAIN Batch 2/2400 loss 30.207008 loss_att 25.691320 loss_ctc 40.743614 lr 0.00229424 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:58:34,859 DEBUG TRAIN Batch 2/2500 loss 16.912996 loss_att 14.865498 loss_ctc 21.690491 lr 0.00231008 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:58:34,860 DEBUG TRAIN Batch 2/2500 loss 10.625891 loss_att 9.402730 loss_ctc 13.479934 lr 0.00231024 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:58:34,865 DEBUG TRAIN Batch 2/2500 loss 15.491737 loss_att 13.628941 loss_ctc 19.838263 lr 0.00230992 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:58:34,865 DEBUG TRAIN Batch 2/2500 loss 13.455685 loss_att 11.762745 loss_ctc 17.405876 lr 0.00231024 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:58:34,868 DEBUG TRAIN Batch 2/2500 loss 19.023865 loss_att 16.501076 loss_ctc 24.910374 lr 0.00231008 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 05:58:34,868 DEBUG TRAIN Batch 2/2500 loss 13.587866 loss_att 12.528041 loss_ctc 16.060791 lr 0.00230992 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:58:34,870 DEBUG TRAIN Batch 2/2500 loss 15.640980 loss_att 13.259924 loss_ctc 21.196777 lr 0.00231008 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:58:34,881 DEBUG TRAIN Batch 2/2500 loss 13.165400 loss_att 11.738663 loss_ctc 16.494453 lr 0.00231024 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:59:08,621 DEBUG TRAIN Batch 2/2600 loss 35.699318 loss_att 31.830357 loss_ctc 44.726891 lr 0.00232624 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 05:59:08,622 DEBUG TRAIN Batch 2/2600 loss 26.148525 loss_att 23.171619 loss_ctc 33.094643 lr 0.00232608 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 05:59:08,623 DEBUG TRAIN Batch 2/2600 loss 28.729523 loss_att 25.130783 loss_ctc 37.126587 lr 0.00232624 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 05:59:08,625 DEBUG TRAIN Batch 2/2600 loss 27.092276 loss_att 23.352581 loss_ctc 35.818233 lr 0.00232592 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 05:59:08,628 DEBUG TRAIN Batch 2/2600 loss 34.406410 loss_att 31.232952 loss_ctc 41.811150 lr 0.00232624 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 05:59:08,629 DEBUG TRAIN Batch 2/2600 loss 24.809372 loss_att 21.976273 loss_ctc 31.419935 lr 0.00232608 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 05:59:08,630 DEBUG TRAIN Batch 2/2600 loss 26.976135 loss_att 23.664640 loss_ctc 34.702953 lr 0.00232592 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 05:59:08,633 DEBUG TRAIN Batch 2/2600 loss 30.512939 loss_att 26.106358 loss_ctc 40.794960 lr 0.00232608 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:01:47,536 DEBUG TRAIN Batch 2/2700 loss 27.226957 loss_att 23.725584 loss_ctc 35.396828 lr 0.00234192 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:01:47,537 DEBUG TRAIN Batch 2/2700 loss 30.392498 loss_att 25.716900 loss_ctc 41.302223 lr 0.00234224 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:01:47,538 DEBUG TRAIN Batch 2/2700 loss 21.840155 loss_att 20.322905 loss_ctc 25.380405 lr 0.00234224 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:01:47,538 DEBUG TRAIN Batch 2/2700 loss 27.542236 loss_att 24.800045 loss_ctc 33.940681 lr 0.00234192 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:01:47,542 DEBUG TRAIN Batch 2/2700 loss 22.575035 loss_att 19.539461 loss_ctc 29.658035 lr 0.00234224 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:01:47,544 DEBUG TRAIN Batch 2/2700 loss 33.995361 loss_att 28.974819 loss_ctc 45.709953 lr 0.00234208 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:01:47,547 DEBUG TRAIN Batch 2/2700 loss 19.765562 loss_att 17.585917 loss_ctc 24.851402 lr 0.00234208 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:01:47,563 DEBUG TRAIN Batch 2/2700 loss 26.434803 loss_att 23.778189 loss_ctc 32.633568 lr 0.00234208 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:04:32,049 DEBUG TRAIN Batch 2/2800 loss 20.530155 loss_att 18.640446 loss_ctc 24.939474 lr 0.00235808 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:04:32,050 DEBUG TRAIN Batch 2/2800 loss 29.311096 loss_att 25.572613 loss_ctc 38.034225 lr 0.00235792 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:04:32,058 DEBUG TRAIN Batch 2/2800 loss 28.138660 loss_att 24.882589 loss_ctc 35.736156 lr 0.00235824 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:04:32,059 DEBUG TRAIN Batch 2/2800 loss 22.467690 loss_att 19.228344 loss_ctc 30.026161 lr 0.00235808 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:04:32,059 DEBUG TRAIN Batch 2/2800 loss 26.643393 loss_att 23.979141 loss_ctc 32.859978 lr 0.00235824 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:04:32,059 DEBUG TRAIN Batch 2/2800 loss 21.919304 loss_att 19.521093 loss_ctc 27.515127 lr 0.00235824 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:04:32,060 DEBUG TRAIN Batch 2/2800 loss 31.589935 loss_att 28.384245 loss_ctc 39.069878 lr 0.00235792 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:04:32,061 DEBUG TRAIN Batch 2/2800 loss 20.688660 loss_att 17.968397 loss_ctc 27.035940 lr 0.00235808 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:07:16,128 DEBUG TRAIN Batch 2/2900 loss 33.423588 loss_att 29.638060 loss_ctc 42.256489 lr 0.00237408 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:07:16,128 DEBUG TRAIN Batch 2/2900 loss 31.305696 loss_att 27.638443 loss_ctc 39.862617 lr 0.00237424 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:07:16,129 DEBUG TRAIN Batch 2/2900 loss 27.799416 loss_att 24.432837 loss_ctc 35.654770 lr 0.00237424 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:07:16,129 DEBUG TRAIN Batch 2/2900 loss 39.068241 loss_att 34.790543 loss_ctc 49.049538 lr 0.00237392 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:07:16,136 DEBUG TRAIN Batch 2/2900 loss 32.360039 loss_att 28.001715 loss_ctc 42.529457 lr 0.00237424 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:07:16,137 DEBUG TRAIN Batch 2/2900 loss 36.857067 loss_att 32.393326 loss_ctc 47.272461 lr 0.00237392 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:07:16,138 DEBUG TRAIN Batch 2/2900 loss 31.978333 loss_att 27.959846 loss_ctc 41.354805 lr 0.00237408 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:07:16,140 DEBUG TRAIN Batch 2/2900 loss 24.036434 loss_att 21.123886 loss_ctc 30.832382 lr 0.00237408 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:00,746 DEBUG TRAIN Batch 2/3000 loss 13.593562 loss_att 11.933573 loss_ctc 17.466871 lr 0.00239024 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:00,747 DEBUG TRAIN Batch 2/3000 loss 11.048960 loss_att 9.998461 loss_ctc 13.500122 lr 0.00239024 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:00,747 DEBUG TRAIN Batch 2/3000 loss 10.114098 loss_att 8.550943 loss_ctc 13.761459 lr 0.00238992 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:00,752 DEBUG TRAIN Batch 2/3000 loss 15.340292 loss_att 13.183434 loss_ctc 20.372961 lr 0.00238992 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:00,752 DEBUG TRAIN Batch 2/3000 loss 11.414729 loss_att 10.034555 loss_ctc 14.635135 lr 0.00239008 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:00,757 DEBUG TRAIN Batch 2/3000 loss 9.910050 loss_att 8.608279 loss_ctc 12.947515 lr 0.00239024 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:00,760 DEBUG TRAIN Batch 2/3000 loss 11.386121 loss_att 9.327996 loss_ctc 16.188412 lr 0.00239008 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:00,766 DEBUG TRAIN Batch 2/3000 loss 10.680315 loss_att 9.188225 loss_ctc 14.161856 lr 0.00239008 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:33,694 DEBUG TRAIN Batch 2/3100 loss 35.615360 loss_att 29.953712 loss_ctc 48.825874 lr 0.00240608 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:33,695 DEBUG TRAIN Batch 2/3100 loss 33.058899 loss_att 28.033819 loss_ctc 44.784081 lr 0.00240624 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:33,696 DEBUG TRAIN Batch 2/3100 loss 26.270529 loss_att 22.375126 loss_ctc 35.359802 lr 0.00240592 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:33,704 DEBUG TRAIN Batch 2/3100 loss 23.266739 loss_att 21.011051 loss_ctc 28.530006 lr 0.00240608 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:33,704 DEBUG TRAIN Batch 2/3100 loss 26.996256 loss_att 23.100943 loss_ctc 36.085320 lr 0.00240608 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:33,704 DEBUG TRAIN Batch 2/3100 loss 27.540335 loss_att 24.012852 loss_ctc 35.771126 lr 0.00240624 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:33,705 DEBUG TRAIN Batch 2/3100 loss 28.153879 loss_att 24.832302 loss_ctc 35.904228 lr 0.00240624 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:10:33,705 DEBUG TRAIN Batch 2/3100 loss 25.899708 loss_att 22.108717 loss_ctc 34.745354 lr 0.00240592 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:13:17,209 DEBUG TRAIN Batch 2/3200 loss 30.488846 loss_att 27.230625 loss_ctc 38.091362 lr 0.00242208 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:13:17,211 DEBUG TRAIN Batch 2/3200 loss 25.993031 loss_att 23.012440 loss_ctc 32.947746 lr 0.00242192 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:13:17,216 DEBUG TRAIN Batch 2/3200 loss 46.628258 loss_att 40.184464 loss_ctc 61.663780 lr 0.00242224 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:13:17,218 DEBUG TRAIN Batch 2/3200 loss 29.735907 loss_att 24.918961 loss_ctc 40.975449 lr 0.00242208 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:13:17,218 DEBUG TRAIN Batch 2/3200 loss 27.561028 loss_att 24.594191 loss_ctc 34.483650 lr 0.00242224 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:13:17,219 DEBUG TRAIN Batch 2/3200 loss 19.094412 loss_att 17.493008 loss_ctc 22.831020 lr 0.00242208 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:13:17,238 DEBUG TRAIN Batch 2/3200 loss 25.343193 loss_att 22.754070 loss_ctc 31.384476 lr 0.00242224 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:13:17,238 DEBUG TRAIN Batch 2/3200 loss 29.063795 loss_att 26.201954 loss_ctc 35.741425 lr 0.00242192 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:15:59,032 DEBUG TRAIN Batch 2/3300 loss 26.871586 loss_att 23.751022 loss_ctc 34.152901 lr 0.00243808 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:15:59,034 DEBUG TRAIN Batch 2/3300 loss 21.391779 loss_att 17.723167 loss_ctc 29.951872 lr 0.00243824 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:15:59,039 DEBUG TRAIN Batch 2/3300 loss 29.460684 loss_att 24.582565 loss_ctc 40.842957 lr 0.00243792 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:15:59,041 DEBUG TRAIN Batch 2/3300 loss 27.808743 loss_att 24.651192 loss_ctc 35.176361 lr 0.00243808 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:15:59,042 DEBUG TRAIN Batch 2/3300 loss 24.896523 loss_att 21.044720 loss_ctc 33.884068 lr 0.00243824 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:15:59,043 DEBUG TRAIN Batch 2/3300 loss 22.693375 loss_att 19.536388 loss_ctc 30.059675 lr 0.00243824 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:15:59,043 DEBUG TRAIN Batch 2/3300 loss 13.473080 loss_att 12.186660 loss_ctc 16.474728 lr 0.00243792 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:15:59,059 DEBUG TRAIN Batch 2/3300 loss 27.742966 loss_att 24.205502 loss_ctc 35.997047 lr 0.00243808 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:18:40,521 DEBUG TRAIN Batch 2/3400 loss 27.668859 loss_att 24.728201 loss_ctc 34.530396 lr 0.00245392 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:18:40,522 DEBUG TRAIN Batch 2/3400 loss 23.390993 loss_att 20.982292 loss_ctc 29.011295 lr 0.00245408 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:18:40,522 DEBUG TRAIN Batch 2/3400 loss 24.442966 loss_att 20.704449 loss_ctc 33.166176 lr 0.00245408 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:18:40,523 DEBUG TRAIN Batch 2/3400 loss 30.346537 loss_att 27.043228 loss_ctc 38.054256 lr 0.00245424 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:18:40,523 DEBUG TRAIN Batch 2/3400 loss 29.998249 loss_att 25.184774 loss_ctc 41.229687 lr 0.00245424 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:18:40,529 DEBUG TRAIN Batch 2/3400 loss 28.757511 loss_att 24.463745 loss_ctc 38.776302 lr 0.00245392 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:18:40,531 DEBUG TRAIN Batch 2/3400 loss 28.467669 loss_att 25.389706 loss_ctc 35.649586 lr 0.00245408 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:18:40,532 DEBUG TRAIN Batch 2/3400 loss 36.636028 loss_att 32.891418 loss_ctc 45.373451 lr 0.00245424 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:19,907 DEBUG TRAIN Batch 2/3500 loss 10.119624 loss_att 9.051977 loss_ctc 12.610802 lr 0.00247008 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:19,915 DEBUG TRAIN Batch 2/3500 loss 8.773001 loss_att 7.353487 loss_ctc 12.085201 lr 0.00247024 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:19,926 DEBUG TRAIN Batch 2/3500 loss 17.199474 loss_att 13.818983 loss_ctc 25.087284 lr 0.00247024 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:19,928 DEBUG TRAIN Batch 2/3500 loss 12.623996 loss_att 10.778903 loss_ctc 16.929213 lr 0.00246992 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:19,930 DEBUG TRAIN Batch 2/3500 loss 15.995140 loss_att 13.928488 loss_ctc 20.817331 lr 0.00247008 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:19,931 DEBUG TRAIN Batch 2/3500 loss 9.325008 loss_att 7.938998 loss_ctc 12.559032 lr 0.00247024 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:19,937 DEBUG TRAIN Batch 2/3500 loss 16.322102 loss_att 14.891169 loss_ctc 19.660946 lr 0.00246992 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:19,942 DEBUG TRAIN Batch 2/3500 loss 13.725174 loss_att 11.342903 loss_ctc 19.283806 lr 0.00247008 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:53,224 DEBUG TRAIN Batch 2/3600 loss 30.861473 loss_att 27.415922 loss_ctc 38.901089 lr 0.00248592 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:53,226 DEBUG TRAIN Batch 2/3600 loss 26.847954 loss_att 23.012318 loss_ctc 35.797771 lr 0.00248608 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:53,227 DEBUG TRAIN Batch 2/3600 loss 36.038727 loss_att 30.501835 loss_ctc 48.958145 lr 0.00248624 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:53,229 DEBUG TRAIN Batch 2/3600 loss 25.562950 loss_att 22.394478 loss_ctc 32.956055 lr 0.00248608 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:53,234 DEBUG TRAIN Batch 2/3600 loss 30.817543 loss_att 28.345634 loss_ctc 36.585327 lr 0.00248592 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:53,234 DEBUG TRAIN Batch 2/3600 loss 38.668568 loss_att 34.468071 loss_ctc 48.469727 lr 0.00248608 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:53,236 DEBUG TRAIN Batch 2/3600 loss 35.205215 loss_att 30.683647 loss_ctc 45.755547 lr 0.00248624 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:21:53,257 DEBUG TRAIN Batch 2/3600 loss 23.340321 loss_att 20.691162 loss_ctc 29.521694 lr 0.00248624 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:24:37,505 DEBUG TRAIN Batch 2/3700 loss 21.791674 loss_att 19.415352 loss_ctc 27.336426 lr 0.00250208 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:24:37,505 DEBUG TRAIN Batch 2/3700 loss 28.320667 loss_att 24.009270 loss_ctc 38.380596 lr 0.00250208 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:24:37,505 DEBUG TRAIN Batch 2/3700 loss 33.455891 loss_att 28.870108 loss_ctc 44.156048 lr 0.00250224 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:24:37,512 DEBUG TRAIN Batch 2/3700 loss 25.594090 loss_att 22.329597 loss_ctc 33.211235 lr 0.00250208 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:24:37,512 DEBUG TRAIN Batch 2/3700 loss 32.671055 loss_att 28.241180 loss_ctc 43.007427 lr 0.00250192 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:24:37,514 DEBUG TRAIN Batch 2/3700 loss 26.920959 loss_att 24.087589 loss_ctc 33.532150 lr 0.00250224 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:24:37,516 DEBUG TRAIN Batch 2/3700 loss 36.098591 loss_att 29.669165 loss_ctc 51.100586 lr 0.00250192 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:24:37,517 DEBUG TRAIN Batch 2/3700 loss 30.562740 loss_att 25.745182 loss_ctc 41.803703 lr 0.00250224 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:27:24,425 DEBUG TRAIN Batch 2/3800 loss 24.940029 loss_att 20.193813 loss_ctc 36.014530 lr 0.00251808 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:27:24,428 DEBUG TRAIN Batch 2/3800 loss 21.568348 loss_att 18.849613 loss_ctc 27.912064 lr 0.00251808 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:27:24,431 DEBUG TRAIN Batch 2/3800 loss 20.634436 loss_att 17.001547 loss_ctc 29.111174 lr 0.00251824 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:27:24,433 DEBUG TRAIN Batch 2/3800 loss 26.724434 loss_att 22.920874 loss_ctc 35.599407 lr 0.00251824 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:27:24,432 DEBUG TRAIN Batch 2/3800 loss 28.387506 loss_att 23.844687 loss_ctc 38.987419 lr 0.00251792 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:27:24,433 DEBUG TRAIN Batch 2/3800 loss 19.200754 loss_att 17.235058 loss_ctc 23.787380 lr 0.00251792 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:27:24,435 DEBUG TRAIN Batch 2/3800 loss 19.954235 loss_att 17.898178 loss_ctc 24.751699 lr 0.00251824 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:27:24,436 DEBUG TRAIN Batch 2/3800 loss 16.643332 loss_att 14.144892 loss_ctc 22.473021 lr 0.00251808 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:30:06,532 DEBUG TRAIN Batch 2/3900 loss 22.411774 loss_att 20.004124 loss_ctc 28.029617 lr 0.00253408 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:30:06,534 DEBUG TRAIN Batch 2/3900 loss 26.452360 loss_att 21.666813 loss_ctc 37.618637 lr 0.00253408 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:30:06,535 DEBUG TRAIN Batch 2/3900 loss 29.489250 loss_att 24.894474 loss_ctc 40.210388 lr 0.00253392 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:30:06,536 DEBUG TRAIN Batch 2/3900 loss 21.279690 loss_att 17.679729 loss_ctc 29.679592 lr 0.00253408 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:30:06,537 DEBUG TRAIN Batch 2/3900 loss 29.419155 loss_att 26.172003 loss_ctc 36.995842 lr 0.00253424 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:30:06,538 DEBUG TRAIN Batch 2/3900 loss 27.174179 loss_att 23.754757 loss_ctc 35.152824 lr 0.00253424 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:30:06,539 DEBUG TRAIN Batch 2/3900 loss 29.281021 loss_att 25.490398 loss_ctc 38.125809 lr 0.00253424 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:30:06,543 DEBUG TRAIN Batch 2/3900 loss 21.434338 loss_att 19.063950 loss_ctc 26.965244 lr 0.00253392 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:32:50,200 DEBUG TRAIN Batch 2/4000 loss 7.828215 loss_att 7.311872 loss_ctc 9.033015 lr 0.00255008 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:32:50,201 DEBUG TRAIN Batch 2/4000 loss 17.676252 loss_att 15.790107 loss_ctc 22.077255 lr 0.00255024 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:32:50,201 DEBUG TRAIN Batch 2/4000 loss 7.810061 loss_att 6.397444 loss_ctc 11.106169 lr 0.00255008 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:32:50,202 DEBUG TRAIN Batch 2/4000 loss 18.352007 loss_att 15.744385 loss_ctc 24.436459 lr 0.00255024 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:32:50,201 DEBUG TRAIN Batch 2/4000 loss 11.750887 loss_att 10.444191 loss_ctc 14.799846 lr 0.00255024 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:32:50,213 DEBUG TRAIN Batch 2/4000 loss 11.484230 loss_att 9.933964 loss_ctc 15.101520 lr 0.00254992 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:32:50,227 DEBUG TRAIN Batch 2/4000 loss 11.539515 loss_att 9.726096 loss_ctc 15.770823 lr 0.00255008 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:32:50,334 DEBUG TRAIN Batch 2/4000 loss 7.957824 loss_att 7.186834 loss_ctc 9.756800 lr 0.00254992 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:33:22,358 DEBUG TRAIN Batch 2/4100 loss 35.343140 loss_att 30.800343 loss_ctc 45.942993 lr 0.00256592 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:33:22,360 DEBUG TRAIN Batch 2/4100 loss 32.857182 loss_att 29.014828 loss_ctc 41.822678 lr 0.00256608 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:33:22,360 DEBUG TRAIN Batch 2/4100 loss 33.665764 loss_att 28.463421 loss_ctc 45.804565 lr 0.00256608 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:33:22,361 DEBUG TRAIN Batch 2/4100 loss 26.730488 loss_att 22.522385 loss_ctc 36.549393 lr 0.00256624 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:33:22,361 DEBUG TRAIN Batch 2/4100 loss 23.874405 loss_att 20.440365 loss_ctc 31.887161 lr 0.00256592 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:33:22,370 DEBUG TRAIN Batch 2/4100 loss 31.852577 loss_att 27.206274 loss_ctc 42.693951 lr 0.00256608 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:33:22,370 DEBUG TRAIN Batch 2/4100 loss 26.170351 loss_att 21.826681 loss_ctc 36.305580 lr 0.00256624 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:33:22,370 DEBUG TRAIN Batch 2/4100 loss 35.953381 loss_att 31.497547 loss_ctc 46.350327 lr 0.00256624 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:36:06,032 DEBUG TRAIN Batch 2/4200 loss 23.907619 loss_att 20.667820 loss_ctc 31.467154 lr 0.00258208 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:36:06,033 DEBUG TRAIN Batch 2/4200 loss 27.413395 loss_att 23.870621 loss_ctc 35.679863 lr 0.00258224 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:36:06,035 DEBUG TRAIN Batch 2/4200 loss 35.661583 loss_att 31.605736 loss_ctc 45.125225 lr 0.00258208 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:36:06,037 DEBUG TRAIN Batch 2/4200 loss 24.905296 loss_att 21.834188 loss_ctc 32.071217 lr 0.00258192 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:36:06,039 DEBUG TRAIN Batch 2/4200 loss 34.909851 loss_att 30.936611 loss_ctc 44.180748 lr 0.00258208 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:36:06,039 DEBUG TRAIN Batch 2/4200 loss 23.647602 loss_att 21.390923 loss_ctc 28.913189 lr 0.00258224 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:36:06,040 DEBUG TRAIN Batch 2/4200 loss 37.963028 loss_att 33.243523 loss_ctc 48.975212 lr 0.00258224 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:36:06,041 DEBUG TRAIN Batch 2/4200 loss 32.352402 loss_att 29.410816 loss_ctc 39.216103 lr 0.00258192 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:38:50,694 DEBUG TRAIN Batch 2/4300 loss 25.189564 loss_att 21.764547 loss_ctc 33.181271 lr 0.00259808 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:38:50,698 DEBUG TRAIN Batch 2/4300 loss 21.764812 loss_att 18.974155 loss_ctc 28.276348 lr 0.00259824 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:38:50,700 DEBUG TRAIN Batch 2/4300 loss 16.678139 loss_att 15.661764 loss_ctc 19.049679 lr 0.00259808 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:38:50,702 DEBUG TRAIN Batch 2/4300 loss 26.521931 loss_att 22.314865 loss_ctc 36.338417 lr 0.00259792 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:38:50,703 DEBUG TRAIN Batch 2/4300 loss 24.666040 loss_att 21.150703 loss_ctc 32.868492 lr 0.00259824 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:38:50,704 DEBUG TRAIN Batch 2/4300 loss 16.276878 loss_att 14.236116 loss_ctc 21.038656 lr 0.00259808 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:38:50,705 DEBUG TRAIN Batch 2/4300 loss 30.020256 loss_att 24.986656 loss_ctc 41.765316 lr 0.00259824 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:38:50,709 DEBUG TRAIN Batch 2/4300 loss 36.079739 loss_att 30.441097 loss_ctc 49.236572 lr 0.00259792 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:41:32,980 DEBUG TRAIN Batch 2/4400 loss 30.126392 loss_att 26.349087 loss_ctc 38.940105 lr 0.00261392 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:41:32,982 DEBUG TRAIN Batch 2/4400 loss 25.057203 loss_att 21.244528 loss_ctc 33.953445 lr 0.00261408 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:41:32,982 DEBUG TRAIN Batch 2/4400 loss 25.203741 loss_att 22.200590 loss_ctc 32.211094 lr 0.00261424 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:41:32,983 DEBUG TRAIN Batch 2/4400 loss 32.575008 loss_att 29.718647 loss_ctc 39.239853 lr 0.00261424 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:41:32,984 DEBUG TRAIN Batch 2/4400 loss 21.891125 loss_att 19.395321 loss_ctc 27.714668 lr 0.00261424 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:41:32,984 DEBUG TRAIN Batch 2/4400 loss 34.280632 loss_att 29.883810 loss_ctc 44.539886 lr 0.00261392 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:41:32,989 DEBUG TRAIN Batch 2/4400 loss 33.926388 loss_att 29.527161 loss_ctc 44.191254 lr 0.00261408 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:41:32,993 DEBUG TRAIN Batch 2/4400 loss 29.687286 loss_att 26.910656 loss_ctc 36.166088 lr 0.00261408 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:14,064 DEBUG TRAIN Batch 2/4500 loss 14.400390 loss_att 12.134265 loss_ctc 19.688015 lr 0.00262992 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:14,067 DEBUG TRAIN Batch 2/4500 loss 14.446892 loss_att 13.016780 loss_ctc 17.783819 lr 0.00262992 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:14,072 DEBUG TRAIN Batch 2/4500 loss 11.011814 loss_att 9.274784 loss_ctc 15.064883 lr 0.00263008 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:14,072 DEBUG TRAIN Batch 2/4500 loss 15.997760 loss_att 14.182651 loss_ctc 20.233013 lr 0.00263024 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:14,074 DEBUG TRAIN Batch 2/4500 loss 12.661060 loss_att 11.053866 loss_ctc 16.411179 lr 0.00263008 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:14,083 DEBUG TRAIN Batch 2/4500 loss 8.264925 loss_att 7.135169 loss_ctc 10.901020 lr 0.00263008 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:14,119 DEBUG TRAIN Batch 2/4500 loss 11.989056 loss_att 10.874874 loss_ctc 14.588814 lr 0.00263024 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:14,121 DEBUG TRAIN Batch 2/4500 loss 11.742050 loss_att 9.607422 loss_ctc 16.722851 lr 0.00263024 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:47,355 DEBUG TRAIN Batch 2/4600 loss 26.564606 loss_att 23.509472 loss_ctc 33.693245 lr 0.00264624 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:47,356 DEBUG TRAIN Batch 2/4600 loss 27.953999 loss_att 24.718445 loss_ctc 35.503624 lr 0.00264624 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:47,357 DEBUG TRAIN Batch 2/4600 loss 38.671417 loss_att 33.974865 loss_ctc 49.630035 lr 0.00264592 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:47,363 DEBUG TRAIN Batch 2/4600 loss 26.547977 loss_att 23.155098 loss_ctc 34.464699 lr 0.00264608 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:47,364 DEBUG TRAIN Batch 2/4600 loss 15.109074 loss_att 13.687327 loss_ctc 18.426479 lr 0.00264608 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:47,366 DEBUG TRAIN Batch 2/4600 loss 30.547836 loss_att 25.313429 loss_ctc 42.761456 lr 0.00264592 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:47,366 DEBUG TRAIN Batch 2/4600 loss 39.906651 loss_att 35.196404 loss_ctc 50.897224 lr 0.00264624 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:44:47,382 DEBUG TRAIN Batch 2/4600 loss 25.875971 loss_att 22.971767 loss_ctc 32.652447 lr 0.00264608 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:47:28,819 DEBUG TRAIN Batch 2/4700 loss 25.083731 loss_att 22.610149 loss_ctc 30.855419 lr 0.00266208 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:47:28,820 DEBUG TRAIN Batch 2/4700 loss 32.143791 loss_att 28.762733 loss_ctc 40.032925 lr 0.00266208 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:47:28,821 DEBUG TRAIN Batch 2/4700 loss 26.877354 loss_att 24.618406 loss_ctc 32.148235 lr 0.00266224 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:47:28,824 DEBUG TRAIN Batch 2/4700 loss 18.106676 loss_att 16.703926 loss_ctc 21.379761 lr 0.00266192 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:47:28,829 DEBUG TRAIN Batch 2/4700 loss 33.636139 loss_att 29.828392 loss_ctc 42.520882 lr 0.00266224 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:47:28,830 DEBUG TRAIN Batch 2/4700 loss 23.235142 loss_att 21.242821 loss_ctc 27.883886 lr 0.00266208 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:47:28,833 DEBUG TRAIN Batch 2/4700 loss 28.839016 loss_att 25.134686 loss_ctc 37.482449 lr 0.00266224 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:47:28,834 DEBUG TRAIN Batch 2/4700 loss 26.533096 loss_att 23.735416 loss_ctc 33.061016 lr 0.00266192 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:50:08,961 DEBUG TRAIN Batch 2/4800 loss 20.006731 loss_att 17.462103 loss_ctc 25.944195 lr 0.00267792 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:50:08,962 DEBUG TRAIN Batch 2/4800 loss 22.208134 loss_att 19.126192 loss_ctc 29.399330 lr 0.00267808 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:50:08,962 DEBUG TRAIN Batch 2/4800 loss 23.570122 loss_att 20.566015 loss_ctc 30.579700 lr 0.00267808 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:50:08,963 DEBUG TRAIN Batch 2/4800 loss 24.877773 loss_att 21.013382 loss_ctc 33.894684 lr 0.00267824 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:50:08,963 DEBUG TRAIN Batch 2/4800 loss 26.864988 loss_att 22.502716 loss_ctc 37.043625 lr 0.00267824 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:50:08,964 DEBUG TRAIN Batch 2/4800 loss 25.676590 loss_att 21.519562 loss_ctc 35.376320 lr 0.00267824 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:50:08,965 DEBUG TRAIN Batch 2/4800 loss 18.974865 loss_att 16.626989 loss_ctc 24.453239 lr 0.00267792 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:50:08,969 DEBUG TRAIN Batch 2/4800 loss 23.049164 loss_att 20.298450 loss_ctc 29.467497 lr 0.00267808 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:52:50,376 DEBUG TRAIN Batch 2/4900 loss 29.467270 loss_att 25.811964 loss_ctc 37.996315 lr 0.00269424 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:52:50,377 DEBUG TRAIN Batch 2/4900 loss 33.783928 loss_att 29.089859 loss_ctc 44.736748 lr 0.00269424 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:52:50,378 DEBUG TRAIN Batch 2/4900 loss 28.041468 loss_att 23.737011 loss_ctc 38.085197 lr 0.00269392 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:52:50,379 DEBUG TRAIN Batch 2/4900 loss 27.730673 loss_att 24.296032 loss_ctc 35.744839 lr 0.00269408 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:52:50,383 DEBUG TRAIN Batch 2/4900 loss 23.780346 loss_att 20.314989 loss_ctc 31.866180 lr 0.00269408 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:52:50,385 DEBUG TRAIN Batch 2/4900 loss 30.123741 loss_att 25.787567 loss_ctc 40.241478 lr 0.00269392 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:52:50,386 DEBUG TRAIN Batch 2/4900 loss 34.196270 loss_att 29.933655 loss_ctc 44.142376 lr 0.00269408 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:52:50,387 DEBUG TRAIN Batch 2/4900 loss 19.828552 loss_att 18.166489 loss_ctc 23.706696 lr 0.00269424 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:55:27,697 DEBUG TRAIN Batch 2/5000 loss 19.772842 loss_att 17.223671 loss_ctc 25.720907 lr 0.00271024 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:55:27,703 DEBUG TRAIN Batch 2/5000 loss 12.217933 loss_att 10.038567 loss_ctc 17.303120 lr 0.00271024 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:55:27,705 DEBUG TRAIN Batch 2/5000 loss 10.394963 loss_att 9.684538 loss_ctc 12.052623 lr 0.00271008 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:55:27,705 DEBUG TRAIN Batch 2/5000 loss 12.258107 loss_att 10.277547 loss_ctc 16.879416 lr 0.00270992 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:55:27,707 DEBUG TRAIN Batch 2/5000 loss 12.520167 loss_att 10.623326 loss_ctc 16.946129 lr 0.00271008 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:55:27,713 DEBUG TRAIN Batch 2/5000 loss 7.898435 loss_att 6.826698 loss_ctc 10.399151 lr 0.00271024 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:55:27,715 DEBUG TRAIN Batch 2/5000 loss 18.375954 loss_att 16.129791 loss_ctc 23.617001 lr 0.00270992 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:55:27,716 DEBUG TRAIN Batch 2/5000 loss 18.749538 loss_att 15.223295 loss_ctc 26.977436 lr 0.00271008 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:56:00,191 DEBUG TRAIN Batch 2/5100 loss 32.818718 loss_att 28.104721 loss_ctc 43.818039 lr 0.00272608 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:56:00,191 DEBUG TRAIN Batch 2/5100 loss 21.682781 loss_att 18.697933 loss_ctc 28.647421 lr 0.00272608 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 06:56:00,192 DEBUG TRAIN Batch 2/5100 loss 26.976757 loss_att 23.349445 loss_ctc 35.440483 lr 0.00272624 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:56:00,193 DEBUG TRAIN Batch 2/5100 loss 22.510818 loss_att 20.117798 loss_ctc 28.094528 lr 0.00272592 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:56:00,197 DEBUG TRAIN Batch 2/5100 loss 24.799828 loss_att 21.365545 loss_ctc 32.813156 lr 0.00272624 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:56:00,201 DEBUG TRAIN Batch 2/5100 loss 34.880280 loss_att 30.123348 loss_ctc 45.979778 lr 0.00272608 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:56:00,203 DEBUG TRAIN Batch 2/5100 loss 26.054600 loss_att 22.356016 loss_ctc 34.684631 lr 0.00272624 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:56:00,216 DEBUG TRAIN Batch 2/5100 loss 32.320724 loss_att 26.485519 loss_ctc 45.936199 lr 0.00272592 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:58:37,111 DEBUG TRAIN Batch 2/5200 loss 27.361914 loss_att 24.756680 loss_ctc 33.440792 lr 0.00274192 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 06:58:37,112 DEBUG TRAIN Batch 2/5200 loss 24.522839 loss_att 21.001881 loss_ctc 32.738411 lr 0.00274208 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 06:58:37,113 DEBUG TRAIN Batch 2/5200 loss 26.942982 loss_att 22.955801 loss_ctc 36.246403 lr 0.00274224 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 06:58:37,114 DEBUG TRAIN Batch 2/5200 loss 35.255421 loss_att 29.362610 loss_ctc 49.005306 lr 0.00274192 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 06:58:37,121 DEBUG TRAIN Batch 2/5200 loss 19.749405 loss_att 17.889097 loss_ctc 24.090124 lr 0.00274208 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 06:58:37,121 DEBUG TRAIN Batch 2/5200 loss 28.797651 loss_att 25.141216 loss_ctc 37.329334 lr 0.00274224 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 06:58:37,122 DEBUG TRAIN Batch 2/5200 loss 26.621830 loss_att 23.345135 loss_ctc 34.267448 lr 0.00274224 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 06:58:37,123 DEBUG TRAIN Batch 2/5200 loss 22.340691 loss_att 19.418892 loss_ctc 29.158218 lr 0.00274208 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:01:14,881 DEBUG TRAIN Batch 2/5300 loss 20.472593 loss_att 17.864910 loss_ctc 26.557186 lr 0.00275792 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:01:14,883 DEBUG TRAIN Batch 2/5300 loss 17.422657 loss_att 15.251373 loss_ctc 22.488981 lr 0.00275808 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:01:14,883 DEBUG TRAIN Batch 2/5300 loss 27.201021 loss_att 23.931126 loss_ctc 34.830780 lr 0.00275824 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:01:14,885 DEBUG TRAIN Batch 2/5300 loss 26.178514 loss_att 23.274122 loss_ctc 32.955429 lr 0.00275792 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:01:14,887 DEBUG TRAIN Batch 2/5300 loss 29.122540 loss_att 25.981573 loss_ctc 36.451462 lr 0.00275824 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:01:14,891 DEBUG TRAIN Batch 2/5300 loss 25.283367 loss_att 22.183243 loss_ctc 32.516994 lr 0.00275808 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:01:14,892 DEBUG TRAIN Batch 2/5300 loss 21.978554 loss_att 19.105555 loss_ctc 28.682217 lr 0.00275824 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:01:14,893 DEBUG TRAIN Batch 2/5300 loss 22.520533 loss_att 19.054638 loss_ctc 30.607618 lr 0.00275808 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:03:58,825 DEBUG TRAIN Batch 2/5400 loss 21.676826 loss_att 19.489807 loss_ctc 26.779873 lr 0.00277392 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:03:58,826 DEBUG TRAIN Batch 2/5400 loss 28.962921 loss_att 24.446774 loss_ctc 39.500599 lr 0.00277408 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:03:58,826 DEBUG TRAIN Batch 2/5400 loss 28.502390 loss_att 24.882462 loss_ctc 36.948891 lr 0.00277424 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:03:58,827 DEBUG TRAIN Batch 2/5400 loss 25.004257 loss_att 20.513739 loss_ctc 35.482136 lr 0.00277392 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:03:58,834 DEBUG TRAIN Batch 2/5400 loss 18.821634 loss_att 16.625172 loss_ctc 23.946714 lr 0.00277408 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:03:58,835 DEBUG TRAIN Batch 2/5400 loss 31.813189 loss_att 28.847446 loss_ctc 38.733253 lr 0.00277424 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:03:58,836 DEBUG TRAIN Batch 2/5400 loss 21.524349 loss_att 18.560577 loss_ctc 28.439816 lr 0.00277408 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:03:58,838 DEBUG TRAIN Batch 2/5400 loss 25.959629 loss_att 22.572491 loss_ctc 33.862953 lr 0.00277424 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:06:45,946 DEBUG TRAIN Batch 2/5500 loss 15.674730 loss_att 14.387759 loss_ctc 18.677662 lr 0.00278992 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:06:45,947 DEBUG TRAIN Batch 2/5500 loss 21.039314 loss_att 17.848286 loss_ctc 28.485043 lr 0.00279008 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:06:45,948 DEBUG TRAIN Batch 2/5500 loss 12.923445 loss_att 11.971522 loss_ctc 15.144596 lr 0.00279024 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:06:45,949 DEBUG TRAIN Batch 2/5500 loss 11.097616 loss_att 8.967719 loss_ctc 16.067373 lr 0.00278992 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:06:45,958 DEBUG TRAIN Batch 2/5500 loss 14.409575 loss_att 12.377581 loss_ctc 19.150890 lr 0.00279008 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:06:45,960 DEBUG TRAIN Batch 2/5500 loss 11.391909 loss_att 9.946456 loss_ctc 14.764629 lr 0.00279008 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:06:45,974 DEBUG TRAIN Batch 2/5500 loss 11.895145 loss_att 10.475389 loss_ctc 15.207907 lr 0.00279024 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:06:45,975 DEBUG TRAIN Batch 2/5500 loss 14.547178 loss_att 12.796021 loss_ctc 18.633213 lr 0.00279024 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:07:19,847 DEBUG TRAIN Batch 2/5600 loss 41.241783 loss_att 35.944046 loss_ctc 53.603172 lr 0.00280624 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:07:19,848 DEBUG TRAIN Batch 2/5600 loss 20.213552 loss_att 18.442932 loss_ctc 24.344999 lr 0.00280608 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:07:19,849 DEBUG TRAIN Batch 2/5600 loss 26.980824 loss_att 23.117504 loss_ctc 35.995232 lr 0.00280624 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:07:19,850 DEBUG TRAIN Batch 2/5600 loss 30.894415 loss_att 25.912041 loss_ctc 42.519955 lr 0.00280592 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:07:19,854 DEBUG TRAIN Batch 2/5600 loss 21.705072 loss_att 19.711338 loss_ctc 26.357121 lr 0.00280592 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:07:19,857 DEBUG TRAIN Batch 2/5600 loss 23.349199 loss_att 19.436874 loss_ctc 32.477955 lr 0.00280608 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:07:19,857 DEBUG TRAIN Batch 2/5600 loss 27.134212 loss_att 22.915817 loss_ctc 36.977135 lr 0.00280624 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:07:19,858 DEBUG TRAIN Batch 2/5600 loss 39.907555 loss_att 34.871284 loss_ctc 51.658855 lr 0.00280608 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:10:02,395 DEBUG TRAIN Batch 2/5700 loss 21.825865 loss_att 18.714458 loss_ctc 29.085812 lr 0.00282192 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:10:02,398 DEBUG TRAIN Batch 2/5700 loss 22.195782 loss_att 18.951269 loss_ctc 29.766308 lr 0.00282224 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:10:02,398 DEBUG TRAIN Batch 2/5700 loss 33.261948 loss_att 28.794437 loss_ctc 43.686142 lr 0.00282208 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:10:02,399 DEBUG TRAIN Batch 2/5700 loss 32.270390 loss_att 27.040722 loss_ctc 44.472942 lr 0.00282192 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:10:02,405 DEBUG TRAIN Batch 2/5700 loss 24.835735 loss_att 21.621319 loss_ctc 32.336044 lr 0.00282208 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:10:02,405 DEBUG TRAIN Batch 2/5700 loss 34.296844 loss_att 29.225048 loss_ctc 46.131039 lr 0.00282224 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:10:02,406 DEBUG TRAIN Batch 2/5700 loss 25.990828 loss_att 23.909801 loss_ctc 30.846558 lr 0.00282224 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:10:02,407 DEBUG TRAIN Batch 2/5700 loss 26.072540 loss_att 22.127556 loss_ctc 35.277504 lr 0.00282208 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:12:48,493 DEBUG TRAIN Batch 2/5800 loss 27.738255 loss_att 23.457045 loss_ctc 37.727741 lr 0.00283808 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:12:48,499 DEBUG TRAIN Batch 2/5800 loss 23.643740 loss_att 20.445507 loss_ctc 31.106283 lr 0.00283792 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:12:48,502 DEBUG TRAIN Batch 2/5800 loss 18.356998 loss_att 16.212406 loss_ctc 23.361050 lr 0.00283808 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:12:48,502 DEBUG TRAIN Batch 2/5800 loss 23.438036 loss_att 20.408970 loss_ctc 30.505857 lr 0.00283808 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:12:48,502 DEBUG TRAIN Batch 2/5800 loss 22.009895 loss_att 18.799398 loss_ctc 29.501059 lr 0.00283824 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:12:48,502 DEBUG TRAIN Batch 2/5800 loss 23.594950 loss_att 20.726685 loss_ctc 30.287567 lr 0.00283824 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:12:48,504 DEBUG TRAIN Batch 2/5800 loss 23.506042 loss_att 20.776512 loss_ctc 29.874945 lr 0.00283824 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:12:48,504 DEBUG TRAIN Batch 2/5800 loss 18.939747 loss_att 16.251575 loss_ctc 25.212151 lr 0.00283792 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:14:48,777 DEBUG TRAIN Batch 2/5900 loss 26.210400 loss_att 24.889395 loss_ctc 29.292746 lr 0.00285392 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:14:48,779 DEBUG TRAIN Batch 2/5900 loss 26.391621 loss_att 23.053181 loss_ctc 34.181313 lr 0.00285408 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:14:48,779 DEBUG TRAIN Batch 2/5900 loss 26.983913 loss_att 23.227110 loss_ctc 35.749794 lr 0.00285424 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:14:48,780 DEBUG TRAIN Batch 2/5900 loss 22.947044 loss_att 19.902027 loss_ctc 30.052090 lr 0.00285424 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:14:48,783 DEBUG TRAIN Batch 2/5900 loss 32.903004 loss_att 28.516315 loss_ctc 43.138611 lr 0.00285392 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:14:48,784 DEBUG TRAIN Batch 2/5900 loss 37.967819 loss_att 33.412613 loss_ctc 48.596630 lr 0.00285424 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:14:48,785 DEBUG TRAIN Batch 2/5900 loss 25.040438 loss_att 21.639503 loss_ctc 32.975948 lr 0.00285408 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:14:48,787 DEBUG TRAIN Batch 2/5900 loss 21.043255 loss_att 17.615135 loss_ctc 29.042198 lr 0.00285408 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:15:49,424 DEBUG CV Batch 2/0 loss 4.706625 loss_att 2.772516 loss_ctc 9.219544 history loss 4.344576 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:15:49,424 DEBUG CV Batch 2/0 loss 4.706625 loss_att 2.772516 loss_ctc 9.219544 history loss 4.344576 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:15:49,424 DEBUG CV Batch 2/0 loss 4.706625 loss_att 2.772516 loss_ctc 9.219544 history loss 4.344576 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:15:49,424 DEBUG CV Batch 2/0 loss 4.706625 loss_att 2.772516 loss_ctc 9.219544 history loss 4.344576 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:15:49,425 DEBUG CV Batch 2/0 loss 4.706625 loss_att 2.772516 loss_ctc 9.219544 history loss 4.344576 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:15:49,427 DEBUG CV Batch 2/0 loss 4.706625 loss_att 2.772516 loss_ctc 9.219544 history loss 4.344576 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:15:49,431 DEBUG CV Batch 2/0 loss 4.706625 loss_att 2.772516 loss_ctc 9.219544 history loss 4.344576 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:15:49,446 DEBUG CV Batch 2/0 loss 4.706625 loss_att 2.772516 loss_ctc 9.219544 history loss 4.344576 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:17:07,909 DEBUG CV Batch 2/100 loss 6.032141 loss_att 5.265900 loss_ctc 7.820037 history loss 7.971432 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:17:07,912 DEBUG CV Batch 2/100 loss 6.032141 loss_att 5.265900 loss_ctc 7.820037 history loss 7.971432 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:17:07,927 DEBUG CV Batch 2/100 loss 6.032141 loss_att 5.265900 loss_ctc 7.820037 history loss 7.971432 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:17:07,937 DEBUG CV Batch 2/100 loss 6.032141 loss_att 5.265900 loss_ctc 7.820037 history loss 7.971432 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:17:07,939 DEBUG CV Batch 2/100 loss 6.032141 loss_att 5.265900 loss_ctc 7.820037 history loss 7.971432 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:17:07,940 DEBUG CV Batch 2/100 loss 6.032141 loss_att 5.265900 loss_ctc 7.820037 history loss 7.971432 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:17:07,962 DEBUG CV Batch 2/100 loss 6.032141 loss_att 5.265900 loss_ctc 7.820037 history loss 7.971432 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:17:07,982 DEBUG CV Batch 2/100 loss 6.032141 loss_att 5.265900 loss_ctc 7.820037 history loss 7.971432 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:18:22,035 DEBUG CV Batch 2/200 loss 12.353931 loss_att 10.028474 loss_ctc 17.779999 history loss 8.643710 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:18:22,058 DEBUG CV Batch 2/200 loss 12.353931 loss_att 10.028474 loss_ctc 17.779999 history loss 8.643710 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:18:22,069 DEBUG CV Batch 2/200 loss 12.353931 loss_att 10.028474 loss_ctc 17.779999 history loss 8.643710 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:18:22,098 DEBUG CV Batch 2/200 loss 12.353931 loss_att 10.028474 loss_ctc 17.779999 history loss 8.643710 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:18:22,110 DEBUG CV Batch 2/200 loss 12.353931 loss_att 10.028474 loss_ctc 17.779999 history loss 8.643710 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:18:22,111 DEBUG CV Batch 2/200 loss 12.353931 loss_att 10.028474 loss_ctc 17.779999 history loss 8.643710 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:18:22,122 DEBUG CV Batch 2/200 loss 12.353931 loss_att 10.028474 loss_ctc 17.779999 history loss 8.643710 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:18:22,141 DEBUG CV Batch 2/200 loss 12.353931 loss_att 10.028474 loss_ctc 17.779999 history loss 8.643710 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:20:13,718 DEBUG CV Batch 2/300 loss 5.542802 loss_att 4.647034 loss_ctc 7.632928 history loss 11.254091 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:20:13,734 DEBUG CV Batch 2/300 loss 5.542802 loss_att 4.647034 loss_ctc 7.632928 history loss 11.254091 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:20:13,735 DEBUG CV Batch 2/300 loss 5.542802 loss_att 4.647034 loss_ctc 7.632928 history loss 11.254091 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:20:13,753 DEBUG CV Batch 2/300 loss 5.542802 loss_att 4.647034 loss_ctc 7.632928 history loss 11.254091 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:20:13,770 DEBUG CV Batch 2/300 loss 5.542802 loss_att 4.647034 loss_ctc 7.632928 history loss 11.254091 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:20:13,774 DEBUG CV Batch 2/300 loss 5.542802 loss_att 4.647034 loss_ctc 7.632928 history loss 11.254091 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:20:13,800 DEBUG CV Batch 2/300 loss 5.542802 loss_att 4.647034 loss_ctc 7.632928 history loss 11.254091 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:20:13,810 DEBUG CV Batch 2/300 loss 5.542802 loss_att 4.647034 loss_ctc 7.632928 history loss 11.254091 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:21:27,135 DEBUG CV Batch 2/400 loss 22.730618 loss_att 18.959217 loss_ctc 31.530554 history loss 12.474574 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:21:27,142 DEBUG CV Batch 2/400 loss 22.730618 loss_att 18.959217 loss_ctc 31.530554 history loss 12.474574 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:21:27,155 DEBUG CV Batch 2/400 loss 22.730618 loss_att 18.959217 loss_ctc 31.530554 history loss 12.474574 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:21:27,157 DEBUG CV Batch 2/400 loss 22.730618 loss_att 18.959217 loss_ctc 31.530554 history loss 12.474574 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:21:27,163 DEBUG CV Batch 2/400 loss 22.730618 loss_att 18.959217 loss_ctc 31.530554 history loss 12.474574 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:21:27,164 DEBUG CV Batch 2/400 loss 22.730618 loss_att 18.959217 loss_ctc 31.530554 history loss 12.474574 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:21:27,220 DEBUG CV Batch 2/400 loss 22.730618 loss_att 18.959217 loss_ctc 31.530554 history loss 12.474574 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:21:27,221 DEBUG CV Batch 2/400 loss 22.730618 loss_att 18.959217 loss_ctc 31.530554 history loss 12.474574 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:23:15,671 DEBUG CV Batch 2/500 loss 3.622848 loss_att 2.774164 loss_ctc 5.603110 history loss 12.903411 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:23:15,672 DEBUG CV Batch 2/500 loss 3.622848 loss_att 2.774164 loss_ctc 5.603110 history loss 12.903411 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:23:15,677 DEBUG CV Batch 2/500 loss 3.622848 loss_att 2.774164 loss_ctc 5.603110 history loss 12.903411 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:23:15,679 DEBUG CV Batch 2/500 loss 3.622848 loss_att 2.774164 loss_ctc 5.603110 history loss 12.903411 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:23:15,684 DEBUG CV Batch 2/500 loss 3.622848 loss_att 2.774164 loss_ctc 5.603110 history loss 12.903411 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:23:15,691 DEBUG CV Batch 2/500 loss 3.622848 loss_att 2.774164 loss_ctc 5.603110 history loss 12.903411 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:23:15,698 DEBUG CV Batch 2/500 loss 3.622848 loss_att 2.774164 loss_ctc 5.603110 history loss 12.903411 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:23:15,703 DEBUG CV Batch 2/500 loss 3.622848 loss_att 2.774164 loss_ctc 5.603110 history loss 12.903411 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:24:29,660 DEBUG CV Batch 2/600 loss 9.666669 loss_att 8.514263 loss_ctc 12.355614 history loss 12.072277 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:24:29,690 DEBUG CV Batch 2/600 loss 9.666669 loss_att 8.514263 loss_ctc 12.355614 history loss 12.072277 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:24:29,695 DEBUG CV Batch 2/600 loss 9.666669 loss_att 8.514263 loss_ctc 12.355614 history loss 12.072277 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:24:29,698 DEBUG CV Batch 2/600 loss 9.666669 loss_att 8.514263 loss_ctc 12.355614 history loss 12.072277 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:24:29,705 DEBUG CV Batch 2/600 loss 9.666669 loss_att 8.514263 loss_ctc 12.355614 history loss 12.072277 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:24:29,709 DEBUG CV Batch 2/600 loss 9.666669 loss_att 8.514263 loss_ctc 12.355614 history loss 12.072277 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:24:29,717 DEBUG CV Batch 2/600 loss 9.666669 loss_att 8.514263 loss_ctc 12.355614 history loss 12.072277 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:24:29,722 DEBUG CV Batch 2/600 loss 9.666669 loss_att 8.514263 loss_ctc 12.355614 history loss 12.072277 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:25:41,395 DEBUG CV Batch 2/700 loss 19.562979 loss_att 16.329311 loss_ctc 27.108200 history loss 11.959414 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:25:41,429 DEBUG CV Batch 2/700 loss 19.562979 loss_att 16.329311 loss_ctc 27.108200 history loss 11.959414 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:25:41,442 DEBUG CV Batch 2/700 loss 19.562979 loss_att 16.329311 loss_ctc 27.108200 history loss 11.959414 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:25:41,457 DEBUG CV Batch 2/700 loss 19.562979 loss_att 16.329311 loss_ctc 27.108200 history loss 11.959414 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:25:41,471 DEBUG CV Batch 2/700 loss 19.562979 loss_att 16.329311 loss_ctc 27.108200 history loss 11.959414 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:25:41,492 DEBUG CV Batch 2/700 loss 19.562979 loss_att 16.329311 loss_ctc 27.108200 history loss 11.959414 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:25:41,517 DEBUG CV Batch 2/700 loss 19.562979 loss_att 16.329311 loss_ctc 27.108200 history loss 11.959414 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:25:41,701 DEBUG CV Batch 2/700 loss 19.562979 loss_att 16.329311 loss_ctc 27.108200 history loss 11.959414 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:27:27,342 DEBUG CV Batch 2/800 loss 8.024066 loss_att 6.377026 loss_ctc 11.867160 history loss 12.597217 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:27:27,381 DEBUG CV Batch 2/800 loss 8.024066 loss_att 6.377026 loss_ctc 11.867160 history loss 12.597217 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:27:27,383 DEBUG CV Batch 2/800 loss 8.024066 loss_att 6.377026 loss_ctc 11.867160 history loss 12.597217 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:27:27,387 DEBUG CV Batch 2/800 loss 8.024066 loss_att 6.377026 loss_ctc 11.867160 history loss 12.597217 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:27:27,389 DEBUG CV Batch 2/800 loss 8.024066 loss_att 6.377026 loss_ctc 11.867160 history loss 12.597217 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:27:27,401 DEBUG CV Batch 2/800 loss 8.024066 loss_att 6.377026 loss_ctc 11.867160 history loss 12.597217 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:27:27,404 DEBUG CV Batch 2/800 loss 8.024066 loss_att 6.377026 loss_ctc 11.867160 history loss 12.597217 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:27:27,405 DEBUG CV Batch 2/800 loss 8.024066 loss_att 6.377026 loss_ctc 11.867160 history loss 12.597217 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:28:34,961 DEBUG CV Batch 2/900 loss 12.601865 loss_att 10.018690 loss_ctc 18.629272 history loss 13.063697 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:28:34,988 DEBUG CV Batch 2/900 loss 12.601865 loss_att 10.018690 loss_ctc 18.629272 history loss 13.063697 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:28:34,990 DEBUG CV Batch 2/900 loss 12.601865 loss_att 10.018690 loss_ctc 18.629272 history loss 13.063697 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:28:35,011 DEBUG CV Batch 2/900 loss 12.601865 loss_att 10.018690 loss_ctc 18.629272 history loss 13.063697 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:28:35,015 DEBUG CV Batch 2/900 loss 12.601865 loss_att 10.018690 loss_ctc 18.629272 history loss 13.063697 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:28:35,015 DEBUG CV Batch 2/900 loss 12.601865 loss_att 10.018690 loss_ctc 18.629272 history loss 13.063697 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:28:35,025 DEBUG CV Batch 2/900 loss 12.601865 loss_att 10.018690 loss_ctc 18.629272 history loss 13.063697 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:28:35,027 DEBUG CV Batch 2/900 loss 12.601865 loss_att 10.018690 loss_ctc 18.629272 history loss 13.063697 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:30:24,200 DEBUG CV Batch 2/1000 loss 3.183958 loss_att 2.965848 loss_ctc 3.692882 history loss 12.867953 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:30:24,201 DEBUG CV Batch 2/1000 loss 3.183958 loss_att 2.965848 loss_ctc 3.692882 history loss 12.867953 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:30:24,206 DEBUG CV Batch 2/1000 loss 3.183958 loss_att 2.965848 loss_ctc 3.692882 history loss 12.867953 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:30:24,206 DEBUG CV Batch 2/1000 loss 3.183958 loss_att 2.965848 loss_ctc 3.692882 history loss 12.867953 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:30:24,206 DEBUG CV Batch 2/1000 loss 3.183958 loss_att 2.965848 loss_ctc 3.692882 history loss 12.867953 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:30:24,226 DEBUG CV Batch 2/1000 loss 3.183958 loss_att 2.965848 loss_ctc 3.692882 history loss 12.867953 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:30:24,230 DEBUG CV Batch 2/1000 loss 3.183958 loss_att 2.965848 loss_ctc 3.692882 history loss 12.867953 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:30:24,232 DEBUG CV Batch 2/1000 loss 3.183958 loss_att 2.965848 loss_ctc 3.692882 history loss 12.867953 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:31:36,041 DEBUG CV Batch 2/1100 loss 9.098656 loss_att 8.354875 loss_ctc 10.834145 history loss 12.465747 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:31:36,063 DEBUG CV Batch 2/1100 loss 9.098656 loss_att 8.354875 loss_ctc 10.834145 history loss 12.465747 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:31:36,070 DEBUG CV Batch 2/1100 loss 9.098656 loss_att 8.354875 loss_ctc 10.834145 history loss 12.465747 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:31:36,079 DEBUG CV Batch 2/1100 loss 9.098656 loss_att 8.354875 loss_ctc 10.834145 history loss 12.465747 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:31:36,084 DEBUG CV Batch 2/1100 loss 9.098656 loss_att 8.354875 loss_ctc 10.834145 history loss 12.465747 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:31:36,102 DEBUG CV Batch 2/1100 loss 9.098656 loss_att 8.354875 loss_ctc 10.834145 history loss 12.465747 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:31:36,121 DEBUG CV Batch 2/1100 loss 9.098656 loss_att 8.354875 loss_ctc 10.834145 history loss 12.465747 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:31:36,129 DEBUG CV Batch 2/1100 loss 9.098656 loss_att 8.354875 loss_ctc 10.834145 history loss 12.465747 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:32:47,568 DEBUG CV Batch 2/1200 loss 20.686584 loss_att 17.289158 loss_ctc 28.613913 history loss 12.717705 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:32:47,584 DEBUG CV Batch 2/1200 loss 20.686584 loss_att 17.289158 loss_ctc 28.613913 history loss 12.717705 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:32:47,614 DEBUG CV Batch 2/1200 loss 20.686584 loss_att 17.289158 loss_ctc 28.613913 history loss 12.717705 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:32:47,654 DEBUG CV Batch 2/1200 loss 20.686584 loss_att 17.289158 loss_ctc 28.613913 history loss 12.717705 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:32:47,655 DEBUG CV Batch 2/1200 loss 20.686584 loss_att 17.289158 loss_ctc 28.613913 history loss 12.717705 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:32:47,658 DEBUG CV Batch 2/1200 loss 20.686584 loss_att 17.289158 loss_ctc 28.613913 history loss 12.717705 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:32:47,703 DEBUG CV Batch 2/1200 loss 20.686584 loss_att 17.289158 loss_ctc 28.613913 history loss 12.717705 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:32:47,741 DEBUG CV Batch 2/1200 loss 20.686584 loss_att 17.289158 loss_ctc 28.613913 history loss 12.717705 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:28,983 DEBUG CV Batch 2/1300 loss 10.641562 loss_att 8.867548 loss_ctc 14.780925 history loss 13.029614 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:28,990 DEBUG CV Batch 2/1300 loss 10.641562 loss_att 8.867548 loss_ctc 14.780925 history loss 13.029614 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:29,000 DEBUG CV Batch 2/1300 loss 10.641562 loss_att 8.867548 loss_ctc 14.780925 history loss 13.029614 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:29,003 DEBUG CV Batch 2/1300 loss 10.641562 loss_att 8.867548 loss_ctc 14.780925 history loss 13.029614 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:29,005 DEBUG CV Batch 2/1300 loss 10.641562 loss_att 8.867548 loss_ctc 14.780925 history loss 13.029614 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:29,009 DEBUG CV Batch 2/1300 loss 10.641562 loss_att 8.867548 loss_ctc 14.780925 history loss 13.029614 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:29,019 DEBUG CV Batch 2/1300 loss 10.641562 loss_att 8.867548 loss_ctc 14.780925 history loss 13.029614 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:29,029 DEBUG CV Batch 2/1300 loss 10.641562 loss_att 8.867548 loss_ctc 14.780925 history loss 13.029614 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,170 INFO Epoch 2 CV info cv_loss 13.272144868217406\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,170 INFO Epoch 3 TRAIN info lr 0.0028648\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,172 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,198 INFO Epoch 2 CV info cv_loss 13.272144868217406\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,198 INFO Checkpoint: save to checkpoint /opt/ml/checkpoints/2.pt\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,231 INFO Epoch 2 CV info cv_loss 13.272144868217406\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,231 INFO Epoch 3 TRAIN info lr 0.00286496\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,233 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,234 INFO Epoch 2 CV info cv_loss 13.272144868217406\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,235 INFO Epoch 3 TRAIN info lr 0.0028651199999999996\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,235 INFO Epoch 2 CV info cv_loss 13.272144868217406\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,235 INFO Epoch 3 TRAIN info lr 0.0028648\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,236 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,237 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,240 INFO Epoch 2 CV info cv_loss 13.272144868217406\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,240 INFO Epoch 3 TRAIN info lr 0.0028651199999999996\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,243 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,243 INFO Epoch 2 CV info cv_loss 13.272144868217406\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,244 INFO Epoch 3 TRAIN info lr 0.00286496\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,245 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,258 INFO Epoch 2 CV info cv_loss 13.272144868217406\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,258 INFO Epoch 3 TRAIN info lr 0.0028648\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,260 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,495 INFO Epoch 3 TRAIN info lr 0.00286496\u001b[0m\n",
      "\u001b[34m2023-03-29 07:34:56,497 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-29 07:37:41,138 DEBUG TRAIN Batch 3/0 loss 17.035343 loss_att 14.482889 loss_ctc 22.991070 lr 0.00286496 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:37:41,139 DEBUG TRAIN Batch 3/0 loss 16.854361 loss_att 14.815084 loss_ctc 21.612671 lr 0.00286496 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:37:41,145 DEBUG TRAIN Batch 3/0 loss 12.735991 loss_att 10.657843 loss_ctc 17.585003 lr 0.00286528 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:37:41,146 DEBUG TRAIN Batch 3/0 loss 10.834421 loss_att 9.453327 loss_ctc 14.056973 lr 0.00286528 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:37:41,150 DEBUG TRAIN Batch 3/0 loss 11.716106 loss_att 10.442535 loss_ctc 14.687771 lr 0.00286512 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:37:41,164 DEBUG TRAIN Batch 3/0 loss 12.414169 loss_att 10.671863 loss_ctc 16.479553 lr 0.00286496 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:37:41,169 DEBUG TRAIN Batch 3/0 loss 11.622134 loss_att 9.908294 loss_ctc 15.621096 lr 0.00286512 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:37:41,172 DEBUG TRAIN Batch 3/0 loss 10.863214 loss_att 9.413735 loss_ctc 14.245329 lr 0.00286512 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:38:15,048 DEBUG TRAIN Batch 3/100 loss 26.489456 loss_att 23.188683 loss_ctc 34.191261 lr 0.00288096 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:38:15,048 DEBUG TRAIN Batch 3/100 loss 25.480152 loss_att 21.514578 loss_ctc 34.733162 lr 0.00288112 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:38:15,050 DEBUG TRAIN Batch 3/100 loss 21.856287 loss_att 19.109388 loss_ctc 28.265717 lr 0.00288128 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:38:15,053 DEBUG TRAIN Batch 3/100 loss 33.033165 loss_att 28.830624 loss_ctc 42.839092 lr 0.00288096 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:38:15,055 DEBUG TRAIN Batch 3/100 loss 22.521183 loss_att 20.939499 loss_ctc 26.211782 lr 0.00288128 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:38:15,056 DEBUG TRAIN Batch 3/100 loss 25.258102 loss_att 22.439621 loss_ctc 31.834560 lr 0.00288112 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:38:15,056 DEBUG TRAIN Batch 3/100 loss 23.036037 loss_att 21.407200 loss_ctc 26.836658 lr 0.00288096 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:38:15,060 DEBUG TRAIN Batch 3/100 loss 25.809118 loss_att 23.248783 loss_ctc 31.783237 lr 0.00288112 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:40:58,729 DEBUG TRAIN Batch 3/200 loss 24.989635 loss_att 21.621853 loss_ctc 32.847797 lr 0.00289696 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:40:58,730 DEBUG TRAIN Batch 3/200 loss 22.897314 loss_att 19.920673 loss_ctc 29.842808 lr 0.00289712 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:40:58,730 DEBUG TRAIN Batch 3/200 loss 20.503237 loss_att 18.481018 loss_ctc 25.221748 lr 0.00289696 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:40:58,732 DEBUG TRAIN Batch 3/200 loss 25.447975 loss_att 22.029549 loss_ctc 33.424301 lr 0.00289728 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:40:58,734 DEBUG TRAIN Batch 3/200 loss 38.864960 loss_att 32.639786 loss_ctc 53.390362 lr 0.00289696 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:40:58,736 DEBUG TRAIN Batch 3/200 loss 23.051283 loss_att 20.341093 loss_ctc 29.375059 lr 0.00289712 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:40:58,739 DEBUG TRAIN Batch 3/200 loss 23.627480 loss_att 21.162601 loss_ctc 29.378860 lr 0.00289712 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:40:58,759 DEBUG TRAIN Batch 3/200 loss 27.583412 loss_att 24.333830 loss_ctc 35.165775 lr 0.00289728 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:43:41,949 DEBUG TRAIN Batch 3/300 loss 23.150188 loss_att 21.320868 loss_ctc 27.418602 lr 0.00291296 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:43:41,951 DEBUG TRAIN Batch 3/300 loss 28.431133 loss_att 25.638403 loss_ctc 34.947502 lr 0.00291296 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:43:41,954 DEBUG TRAIN Batch 3/300 loss 18.776377 loss_att 16.056597 loss_ctc 25.122528 lr 0.00291312 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:43:41,955 DEBUG TRAIN Batch 3/300 loss 15.052391 loss_att 12.882881 loss_ctc 20.114580 lr 0.00291328 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:43:41,956 DEBUG TRAIN Batch 3/300 loss 20.262922 loss_att 17.352144 loss_ctc 27.054739 lr 0.00291328 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:43:41,957 DEBUG TRAIN Batch 3/300 loss 25.870676 loss_att 22.926838 loss_ctc 32.739632 lr 0.00291296 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:43:41,959 DEBUG TRAIN Batch 3/300 loss 23.452938 loss_att 20.069981 loss_ctc 31.346504 lr 0.00291312 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:43:41,958 DEBUG TRAIN Batch 3/300 loss 20.293524 loss_att 18.329489 loss_ctc 24.876272 lr 0.00291312 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:46:26,439 DEBUG TRAIN Batch 3/400 loss 20.480774 loss_att 17.821590 loss_ctc 26.685535 lr 0.00292896 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:46:26,441 DEBUG TRAIN Batch 3/400 loss 28.481724 loss_att 25.213402 loss_ctc 36.107807 lr 0.00292896 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:46:26,442 DEBUG TRAIN Batch 3/400 loss 32.387405 loss_att 27.803875 loss_ctc 43.082317 lr 0.00292912 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:46:26,443 DEBUG TRAIN Batch 3/400 loss 27.717236 loss_att 24.863714 loss_ctc 34.375454 lr 0.00292928 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:46:26,442 DEBUG TRAIN Batch 3/400 loss 23.986801 loss_att 21.472668 loss_ctc 29.853109 lr 0.00292912 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:46:26,443 DEBUG TRAIN Batch 3/400 loss 22.659119 loss_att 20.074055 loss_ctc 28.690931 lr 0.00292928 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:46:26,444 DEBUG TRAIN Batch 3/400 loss 30.902225 loss_att 26.465244 loss_ctc 41.255184 lr 0.00292896 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:46:26,444 DEBUG TRAIN Batch 3/400 loss 24.571011 loss_att 22.421295 loss_ctc 29.587017 lr 0.00292912 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:09,714 DEBUG TRAIN Batch 3/500 loss 11.650022 loss_att 9.769805 loss_ctc 16.037193 lr 0.00294512 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:09,715 DEBUG TRAIN Batch 3/500 loss 10.761480 loss_att 9.427659 loss_ctc 13.873730 lr 0.00294528 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:09,716 DEBUG TRAIN Batch 3/500 loss 12.937511 loss_att 11.138667 loss_ctc 17.134815 lr 0.00294528 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:09,716 DEBUG TRAIN Batch 3/500 loss 11.948697 loss_att 10.262343 loss_ctc 15.883522 lr 0.00294496 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:09,720 DEBUG TRAIN Batch 3/500 loss 15.585060 loss_att 12.183167 loss_ctc 23.522810 lr 0.00294496 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:09,718 DEBUG TRAIN Batch 3/500 loss 12.953817 loss_att 11.081266 loss_ctc 17.323101 lr 0.00294512 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:09,734 DEBUG TRAIN Batch 3/500 loss 7.888811 loss_att 6.497320 loss_ctc 11.135622 lr 0.00294496 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:09,740 DEBUG TRAIN Batch 3/500 loss 10.561401 loss_att 9.430454 loss_ctc 13.200277 lr 0.00294512 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:43,062 DEBUG TRAIN Batch 3/600 loss 29.402138 loss_att 25.644333 loss_ctc 38.170349 lr 0.00296096 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:43,064 DEBUG TRAIN Batch 3/600 loss 25.207640 loss_att 22.787260 loss_ctc 30.855190 lr 0.00296112 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:43,065 DEBUG TRAIN Batch 3/600 loss 33.904583 loss_att 29.280241 loss_ctc 44.694717 lr 0.00296096 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:43,065 DEBUG TRAIN Batch 3/600 loss 22.240391 loss_att 20.263189 loss_ctc 26.853863 lr 0.00296112 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:43,066 DEBUG TRAIN Batch 3/600 loss 22.739527 loss_att 20.214664 loss_ctc 28.630873 lr 0.00296128 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:43,066 DEBUG TRAIN Batch 3/600 loss 26.260359 loss_att 22.481573 loss_ctc 35.077526 lr 0.00296096 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:43,072 DEBUG TRAIN Batch 3/600 loss 23.431419 loss_att 19.470993 loss_ctc 32.672413 lr 0.00296128 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:49:43,074 DEBUG TRAIN Batch 3/600 loss 32.804199 loss_att 27.545303 loss_ctc 45.074959 lr 0.00296112 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:52:26,185 DEBUG TRAIN Batch 3/700 loss 35.316101 loss_att 30.608612 loss_ctc 46.300243 lr 0.00297712 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:52:26,186 DEBUG TRAIN Batch 3/700 loss 41.169991 loss_att 35.040985 loss_ctc 55.471004 lr 0.00297728 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:52:26,187 DEBUG TRAIN Batch 3/700 loss 33.980213 loss_att 30.100414 loss_ctc 43.033081 lr 0.00297696 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:52:26,191 DEBUG TRAIN Batch 3/700 loss 27.300461 loss_att 24.419088 loss_ctc 34.023663 lr 0.00297728 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:52:26,192 DEBUG TRAIN Batch 3/700 loss 26.477179 loss_att 22.295311 loss_ctc 36.234871 lr 0.00297696 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:52:26,193 DEBUG TRAIN Batch 3/700 loss 31.004627 loss_att 26.846252 loss_ctc 40.707504 lr 0.00297712 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:52:26,196 DEBUG TRAIN Batch 3/700 loss 24.684307 loss_att 22.061890 loss_ctc 30.803280 lr 0.00297712 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:52:26,212 DEBUG TRAIN Batch 3/700 loss 27.747204 loss_att 23.596432 loss_ctc 37.432339 lr 0.00297696 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:55:05,103 DEBUG TRAIN Batch 3/800 loss 22.968590 loss_att 20.749294 loss_ctc 28.146948 lr 0.00299296 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:55:05,106 DEBUG TRAIN Batch 3/800 loss 25.107594 loss_att 21.375090 loss_ctc 33.816772 lr 0.00299328 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:55:05,106 DEBUG TRAIN Batch 3/800 loss 21.922626 loss_att 18.293982 loss_ctc 30.389462 lr 0.00299328 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:55:05,106 DEBUG TRAIN Batch 3/800 loss 21.114649 loss_att 18.991932 loss_ctc 26.067656 lr 0.00299296 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:55:05,113 DEBUG TRAIN Batch 3/800 loss 24.572834 loss_att 21.785992 loss_ctc 31.075466 lr 0.00299312 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:55:05,114 DEBUG TRAIN Batch 3/800 loss 26.867809 loss_att 23.310614 loss_ctc 35.167931 lr 0.00299296 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:55:05,114 DEBUG TRAIN Batch 3/800 loss 21.592594 loss_att 18.872082 loss_ctc 27.940456 lr 0.00299312 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 07:55:05,116 DEBUG TRAIN Batch 3/800 loss 22.895458 loss_att 20.682419 loss_ctc 28.059217 lr 0.00299312 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:57:44,907 DEBUG TRAIN Batch 3/900 loss 23.834328 loss_att 19.725403 loss_ctc 33.421822 lr 0.00300896 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 07:57:44,908 DEBUG TRAIN Batch 3/900 loss 22.765743 loss_att 19.722176 loss_ctc 29.867397 lr 0.00300928 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 07:57:44,908 DEBUG TRAIN Batch 3/900 loss 27.339502 loss_att 23.991531 loss_ctc 35.151436 lr 0.00300928 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 07:57:44,915 DEBUG TRAIN Batch 3/900 loss 31.636127 loss_att 26.865286 loss_ctc 42.768089 lr 0.00300896 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 07:57:44,916 DEBUG TRAIN Batch 3/900 loss 21.234234 loss_att 18.445992 loss_ctc 27.740133 lr 0.00300912 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 07:57:44,926 DEBUG TRAIN Batch 3/900 loss 33.902992 loss_att 28.791451 loss_ctc 45.829918 lr 0.00300912 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 07:57:44,950 DEBUG TRAIN Batch 3/900 loss 23.297806 loss_att 20.523376 loss_ctc 29.771469 lr 0.00300896 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 07:57:44,951 DEBUG TRAIN Batch 3/900 loss 29.278065 loss_att 25.560080 loss_ctc 37.953365 lr 0.00300912 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:24,420 DEBUG TRAIN Batch 3/1000 loss 11.124784 loss_att 9.478605 loss_ctc 14.965870 lr 0.00302496 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:24,421 DEBUG TRAIN Batch 3/1000 loss 15.229422 loss_att 13.672118 loss_ctc 18.863129 lr 0.00302496 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:24,421 DEBUG TRAIN Batch 3/1000 loss 13.134912 loss_att 10.956077 loss_ctc 18.218861 lr 0.00302512 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:24,424 DEBUG TRAIN Batch 3/1000 loss 9.387976 loss_att 8.623257 loss_ctc 11.172319 lr 0.00302496 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:24,427 DEBUG TRAIN Batch 3/1000 loss 13.630651 loss_att 11.916914 loss_ctc 17.629374 lr 0.00302528 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:24,430 DEBUG TRAIN Batch 3/1000 loss 11.874067 loss_att 10.221268 loss_ctc 15.730601 lr 0.00302512 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:24,446 DEBUG TRAIN Batch 3/1000 loss 14.576582 loss_att 12.078844 loss_ctc 20.404636 lr 0.00302512 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:24,452 DEBUG TRAIN Batch 3/1000 loss 15.454474 loss_att 13.998725 loss_ctc 18.851223 lr 0.00302528 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:56,907 DEBUG TRAIN Batch 3/1100 loss 20.655224 loss_att 18.744215 loss_ctc 25.114243 lr 0.00304128 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:56,908 DEBUG TRAIN Batch 3/1100 loss 23.065414 loss_att 20.034439 loss_ctc 30.137693 lr 0.00304128 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:56,908 DEBUG TRAIN Batch 3/1100 loss 32.483742 loss_att 27.691727 loss_ctc 43.665115 lr 0.00304096 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:56,914 DEBUG TRAIN Batch 3/1100 loss 32.746944 loss_att 26.940548 loss_ctc 46.295204 lr 0.00304112 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:56,915 DEBUG TRAIN Batch 3/1100 loss 26.163994 loss_att 22.172844 loss_ctc 35.476673 lr 0.00304112 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:56,914 DEBUG TRAIN Batch 3/1100 loss 34.143158 loss_att 29.827053 loss_ctc 44.214062 lr 0.00304096 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:56,915 DEBUG TRAIN Batch 3/1100 loss 31.869787 loss_att 27.115765 loss_ctc 42.962505 lr 0.00304096 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 08:00:56,919 DEBUG TRAIN Batch 3/1100 loss 33.264515 loss_att 28.977524 loss_ctc 43.267498 lr 0.00304112 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 08:03:43,522 DEBUG TRAIN Batch 3/1200 loss 22.858797 loss_att 20.151760 loss_ctc 29.175217 lr 0.00305696 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 08:03:43,525 DEBUG TRAIN Batch 3/1200 loss 21.102293 loss_att 19.319382 loss_ctc 25.262421 lr 0.00305712 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:03:43,526 DEBUG TRAIN Batch 3/1200 loss 41.023972 loss_att 34.788036 loss_ctc 55.574486 lr 0.00305728 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 08:03:43,530 DEBUG TRAIN Batch 3/1200 loss 22.801720 loss_att 19.498655 loss_ctc 30.508873 lr 0.00305696 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 08:03:43,530 DEBUG TRAIN Batch 3/1200 loss 29.113695 loss_att 25.422722 loss_ctc 37.725967 lr 0.00305728 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 08:03:43,532 DEBUG TRAIN Batch 3/1200 loss 24.425373 loss_att 21.657665 loss_ctc 30.883358 lr 0.00305696 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 08:03:43,534 DEBUG TRAIN Batch 3/1200 loss 22.695656 loss_att 20.409294 loss_ctc 28.030495 lr 0.00305712 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 08:03:43,534 DEBUG TRAIN Batch 3/1200 loss 23.959942 loss_att 21.444447 loss_ctc 29.829433 lr 0.00305712 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 08:06:29,562 DEBUG TRAIN Batch 3/1300 loss 27.813309 loss_att 25.302176 loss_ctc 33.672615 lr 0.00307296 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 08:06:29,565 DEBUG TRAIN Batch 3/1300 loss 25.098522 loss_att 21.599981 loss_ctc 33.261787 lr 0.00307328 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 08:06:29,565 DEBUG TRAIN Batch 3/1300 loss 20.664429 loss_att 17.493147 loss_ctc 28.064087 lr 0.00307296 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 08:06:29,571 DEBUG TRAIN Batch 3/1300 loss 29.680897 loss_att 26.772949 loss_ctc 36.466106 lr 0.00307312 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 08:06:29,571 DEBUG TRAIN Batch 3/1300 loss 23.953012 loss_att 21.354572 loss_ctc 30.016041 lr 0.00307328 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 08:06:29,572 DEBUG TRAIN Batch 3/1300 loss 18.743471 loss_att 16.240812 loss_ctc 24.583006 lr 0.00307296 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 08:06:29,572 DEBUG TRAIN Batch 3/1300 loss 21.704277 loss_att 18.320089 loss_ctc 29.600716 lr 0.00307312 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:06:29,574 DEBUG TRAIN Batch 3/1300 loss 23.403761 loss_att 20.166584 loss_ctc 30.957172 lr 0.00307312 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 08:09:18,576 DEBUG TRAIN Batch 3/1400 loss 29.170044 loss_att 25.042952 loss_ctc 38.799923 lr 0.00308896 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 08:09:18,577 DEBUG TRAIN Batch 3/1400 loss 24.338989 loss_att 20.287317 loss_ctc 33.792892 lr 0.00308912 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 08:09:18,579 DEBUG TRAIN Batch 3/1400 loss 32.045082 loss_att 27.516140 loss_ctc 42.612617 lr 0.00308928 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 08:09:18,579 DEBUG TRAIN Batch 3/1400 loss 33.556679 loss_att 29.579596 loss_ctc 42.836540 lr 0.00308896 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 08:09:18,580 DEBUG TRAIN Batch 3/1400 loss 28.117622 loss_att 24.634842 loss_ctc 36.244110 lr 0.00308896 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 08:09:18,585 DEBUG TRAIN Batch 3/1400 loss 36.470818 loss_att 31.918396 loss_ctc 47.093136 lr 0.00308912 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:09:18,586 DEBUG TRAIN Batch 3/1400 loss 24.038128 loss_att 21.201176 loss_ctc 30.657684 lr 0.00308928 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 08:09:18,590 DEBUG TRAIN Batch 3/1400 loss 28.929974 loss_att 23.797148 loss_ctc 40.906563 lr 0.00308912 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:07,363 DEBUG TRAIN Batch 3/1500 loss 8.483038 loss_att 6.995256 loss_ctc 11.954528 lr 0.00310496 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:07,366 DEBUG TRAIN Batch 3/1500 loss 12.468937 loss_att 10.677996 loss_ctc 16.647800 lr 0.00310528 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:07,367 DEBUG TRAIN Batch 3/1500 loss 13.497660 loss_att 11.702551 loss_ctc 17.686247 lr 0.00310496 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:07,369 DEBUG TRAIN Batch 3/1500 loss 12.269682 loss_att 10.595403 loss_ctc 16.176331 lr 0.00310528 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:07,370 DEBUG TRAIN Batch 3/1500 loss 14.328899 loss_att 12.482712 loss_ctc 18.636669 lr 0.00310496 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:07,370 DEBUG TRAIN Batch 3/1500 loss 13.418792 loss_att 11.444454 loss_ctc 18.025579 lr 0.00310512 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:07,373 DEBUG TRAIN Batch 3/1500 loss 9.155084 loss_att 7.552150 loss_ctc 12.895264 lr 0.00310512 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:07,381 DEBUG TRAIN Batch 3/1500 loss 10.164009 loss_att 8.635130 loss_ctc 13.731392 lr 0.00310512 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:40,998 DEBUG TRAIN Batch 3/1600 loss 22.345135 loss_att 19.313723 loss_ctc 29.418427 lr 0.00312112 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:40,999 DEBUG TRAIN Batch 3/1600 loss 23.211575 loss_att 18.939377 loss_ctc 33.180031 lr 0.00312096 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:40,999 DEBUG TRAIN Batch 3/1600 loss 22.652706 loss_att 19.374733 loss_ctc 30.301311 lr 0.00312128 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:41,000 DEBUG TRAIN Batch 3/1600 loss 36.213490 loss_att 29.953548 loss_ctc 50.820023 lr 0.00312096 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:41,007 DEBUG TRAIN Batch 3/1600 loss 27.863945 loss_att 24.449570 loss_ctc 35.830818 lr 0.00312128 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:41,009 DEBUG TRAIN Batch 3/1600 loss 28.523331 loss_att 23.783689 loss_ctc 39.582497 lr 0.00312112 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:41,009 DEBUG TRAIN Batch 3/1600 loss 35.515434 loss_att 31.515776 loss_ctc 44.847977 lr 0.00312096 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 08:12:41,011 DEBUG TRAIN Batch 3/1600 loss 17.521053 loss_att 14.965422 loss_ctc 23.484192 lr 0.00312112 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 08:15:27,403 DEBUG TRAIN Batch 3/1700 loss 23.862350 loss_att 21.386038 loss_ctc 29.640415 lr 0.00313696 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 08:15:27,407 DEBUG TRAIN Batch 3/1700 loss 29.690104 loss_att 27.470177 loss_ctc 34.869930 lr 0.00313696 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 08:15:27,408 DEBUG TRAIN Batch 3/1700 loss 24.119415 loss_att 21.151394 loss_ctc 31.044792 lr 0.00313712 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 08:15:27,412 DEBUG TRAIN Batch 3/1700 loss 29.399912 loss_att 24.741110 loss_ctc 40.270447 lr 0.00313712 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:15:27,412 DEBUG TRAIN Batch 3/1700 loss 21.844261 loss_att 19.197126 loss_ctc 28.020910 lr 0.00313728 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 08:15:27,413 DEBUG TRAIN Batch 3/1700 loss 26.467686 loss_att 22.499899 loss_ctc 35.725857 lr 0.00313696 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 08:15:27,413 DEBUG TRAIN Batch 3/1700 loss 22.378082 loss_att 19.374672 loss_ctc 29.386038 lr 0.00313728 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 08:15:27,415 DEBUG TRAIN Batch 3/1700 loss 23.376614 loss_att 20.810696 loss_ctc 29.363750 lr 0.00313712 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 08:18:14,745 DEBUG TRAIN Batch 3/1800 loss 18.784834 loss_att 16.642014 loss_ctc 23.784748 lr 0.00315296 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 08:18:14,749 DEBUG TRAIN Batch 3/1800 loss 21.755341 loss_att 18.424969 loss_ctc 29.526207 lr 0.00315328 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 08:18:14,749 DEBUG TRAIN Batch 3/1800 loss 18.430969 loss_att 15.629457 loss_ctc 24.967825 lr 0.00315296 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 08:18:14,751 DEBUG TRAIN Batch 3/1800 loss 20.396269 loss_att 18.083395 loss_ctc 25.792974 lr 0.00315296 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 08:18:14,753 DEBUG TRAIN Batch 3/1800 loss 21.917791 loss_att 18.958649 loss_ctc 28.822454 lr 0.00315312 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 08:18:14,753 DEBUG TRAIN Batch 3/1800 loss 25.416206 loss_att 22.088499 loss_ctc 33.180859 lr 0.00315312 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:18:14,756 DEBUG TRAIN Batch 3/1800 loss 20.661747 loss_att 18.016354 loss_ctc 26.834332 lr 0.00315312 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 08:18:14,776 DEBUG TRAIN Batch 3/1800 loss 26.300211 loss_att 22.103622 loss_ctc 36.092251 lr 0.00315328 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 08:21:02,952 DEBUG TRAIN Batch 3/1900 loss 29.468155 loss_att 26.015533 loss_ctc 37.524277 lr 0.00316928 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 08:21:02,952 DEBUG TRAIN Batch 3/1900 loss 25.912167 loss_att 22.313858 loss_ctc 34.308216 lr 0.00316928 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 08:21:02,952 DEBUG TRAIN Batch 3/1900 loss 31.888899 loss_att 27.390713 loss_ctc 42.384666 lr 0.00316896 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 08:21:02,956 DEBUG TRAIN Batch 3/1900 loss 26.318283 loss_att 22.537237 loss_ctc 35.140724 lr 0.00316912 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:21:02,957 DEBUG TRAIN Batch 3/1900 loss 27.289486 loss_att 22.992107 loss_ctc 37.316704 lr 0.00316896 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 08:21:02,960 DEBUG TRAIN Batch 3/1900 loss 23.371857 loss_att 19.958996 loss_ctc 31.335201 lr 0.00316912 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 08:21:02,963 DEBUG TRAIN Batch 3/1900 loss 31.508183 loss_att 27.669495 loss_ctc 40.465118 lr 0.00316912 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 08:21:02,967 DEBUG TRAIN Batch 3/1900 loss 21.341702 loss_att 18.078888 loss_ctc 28.954933 lr 0.00316896 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 08:23:53,068 DEBUG TRAIN Batch 3/2000 loss 11.700214 loss_att 10.065638 loss_ctc 15.514229 lr 0.00318496 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 08:23:53,068 DEBUG TRAIN Batch 3/2000 loss 11.008489 loss_att 9.464183 loss_ctc 14.611868 lr 0.00318512 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 08:23:53,069 DEBUG TRAIN Batch 3/2000 loss 12.510584 loss_att 10.711493 loss_ctc 16.708462 lr 0.00318496 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 08:23:53,073 DEBUG TRAIN Batch 3/2000 loss 10.097509 loss_att 8.536268 loss_ctc 13.740405 lr 0.00318496 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 08:23:53,075 DEBUG TRAIN Batch 3/2000 loss 13.220288 loss_att 12.362640 loss_ctc 15.221468 lr 0.00318512 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:23:53,077 DEBUG TRAIN Batch 3/2000 loss 10.768570 loss_att 8.833479 loss_ctc 15.283781 lr 0.00318512 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 08:23:53,077 DEBUG TRAIN Batch 3/2000 loss 14.840046 loss_att 12.809105 loss_ctc 19.578907 lr 0.00318528 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 08:23:53,095 DEBUG TRAIN Batch 3/2000 loss 10.420510 loss_att 8.486887 loss_ctc 14.932299 lr 0.00318528 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 08:24:26,594 DEBUG TRAIN Batch 3/2100 loss 24.292267 loss_att 20.961597 loss_ctc 32.063831 lr 0.00320096 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 08:24:26,596 DEBUG TRAIN Batch 3/2100 loss 24.524887 loss_att 21.934904 loss_ctc 30.568176 lr 0.00320112 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 08:24:26,599 DEBUG TRAIN Batch 3/2100 loss 23.130161 loss_att 19.514364 loss_ctc 31.567022 lr 0.00320128 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 08:24:26,600 DEBUG TRAIN Batch 3/2100 loss 26.356579 loss_att 21.572952 loss_ctc 37.518372 lr 0.00320096 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 08:24:26,604 DEBUG TRAIN Batch 3/2100 loss 41.999855 loss_att 36.394325 loss_ctc 55.079422 lr 0.00320128 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 08:24:26,605 DEBUG TRAIN Batch 3/2100 loss 24.151424 loss_att 20.916147 loss_ctc 31.700401 lr 0.00320112 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-29 08:24:26,615 DEBUG TRAIN Batch 3/2100 loss 27.355028 loss_att 24.412273 loss_ctc 34.221458 lr 0.00320112 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:24:26,623 DEBUG TRAIN Batch 3/2100 loss 19.852022 loss_att 18.427229 loss_ctc 23.176540 lr 0.00320096 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 08:27:15,927 DEBUG TRAIN Batch 3/2200 loss 23.434784 loss_att 19.664867 loss_ctc 32.231258 lr 0.00321712 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-29 08:27:15,927 DEBUG TRAIN Batch 3/2200 loss 24.208115 loss_att 20.472408 loss_ctc 32.924759 lr 0.00321696 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-29 08:27:15,928 DEBUG TRAIN Batch 3/2200 loss 32.150852 loss_att 27.295597 loss_ctc 43.479774 lr 0.00321696 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-29 08:27:15,929 DEBUG TRAIN Batch 3/2200 loss 31.394302 loss_att 26.711796 loss_ctc 42.320152 lr 0.00321696 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-29 08:27:15,929 DEBUG TRAIN Batch 3/2200 loss 20.927814 loss_att 18.948246 loss_ctc 25.546810 lr 0.00321728 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-29 08:27:15,936 DEBUG TRAIN Batch 3/2200 loss 21.364628 loss_att 18.640478 loss_ctc 27.720974 lr 0.00321712 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-29 08:27:15,937 DEBUG TRAIN Batch 3/2200 loss 17.421227 loss_att 15.351237 loss_ctc 22.251202 lr 0.00321728 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-29 08:27:15,963 DEBUG TRAIN Batch 3/2200 loss 28.369703 loss_att 24.331291 loss_ctc 37.792664 lr 0.00321712 rank 0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "job_name = est.fit({\"training\":training})\n",
    "#job_name = est.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d07b7c9-acde-4d0a-b179-3134d28f6f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact saved at:\n",
      " s3://sagemaker-us-east-1-348052051973/wenet/wenet-2022-12-08-05-36-25-104/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_data = est.model_data\n",
    "print(\"Model artifact saved at:\\n\", model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55192e1-23f5-4c2f-8c0f-5d1c43655226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
