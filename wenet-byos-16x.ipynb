{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fecbc5ab-852b-4157-916d-8feb9207b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3.__version__:1.26.8\n",
      "sagemaker.__version__:2.116.0\n",
      "bucket:sagemaker-us-east-1-348052051973\n",
      "role:arn:aws:iam::348052051973:role/service-role/AmazonSageMakerServiceCatalogProductsExecutionRole\n",
      "CPU times: user 1.23 s, sys: 925 ms, total: 2.16 s\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.utils import unique_name_from_base\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = get_execution_role()\n",
    "prefix = 'wenet'\n",
    "output_path = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "print(\"boto3.__version__:{}\".format(boto3.__version__))\n",
    "print(\"sagemaker.__version__:{}\".format(sagemaker.__version__))\n",
    "print(\"bucket:{}\".format(bucket))\n",
    "print(\"role:{}\".format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aad794e-6139-4422-892b-936ccaaa3c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Copy the wenet/examples/librispeech/s0/*.sh and wenet/examples/librispeech/s0/local to wenet/ as requested by Sagemaker\n",
       "Overwrite the wenet/wenet/bin/train.py with the given one\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "Copy the wenet/examples/librispeech/s0/*.sh and wenet/examples/librispeech/s0/local to wenet/ as requested by Sagemaker\n",
    "Overwrite the wenet/wenet/bin/train.py with the given one\n",
    "Change the /root/wenet to /opt/ml/input in all data.list files (especially for train_960 and dev)\n",
    "The \"Librispeech\" in data.list file in Github has the wrong captalization because it's wrong when I upload it. Please change it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "922ce9af-f711-4702-b811-4379efbcc8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.5 ms, sys: 16 ms, total: 64.6 ms\n",
      "Wall time: 63.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "instance_type = \"ml.p3.16xlarge\"\n",
    "\n",
    "hyperparameters = {\n",
    "    'datadir':'/opt/ml/input/data/training',\n",
    "    'stage': '4',\n",
    "    'stop_stage': '5',\n",
    "    'train_config': 'examples/librispeech/s0/conf/train_conformer.yaml',\n",
    "    'model_dir': '/opt/ml/model',\n",
    "}\n",
    "\n",
    "est = PyTorch(\n",
    "    entry_point=\"run-8gpu.sh\",\n",
    "    source_dir=\"./wenet\",\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=200,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=prefix,\n",
    "    hyperparameters = hyperparameters,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "991f505d-b0c4-4aa1-abae-fd3c3d4534d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "prefix_dataset = \"wenet/export\"\n",
    "loc =f\"s3://{bucket}/{prefix_dataset}\"\n",
    "\n",
    "training = TrainingInput(\n",
    "    s3_data_type='S3Prefix', # Available Options: S3Prefix | ManifestFile | AugmentedManifestFile\n",
    "    s3_data=loc,\n",
    "    distribution='FullyReplicated', # Available Options: FullyReplicated | ShardedByS3Key \n",
    "    input_mode='FastFile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653e3f8-9feb-449b-9dad-a53b1735e33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-28 03:24:31 Starting - Starting the training job......\n",
      "2022-11-28 03:25:29 Starting - Preparing the instances for training.........\n",
      "2022-11-28 03:26:53 Downloading - Downloading input data......\n",
      "2022-11-28 03:27:44 Training - Downloading the training image.................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-11-28 03:30:39,044 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-11-28 03:30:39,120 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-11-28 03:30:39,135 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\n",
      "2022-11-28 03:30:35 Training - Training image download completed. Training in progress.\u001b[34m2022-11-28 03:30:50,654 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"datadir\": \"/opt/ml/input/data/training\",\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"stage\": \"4\",\n",
      "        \"stop_stage\": \"5\",\n",
      "        \"train_config\": \"examples/librispeech/s0/conf/train_conformer.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.16xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"wenet-2022-11-28-03-22-54-109\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-348052051973/wenet-2022-11-28-03-22-54-109/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run-8gpu.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run-8gpu.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"datadir\":\"/opt/ml/input/data/training\",\"model_dir\":\"/opt/ml/model\",\"stage\":\"4\",\"stop_stage\":\"5\",\"train_config\":\"examples/librispeech/s0/conf/train_conformer.yaml\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run-8gpu.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.16xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run-8gpu.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-348052051973/wenet-2022-11-28-03-22-54-109/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.16xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"datadir\":\"/opt/ml/input/data/training\",\"model_dir\":\"/opt/ml/model\",\"stage\":\"4\",\"stop_stage\":\"5\",\"train_config\":\"examples/librispeech/s0/conf/train_conformer.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"wenet-2022-11-28-03-22-54-109\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-348052051973/wenet-2022-11-28-03-22-54-109/source/sourcedir.tar.gz\",\"module_name\":\"run-8gpu.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run-8gpu.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--datadir\",\"/opt/ml/input/data/training\",\"--model_dir\",\"/opt/ml/model\",\"--stage\",\"4\",\"--stop_stage\",\"5\",\"--train_config\",\"examples/librispeech/s0/conf/train_conformer.yaml\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATADIR=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_STAGE=4\u001b[0m\n",
      "\u001b[34mSM_HP_STOP_STAGE=5\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_CONFIG=examples/librispeech/s0/conf/train_conformer.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221003-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./run-8gpu.sh --datadir /opt/ml/input/data/training --model_dir /opt/ml/model --stage 4 --stop_stage 5 --train_config examples/librispeech/s0/conf/train_conformer.yaml\"\u001b[0m\n",
      "\u001b[34mCollecting torchaudio==0.10.0\u001b[0m\n",
      "\u001b[34mDownloading torchaudio-0.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 58.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (9.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 58.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 71.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboardX\u001b[0m\n",
      "\u001b[34mDownloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.4/125.4 kB 21.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting typeguard\u001b[0m\n",
      "\u001b[34mDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting textgrid\u001b[0m\n",
      "\u001b[34mDownloading TextGrid-1.5-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytest\u001b[0m\n",
      "\u001b[34mDownloading pytest-7.2.0-py3-none-any.whl (316 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.8/316.8 kB 33.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting flake8==3.8.2\u001b[0m\n",
      "\u001b[34mDownloading flake8-3.8.2-py2.py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.7/72.7 kB 15.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting flake8-bugbear\u001b[0m\n",
      "\u001b[34mDownloading flake8_bugbear-22.10.27-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-comprehensions\u001b[0m\n",
      "\u001b[34mDownloading flake8_comprehensions-3.10.1-py3-none-any.whl (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-executable\u001b[0m\n",
      "\u001b[34mDownloading flake8_executable-2.1.2-py3-none-any.whl (35 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-pyi==20.5.0\u001b[0m\n",
      "\u001b[34mDownloading flake8_pyi-20.5.0-py36-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting mccabe\u001b[0m\n",
      "\u001b[34mDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pycodestyle==2.6.0\u001b[0m\n",
      "\u001b[34mDownloading pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.4/41.4 kB 6.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyflakes==2.2.0\u001b[0m\n",
      "\u001b[34mDownloading pyflakes-2.2.0-py2.py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 11.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting torch==1.10.0\u001b[0m\n",
      "\u001b[34mDownloading torch-1.10.0-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 881.9/881.9 MB 1.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mccabe\u001b[0m\n",
      "\u001b[34mDownloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs in /opt/conda/lib/python3.8/site-packages (from flake8-pyi==20.5.0->-r requirements.txt (line 14)) (21.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.10.0->torchaudio==0.10.0->-r requirements.txt (line 1)) (4.3.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.7.0,>=0.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 71.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.24.3\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.50.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 70.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 5)) (3.19.6)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.3.0-py3-none-any.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.6/124.6 kB 23.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 5)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 5)) (2.2.2)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 56.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 5)) (65.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 5)) (0.37.1)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.14.1-py2.py3-none-any.whl (175 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175.4/175.4 kB 30.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.1-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 kB 7.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 5)) (2.28.1)\u001b[0m\n",
      "\u001b[34mCollecting tomli>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading tomli-2.0.1-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting iniconfig\u001b[0m\n",
      "\u001b[34mDownloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pluggy<2.0,>=0.12\u001b[0m\n",
      "\u001b[34mDownloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from pytest->-r requirements.txt (line 9)) (21.3)\u001b[0m\n",
      "\u001b[34mCollecting exceptiongroup>=1.0.0rc8\u001b[0m\n",
      "\u001b[34mDownloading exceptiongroup-1.0.4-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 17.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 5)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 5)) (1.26.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 5)) (2022.9.24)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 5)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 5)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->pytest->-r requirements.txt (line 9)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 5)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 10.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: textgrid, tensorboard-plugin-wit, sentencepiece, mccabe, iniconfig, typeguard, torch, tomli, tensorboardX, tensorboard-data-server, pyflakes, pycodestyle, pyasn1-modules, pluggy, oauthlib, grpcio, exceptiongroup, cachetools, absl-py, torchaudio, requests-oauthlib, pytest, markdown, google-auth, flake8, google-auth-oauthlib, flake8-pyi, flake8-executable, flake8-comprehensions, flake8-bugbear, tensorboard\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 1.11.0+cu113\u001b[0m\n",
      "\u001b[34mUninstalling torch-1.11.0+cu113:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-1.11.0+cu113\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torchaudio\u001b[0m\n",
      "\u001b[34mFound existing installation: torchaudio 0.11.0+cu113\u001b[0m\n",
      "\u001b[34mUninstalling torchaudio-0.11.0+cu113:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torchaudio-0.11.0+cu113\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mtorchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.10.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.3.0 cachetools-5.2.0 exceptiongroup-1.0.4 flake8-3.8.2 flake8-bugbear-22.10.27 flake8-comprehensions-3.10.1 flake8-executable-2.1.2 flake8-pyi-20.5.0 google-auth-2.14.1 google-auth-oauthlib-0.4.6 grpcio-1.50.0 iniconfig-1.1.1 markdown-3.4.1 mccabe-0.6.1 oauthlib-3.2.2 pluggy-1.0.0 pyasn1-modules-0.2.8 pycodestyle-2.6.0 pyflakes-2.2.0 pytest-7.2.0 requests-oauthlib-1.3.1 sentencepiece-0.1.97 tensorboard-2.11.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorboardX-2.5.1 textgrid-1.5 tomli-2.0.1 torch-1.10.0 torchaudio-0.10.0 typeguard-2.13.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 22.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mdictionary: data/lang_char/train_960_unigram5000_units.txt\u001b[0m\n",
      "\u001b[34m./run-8gpu.sh: init method is file:///opt/ml/code/exp/sp_spec_aug/ddp_init\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 0\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 1\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 2\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 3\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 4\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 5\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 6\u001b[0m\n",
      "\u001b[34mStarting GPU Id: 7\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescoreFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:47,714 INFO training on multiple gpus, this gpu 1\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:47,714 INFO training on multiple gpus, this gpu 7\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:47,714 INFO training on multiple gpus, this gpu 4\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:47,714 INFO training on multiple gpus, this gpu 6\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:47,714 INFO training on multiple gpus, this gpu 0\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:47,715 INFO training on multiple gpus, this gpu 5\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:47,715 INFO training on multiple gpus, this gpu 3\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:47,715 INFO training on multiple gpus, this gpu 2\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,403 INFO Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,434 INFO Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,469 INFO Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,519 INFO Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,576 INFO Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,627 INFO Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,692 INFO Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,758 INFO Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,758 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,760 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,760 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,760 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,761 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,764 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,764 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:48,765 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,152 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,152 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,152 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,152 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,152 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,152 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,152 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,153 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,153 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,153 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,153 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,153 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,153 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,153 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,153 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:49,154 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34mASRModel(\n",
      "  (encoder): ConformerEncoder(\n",
      "    (global_cmvn): GlobalCMVN()\n",
      "    (embed): Conv2dSubsampling4(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (out): Sequential(\n",
      "        (0): Linear(in_features=4864, out_features=256, bias=True)\n",
      "      )\n",
      "      (pos_enc): RelPositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoders): ModuleList(\n",
      "      (0): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (1): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (2): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (3): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (4): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (5): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (6): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (7): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (8): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (9): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (10): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (11): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed): Sequential(\n",
      "      (0): Embedding(5001, 256)\n",
      "      (1): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_layer): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (decoders): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (4): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (5): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ctc): CTC(\n",
      "    (ctc_lo): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (ctc_loss): CTCLoss()\n",
      "  )\n",
      "  (criterion_att): LabelSmoothingLoss(\n",
      "    (criterion): KLDivLoss()\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mASRModel(\n",
      "  (encoder): ConformerEncoder(\n",
      "    (global_cmvn): GlobalCMVN()\n",
      "    (embed): Conv2dSubsampling4(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (out): Sequential(\n",
      "        (0): Linear(in_features=4864, out_features=256, bias=True)\n",
      "      )\n",
      "      (pos_enc): RelPositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoders): ModuleList(\n",
      "      (0): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (1): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (2): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (3): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (4): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (5): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (6): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (7): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (8): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (9): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (10): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (11): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed): Sequential(\n",
      "      (0): Embedding(5001, 256)\n",
      "      (1): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_layer): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (decoders): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (4): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplac\u001b[0m\n",
      "\u001b[34me=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (5): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ctc): CTC(\n",
      "    (ctc_lo): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (ctc_loss): CTCLoss()\n",
      "  )\n",
      "  (criterion_att): LabelSmoothingLoss(\n",
      "    (criterion): KLDivLoss()\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mASRModel(\n",
      "  (encoder): ConformerEncoder(\n",
      "    (global_cmvn): GlobalCMVN()\n",
      "    (embed): Conv2dSubsampling4(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (out): Sequential(\n",
      "        (0): Linear(in_features=4864, out_features=256, bias=True)\n",
      "      )\n",
      "      (pos_enc): RelPositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoders): ModuleList(\n",
      "      (0): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (1): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (2): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (3): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (4): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (5): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (6): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (7): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (8): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (9): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (10): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (11): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed): Sequential(\n",
      "      (0): Embedding(5001, 256)\n",
      "      (1): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_layer): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (decoders): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (4): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (5): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ctc): CTC(\n",
      "    (ctc_lo): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (ctc_loss): CTCLoss()\n",
      "  )\n",
      "  (criterion_att): LabelSmoothingLoss(\n",
      "    (criterion): KLDivLoss()\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mASRModel(\n",
      "  (encoder): ConformerEncoder(\n",
      "    (global_cmvn): GlobalCMVN()\n",
      "    (embed): Conv2dSubsampling4(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (out): Sequential(\n",
      "        (0): Linear(in_features=4864, out_features=256, bias=True)\n",
      "      )\n",
      "      (pos_enc): RelPositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoders): ModuleList(\n",
      "      (0): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (1): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (2): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (3): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (4): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (5): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (6): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (7): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (8): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (9): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (10): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (11): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed): Sequential(\n",
      "      (0): Embedding(5001, 256)\n",
      "      (1): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_layer): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (decoders): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (4): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (5): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ctc): CTC(\n",
      "    (ctc_lo): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (ctc_loss): CTCLoss()\n",
      "  )\n",
      "  (criterion_att): LabelSmoothingLoss(\n",
      "    (criterion): KLDivLoss()\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mASRModel(\n",
      "  (encoder): ConformerEncoder(\n",
      "    (global_cmvn): GlobalCMVN()\n",
      "    (embed): Conv2dSubsampling4(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (out): Sequential(\n",
      "        (0): Linear(in_features=4864, out_features=256, bias=True)\n",
      "      )\n",
      "      (pos_enc): RelPositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoders): ModuleList(\n",
      "      (0): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (1): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (2): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (3): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (4): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (5): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (6): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (7): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (8): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (9): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (10): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (11): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed): Sequential(\n",
      "      (0): Embedding(5001, 256)\n",
      "      (1): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_layer): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (decoders): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (4): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (5): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ctc): CTC(\n",
      "    (ctc_lo): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (ctc_loss): CTCLoss()\n",
      "  )\n",
      "  (criterion_att): LabelSmoothingLoss(\n",
      "    (criterion): KLDivLoss()\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mASRModel(\n",
      "  (encoder): ConformerEncoder(\n",
      "    (global_cmvn): GlobalCMVN()\n",
      "    (embed): Conv2dSubsampling4(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (out): Sequential(\n",
      "        (0): Linear(in_features=4864, out_features=256, bias=True)\n",
      "      )\n",
      "      (pos_enc): RelPositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoders): ModuleList(\n",
      "      (0): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (1): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (2): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (3): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (4): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (5): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (6): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (7): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (8): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (9): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (10): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (11): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed): Sequential(\n",
      "      (0): Embedding(5001, 256)\n",
      "      (1): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_layer): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (decoders): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "      ASRModel(\n",
      "  (encoder): ConformerEncoder(\n",
      "    (global_cmvn): GlobalCMVN()\n",
      "    (embed): Conv2dSubsampling4(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (out): Sequential(\n",
      "        (0): Linear(in_features=4864, out_features=256, bias=True)\n",
      "      )\n",
      "      (pos_enc): RelPositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoders): ModuleList(\n",
      "      (0): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (1): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (2): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\u001b[0m\n",
      "\u001b[34m)\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (3): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (4): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (5): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (6): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (7): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (8): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (9): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (10): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (11): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed): Sequential(\n",
      "      (0): Embedding(5001, 256)\n",
      "      (1): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_layer): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (decoders): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (4): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (5): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ctc): CTC(\n",
      "    (ctc_lo): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (ctc_loss): CTCLoss()\n",
      "  )\n",
      "  (criterion_att): LabelSmoothingLoss(\n",
      "    (criterion): KLDivLoss()\n",
      "  )\u001b[0m\n",
      "\u001b[34m)ASRModel(\n",
      "  (encoder): ConformerEncoder(\n",
      "    (global_cmvn): GlobalCMVN()\n",
      "    (embed): Conv2dSubsampling4(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (out): Sequential(\n",
      "        (0): Linear(in_features=4864, out_features=256, bias=True)\n",
      "      )\n",
      "      (pos_enc): RelPositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoders): ModuleList(\n",
      "      (0): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (1): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (2): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (3): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (4): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (5): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (6): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (7): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (8): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (9): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (10): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "      (11): ConformerEncoderLayer(\n",
      "        (self_attn): RelPositionMultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear_pos): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (feed_forward_macaron): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): SiLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (conv_module): ConvolutionModule(\n",
      "          (pointwise_conv1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "          (depthwise_conv): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)\n",
      "          (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (pointwise_conv2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "          (activation): SiLU()\n",
      "        )\n",
      "        (norm_ff): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_mha): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_ff_macaron): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_conv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_final): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed): Sequential(\n",
      "      (0): Embedding(5001, 256)\n",
      "      (1): PositionalEncoding(\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (after_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (output_layer): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (decoders): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (4): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (5): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ctc): CTC(\n",
      "    (ctc_lo): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (ctc_loss): CTCLoss()\n",
      "  )\n",
      "  (criterion_att): LabelSmoothingLoss(\n",
      "    (criterion): KLDivLoss(\u001b[0m\n",
      "\u001b[34m)\n",
      "  )\u001b[0m\n",
      "\u001b[34m)the number of model params: 46,788,626_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (3): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (4): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "      (5): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linear_q): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_k): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_v): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (linear_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (concat_linear1): Identity()\n",
      "        (concat_linear2): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ctc): CTC(\n",
      "    (ctc_lo): Linear(in_features=256, out_features=5001, bias=True)\n",
      "    (ctc_loss): CTCLoss()\n",
      "  )\n",
      "  (criterion_att): LabelSmoothingLoss(\n",
      "    (criterion): KLDivLoss()\n",
      "  )\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34malgo-1:166:166 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:166:166 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34mNCCL version 2.10.3+cuda10.2\u001b[0m\n",
      "\u001b[34malgo-1:196:196 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:196:196 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:176:176 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:176:176 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:191:191 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:191:191 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:200:200 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:200:200 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:181:181 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:181:181 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:171:171 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:171:171 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34malgo-1:186:186 [0] ofi_init:1304 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:186:186 [0] ofi_init:1355 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,738 INFO Checkpoint: save to checkpoint /opt/ml/model/init.pt\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,741 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,741 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,742 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,742 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,743 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,743 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,743 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,744 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,744 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,744 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,744 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,745 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,746 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:56,747 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:57,101 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2022-11-28 03:31:57,103 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m2022-11-28 03:34:58,532 DEBUG TRAIN Batch 0/0 loss 184.522125 loss_att 66.375015 loss_ctc 460.198700 lr 0.00000016 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 03:34:58,536 DEBUG TRAIN Batch 0/0 loss 180.684357 loss_att 65.109192 loss_ctc 450.359711 lr 0.00000016 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 03:34:58,540 DEBUG TRAIN Batch 0/0 loss 196.635254 loss_att 78.269943 loss_ctc 472.820984 lr 0.00000016 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 03:34:58,541 DEBUG TRAIN Batch 0/0 loss 195.734116 loss_att 71.475433 loss_ctc 485.670990 lr 0.00000016 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 03:34:58,551 DEBUG TRAIN Batch 0/0 loss 175.876190 loss_att 61.516464 loss_ctc 442.715515 lr 0.00000016 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 03:34:58,552 DEBUG TRAIN Batch 0/0 loss 190.634476 loss_att 78.002632 loss_ctc 453.442108 lr 0.00000016 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 03:34:58,555 DEBUG TRAIN Batch 0/0 loss 190.706223 loss_att 71.526863 loss_ctc 468.791382 lr 0.00000016 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 03:34:58,720 DEBUG TRAIN Batch 0/0 loss 180.920181 loss_att 63.337517 loss_ctc 455.279724 lr 0.00000016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 03:35:34,604 DEBUG TRAIN Batch 0/100 loss 368.910645 loss_att 333.421936 loss_ctc 451.717621 lr 0.00001616 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 03:35:34,605 DEBUG TRAIN Batch 0/100 loss 379.976868 loss_att 344.268066 loss_ctc 463.297455 lr 0.00001616 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 03:35:34,606 DEBUG TRAIN Batch 0/100 loss 370.643585 loss_att 335.451569 loss_ctc 452.758240 lr 0.00001616 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 03:35:34,606 DEBUG TRAIN Batch 0/100 loss 365.449066 loss_att 332.041992 loss_ctc 443.398895 lr 0.00001616 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 03:35:34,607 DEBUG TRAIN Batch 0/100 loss 352.842529 loss_att 316.813721 loss_ctc 436.909668 lr 0.00001616 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 03:35:34,607 DEBUG TRAIN Batch 0/100 loss 365.699768 loss_att 331.116333 loss_ctc 446.394501 lr 0.00001616 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 03:35:34,611 DEBUG TRAIN Batch 0/100 loss 359.956238 loss_att 325.144073 loss_ctc 441.184692 lr 0.00001616 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 03:35:34,642 DEBUG TRAIN Batch 0/100 loss 404.292847 loss_att 369.820251 loss_ctc 484.728943 lr 0.00001616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 03:38:33,348 DEBUG TRAIN Batch 0/200 loss 344.109375 loss_att 322.285767 loss_ctc 395.031128 lr 0.00003216 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 03:38:33,348 DEBUG TRAIN Batch 0/200 loss 304.450073 loss_att 284.736267 loss_ctc 350.448914 lr 0.00003216 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 03:38:33,350 DEBUG TRAIN Batch 0/200 loss 367.119659 loss_att 343.792542 loss_ctc 421.549652 lr 0.00003216 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 03:38:33,351 DEBUG TRAIN Batch 0/200 loss 315.937347 loss_att 294.902649 loss_ctc 365.018280 lr 0.00003216 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 03:38:33,355 DEBUG TRAIN Batch 0/200 loss 339.194092 loss_att 318.180145 loss_ctc 388.226654 lr 0.00003216 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 03:38:33,356 DEBUG TRAIN Batch 0/200 loss 347.385193 loss_att 325.522827 loss_ctc 398.397308 lr 0.00003216 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 03:38:33,356 DEBUG TRAIN Batch 0/200 loss 304.639221 loss_att 284.795776 loss_ctc 350.940613 lr 0.00003216 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 03:38:33,375 DEBUG TRAIN Batch 0/200 loss 352.296906 loss_att 329.333221 loss_ctc 405.878845 lr 0.00003216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 04:42:16,422 DEBUG TRAIN Batch 0/2800 loss 94.697021 loss_att 101.695671 loss_ctc 78.366844 lr 0.00044816 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 04:42:16,422 DEBUG TRAIN Batch 0/2800 loss 88.393715 loss_att 97.230003 loss_ctc 67.775696 lr 0.00044816 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 04:42:16,426 DEBUG TRAIN Batch 0/2800 loss 98.325401 loss_att 106.323395 loss_ctc 79.663406 lr 0.00044816 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 04:42:16,433 DEBUG TRAIN Batch 0/2800 loss 100.103516 loss_att 110.398514 loss_ctc 76.081833 lr 0.00044816 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 04:42:16,433 DEBUG TRAIN Batch 0/2800 loss 97.390175 loss_att 108.179916 loss_ctc 72.214127 lr 0.00044816 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 04:42:16,434 DEBUG TRAIN Batch 0/2800 loss 110.902832 loss_att 121.069336 loss_ctc 87.180992 lr 0.00044816 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 04:42:16,438 DEBUG TRAIN Batch 0/2800 loss 97.568222 loss_att 109.865135 loss_ctc 68.875427 lr 0.00044816 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 04:42:16,445 DEBUG TRAIN Batch 0/2800 loss 98.652824 loss_att 105.946640 loss_ctc 81.633934 lr 0.00044816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 04:45:04,732 DEBUG TRAIN Batch 0/2900 loss 140.455017 loss_att 151.894012 loss_ctc 113.764061 lr 0.00046416 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 04:45:04,733 DEBUG TRAIN Batch 0/2900 loss 130.351959 loss_att 144.968262 loss_ctc 96.247246 lr 0.00046416 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 04:45:04,735 DEBUG TRAIN Batch 0/2900 loss 133.004364 loss_att 145.528305 loss_ctc 103.781845 lr 0.00046416 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 04:45:04,738 DEBUG TRAIN Batch 0/2900 loss 153.975082 loss_att 164.082809 loss_ctc 130.390381 lr 0.00046416 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 04:45:04,742 DEBUG TRAIN Batch 0/2900 loss 147.494583 loss_att 161.974426 loss_ctc 113.708298 lr 0.00046416 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 04:45:04,743 DEBUG TRAIN Batch 0/2900 loss 131.083908 loss_att 147.455307 loss_ctc 92.883972 lr 0.00046416 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 04:45:04,754 DEBUG TRAIN Batch 0/2900 loss 129.104340 loss_att 145.373337 loss_ctc 91.143311 lr 0.00046416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 04:45:04,760 DEBUG TRAIN Batch 0/2900 loss 125.866524 loss_att 139.021423 loss_ctc 95.171753 lr 0.00046416 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 04:47:53,453 DEBUG TRAIN Batch 0/3000 loss 25.270391 loss_att 24.894264 loss_ctc 26.148018 lr 0.00048016 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 04:47:53,455 DEBUG TRAIN Batch 0/3000 loss 27.671841 loss_att 27.320791 loss_ctc 28.490955 lr 0.00048016 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 04:47:53,456 DEBUG TRAIN Batch 0/3000 loss 28.206161 loss_att 28.981594 loss_ctc 26.396820 lr 0.00048016 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 04:47:53,457 DEBUG TRAIN Batch 0/3000 loss 26.178368 loss_att 26.483276 loss_ctc 25.466911 lr 0.00048016 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 04:47:53,463 DEBUG TRAIN Batch 0/3000 loss 31.480633 loss_att 30.765865 loss_ctc 33.148422 lr 0.00048016 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 04:47:53,483 DEBUG TRAIN Batch 0/3000 loss 26.072458 loss_att 26.270208 loss_ctc 25.611046 lr 0.00048016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 04:47:53,527 DEBUG TRAIN Batch 0/3000 loss 31.086727 loss_att 30.897072 loss_ctc 31.529257 lr 0.00048016 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 04:47:53,528 DEBUG TRAIN Batch 0/3000 loss 22.745768 loss_att 23.279058 loss_ctc 21.501417 lr 0.00048016 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 04:48:26,208 DEBUG TRAIN Batch 0/3100 loss 99.842346 loss_att 108.224014 loss_ctc 80.285126 lr 0.00049616 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 04:48:26,213 DEBUG TRAIN Batch 0/3100 loss 102.046654 loss_att 109.376190 loss_ctc 84.944412 lr 0.00049616 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 04:48:26,220 DEBUG TRAIN Batch 0/3100 loss 100.968552 loss_att 107.283340 loss_ctc 86.234055 lr 0.00049616 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 04:48:26,222 DEBUG TRAIN Batch 0/3100 loss 104.326653 loss_att 110.090027 loss_ctc 90.878777 lr 0.00049616 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 04:48:26,223 DEBUG TRAIN Batch 0/3100 loss 104.059196 loss_att 112.606812 loss_ctc 84.114761 lr 0.00049616 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 04:48:26,225 DEBUG TRAIN Batch 0/3100 loss 124.835075 loss_att 134.030746 loss_ctc 103.378525 lr 0.00049616 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 04:48:26,226 DEBUG TRAIN Batch 0/3100 loss 103.215858 loss_att 110.045586 loss_ctc 87.279846 lr 0.00049616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 04:48:26,230 DEBUG TRAIN Batch 0/3100 loss 128.756760 loss_att 136.531967 loss_ctc 110.614609 lr 0.00049616 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 04:51:12,472 DEBUG TRAIN Batch 0/3200 loss 115.326775 loss_att 122.970261 loss_ctc 97.491989 lr 0.00051216 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 04:51:12,473 DEBUG TRAIN Batch 0/3200 loss 101.457489 loss_att 110.113670 loss_ctc 81.259727 lr 0.00051216 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 04:51:12,474 DEBUG TRAIN Batch 0/3200 loss 109.746651 loss_att 116.910912 loss_ctc 93.030029 lr 0.00051216 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 04:51:12,474 DEBUG TRAIN Batch 0/3200 loss 102.146034 loss_att 107.909325 loss_ctc 88.698349 lr 0.00051216 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 04:51:12,475 DEBUG TRAIN Batch 0/3200 loss 111.681145 loss_att 119.930359 loss_ctc 92.432983 lr 0.00051216 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 04:51:12,482 DEBUG TRAIN Batch 0/3200 loss 118.112953 loss_att 125.138428 loss_ctc 101.720169 lr 0.00051216 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 04:51:12,482 DEBUG TRAIN Batch 0/3200 loss 93.377563 loss_att 100.985153 loss_ctc 75.626511 lr 0.00051216 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 04:51:12,495 DEBUG TRAIN Batch 0/3200 loss 124.102303 loss_att 133.529663 loss_ctc 102.105125 lr 0.00051216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 04:53:56,441 DEBUG TRAIN Batch 0/3300 loss 75.174858 loss_att 77.211395 loss_ctc 70.422928 lr 0.00052816 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 04:53:56,447 DEBUG TRAIN Batch 0/3300 loss 80.358772 loss_att 84.534157 loss_ctc 70.616203 lr 0.00052816 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 04:53:56,450 DEBUG TRAIN Batch 0/3300 loss 78.886490 loss_att 83.328079 loss_ctc 68.522797 lr 0.00052816 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 04:53:56,451 DEBUG TRAIN Batch 0/3300 loss 81.054749 loss_att 81.893570 loss_ctc 79.097504 lr 0.00052816 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 04:53:56,454 DEBUG TRAIN Batch 0/3300 loss 66.283600 loss_att 70.396408 loss_ctc 56.687035 lr 0.00052816 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 04:53:56,454 DEBUG TRAIN Batch 0/3300 loss 68.869675 loss_att 70.773361 loss_ctc 64.427727 lr 0.00052816 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 04:53:56,459 DEBUG TRAIN Batch 0/3300 loss 77.298035 loss_att 78.985405 loss_ctc 73.360825 lr 0.00052816 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 04:53:56,463 DEBUG TRAIN Batch 0/3300 loss 85.614288 loss_att 89.382553 loss_ctc 76.821671 lr 0.00052816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 04:56:43,038 DEBUG TRAIN Batch 0/3400 loss 93.952835 loss_att 99.626816 loss_ctc 80.713547 lr 0.00054416 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 04:56:43,042 DEBUG TRAIN Batch 0/3400 loss 98.151855 loss_att 102.801842 loss_ctc 87.301880 lr 0.00054416 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 04:56:43,047 DEBUG TRAIN Batch 0/3400 loss 123.791542 loss_att 128.164841 loss_ctc 113.587181 lr 0.00054416 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 04:56:43,048 DEBUG TRAIN Batch 0/3400 loss 100.398758 loss_att 105.327110 loss_ctc 88.899277 lr 0.00054416 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 04:56:43,050 DEBUG TRAIN Batch 0/3400 loss 111.873169 loss_att 120.274780 loss_ctc 92.269409 lr 0.00054416 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 04:56:43,054 DEBUG TRAIN Batch 0/3400 loss 94.015167 loss_att 97.972549 loss_ctc 84.781281 lr 0.00054416 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 04:56:43,062 DEBUG TRAIN Batch 0/3400 loss 102.774162 loss_att 107.848137 loss_ctc 90.934875 lr 0.00054416 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 04:56:43,067 DEBUG TRAIN Batch 0/3400 loss 94.988472 loss_att 98.995964 loss_ctc 85.637657 lr 0.00054416 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:19,373 DEBUG TRAIN Batch 0/5500 loss 17.586403 loss_att 15.765133 loss_ctc 21.836029 lr 0.00088016 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:19,376 DEBUG TRAIN Batch 0/5500 loss 16.539097 loss_att 14.877238 loss_ctc 20.416767 lr 0.00088016 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:19,376 DEBUG TRAIN Batch 0/5500 loss 22.534241 loss_att 20.108852 loss_ctc 28.193481 lr 0.00088016 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:19,378 DEBUG TRAIN Batch 0/5500 loss 21.280031 loss_att 18.982338 loss_ctc 26.641312 lr 0.00088016 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:19,384 DEBUG TRAIN Batch 0/5500 loss 17.386683 loss_att 15.843669 loss_ctc 20.987043 lr 0.00088016 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:19,391 DEBUG TRAIN Batch 0/5500 loss 17.784660 loss_att 16.000439 loss_ctc 21.947845 lr 0.00088016 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:19,403 DEBUG TRAIN Batch 0/5500 loss 18.825180 loss_att 16.861963 loss_ctc 23.406019 lr 0.00088016 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:19,439 DEBUG TRAIN Batch 0/5500 loss 16.683937 loss_att 15.387617 loss_ctc 19.708687 lr 0.00088016 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:52,365 DEBUG TRAIN Batch 0/5600 loss 55.735420 loss_att 52.437786 loss_ctc 63.429901 lr 0.00089616 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:52,370 DEBUG TRAIN Batch 0/5600 loss 51.231674 loss_att 48.270996 loss_ctc 58.139931 lr 0.00089616 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:52,371 DEBUG TRAIN Batch 0/5600 loss 56.978111 loss_att 52.505814 loss_ctc 67.413467 lr 0.00089616 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:52,372 DEBUG TRAIN Batch 0/5600 loss 62.668930 loss_att 59.814415 loss_ctc 69.329460 lr 0.00089616 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:52,372 DEBUG TRAIN Batch 0/5600 loss 53.942482 loss_att 49.423393 loss_ctc 64.487030 lr 0.00089616 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:52,375 DEBUG TRAIN Batch 0/5600 loss 51.320869 loss_att 47.854912 loss_ctc 59.408104 lr 0.00089616 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:52,385 DEBUG TRAIN Batch 0/5600 loss 53.029243 loss_att 48.780121 loss_ctc 62.943871 lr 0.00089616 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 05:48:52,388 DEBUG TRAIN Batch 0/5600 loss 65.272026 loss_att 60.949463 loss_ctc 75.358002 lr 0.00089616 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 05:51:46,185 DEBUG TRAIN Batch 0/5700 loss 43.992455 loss_att 41.690472 loss_ctc 49.363747 lr 0.00091216 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 05:51:46,189 DEBUG TRAIN Batch 0/5700 loss 56.048607 loss_att 51.500526 loss_ctc 66.660789 lr 0.00091216 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 05:51:46,191 DEBUG TRAIN Batch 0/5700 loss 52.313103 loss_att 48.688835 loss_ctc 60.769726 lr 0.00091216 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 05:51:46,193 DEBUG TRAIN Batch 0/5700 loss 51.163593 loss_att 47.265030 loss_ctc 60.260239 lr 0.00091216 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 05:51:46,195 DEBUG TRAIN Batch 0/5700 loss 55.673267 loss_att 51.309307 loss_ctc 65.855835 lr 0.00091216 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 05:51:46,198 DEBUG TRAIN Batch 0/5700 loss 55.552689 loss_att 52.212822 loss_ctc 63.345711 lr 0.00091216 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 05:51:46,200 DEBUG TRAIN Batch 0/5700 loss 62.445858 loss_att 56.838661 loss_ctc 75.529312 lr 0.00091216 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 05:51:46,234 DEBUG TRAIN Batch 0/5700 loss 52.578190 loss_att 48.708275 loss_ctc 61.607994 lr 0.00091216 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 05:54:12,063 DEBUG TRAIN Batch 0/5800 loss 49.306358 loss_att 47.146622 loss_ctc 54.345730 lr 0.00092816 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 05:54:12,068 DEBUG TRAIN Batch 0/5800 loss 37.827400 loss_att 35.255478 loss_ctc 43.828552 lr 0.00092816 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 05:54:12,070 DEBUG TRAIN Batch 0/5800 loss 45.002865 loss_att 42.749096 loss_ctc 50.261665 lr 0.00092816 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 05:54:12,071 DEBUG TRAIN Batch 0/5800 loss 40.465427 loss_att 39.561836 loss_ctc 42.573807 lr 0.00092816 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 05:54:12,071 DEBUG TRAIN Batch 0/5800 loss 39.463245 loss_att 36.208416 loss_ctc 47.057846 lr 0.00092816 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 05:54:12,074 DEBUG TRAIN Batch 0/5800 loss 50.970169 loss_att 47.837017 loss_ctc 58.280853 lr 0.00092816 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 05:54:12,077 DEBUG TRAIN Batch 0/5800 loss 47.781540 loss_att 49.324764 loss_ctc 44.180676 lr 0.00092816 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 05:54:12,086 DEBUG TRAIN Batch 0/5800 loss 44.583164 loss_att 41.098434 loss_ctc 52.714203 lr 0.00092816 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 05:55:13,962 DEBUG CV Batch 0/0 loss 7.740501 loss_att 6.827770 loss_ctc 9.870208 history loss 7.145078 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 05:55:13,964 DEBUG CV Batch 0/0 loss 7.740501 loss_att 6.827770 loss_ctc 9.870208 history loss 7.145078 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 05:55:13,967 DEBUG CV Batch 0/0 loss 7.740501 loss_att 6.827770 loss_ctc 9.870208 history loss 7.145078 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 05:55:13,970 DEBUG CV Batch 0/0 loss 7.740501 loss_att 6.827770 loss_ctc 9.870208 history loss 7.145078 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 05:55:13,970 DEBUG CV Batch 0/0 loss 7.740501 loss_att 6.827770 loss_ctc 9.870208 history loss 7.145078 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 05:55:13,971 DEBUG CV Batch 0/0 loss 7.740501 loss_att 6.827770 loss_ctc 9.870208 history loss 7.145078 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 05:55:13,981 DEBUG CV Batch 0/0 loss 7.740501 loss_att 6.827770 loss_ctc 9.870208 history loss 7.145078 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 05:55:13,982 DEBUG CV Batch 0/0 loss 7.740501 loss_att 6.827770 loss_ctc 9.870208 history loss 7.145078 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 05:56:39,471 DEBUG CV Batch 0/100 loss 14.172925 loss_att 12.688139 loss_ctc 17.637426 history loss 17.491975 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 05:56:39,478 DEBUG CV Batch 0/100 loss 14.172925 loss_att 12.688139 loss_ctc 17.637426 history loss 17.491975 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 05:56:39,499 DEBUG CV Batch 0/100 loss 14.172925 loss_att 12.688139 loss_ctc 17.637426 history loss 17.491975 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 05:56:39,500 DEBUG CV Batch 0/100 loss 14.172925 loss_att 12.688139 loss_ctc 17.637426 history loss 17.491975 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 05:56:39,517 DEBUG CV Batch 0/100 loss 14.172925 loss_att 12.688139 loss_ctc 17.637426 history loss 17.491975 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 05:56:39,537 DEBUG CV Batch 0/100 loss 14.172925 loss_att 12.688139 loss_ctc 17.637426 history loss 17.491975 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 05:56:39,542 DEBUG CV Batch 0/100 loss 14.172925 loss_att 12.688139 loss_ctc 17.637426 history loss 17.491975 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 05:56:39,553 DEBUG CV Batch 0/100 loss 14.172925 loss_att 12.688139 loss_ctc 17.637426 history loss 17.491975 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 05:58:00,839 DEBUG CV Batch 0/200 loss 30.151371 loss_att 27.919470 loss_ctc 35.359138 history loss 18.442020 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 05:58:00,865 DEBUG CV Batch 0/200 loss 30.151371 loss_att 27.919470 loss_ctc 35.359138 history loss 18.442020 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 05:58:00,866 DEBUG CV Batch 0/200 loss 30.151371 loss_att 27.919470 loss_ctc 35.359138 history loss 18.442020 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 05:58:00,870 DEBUG CV Batch 0/200 loss 30.151371 loss_att 27.919470 loss_ctc 35.359138 history loss 18.442020 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 05:58:00,904 DEBUG CV Batch 0/200 loss 30.151371 loss_att 27.919470 loss_ctc 35.359138 history loss 18.442020 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 05:58:00,938 DEBUG CV Batch 0/200 loss 30.151371 loss_att 27.919470 loss_ctc 35.359138 history loss 18.442020 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 05:58:00,946 DEBUG CV Batch 0/200 loss 30.151371 loss_att 27.919470 loss_ctc 35.359138 history loss 18.442020 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 05:58:00,972 DEBUG CV Batch 0/200 loss 30.151371 loss_att 27.919470 loss_ctc 35.359138 history loss 18.442020 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 05:59:58,464 DEBUG CV Batch 0/300 loss 10.914417 loss_att 9.217051 loss_ctc 14.874941 history loss 21.252703 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 05:59:58,500 DEBUG CV Batch 0/300 loss 10.914417 loss_att 9.217051 loss_ctc 14.874941 history loss 21.252703 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 05:59:58,531 DEBUG CV Batch 0/300 loss 10.914417 loss_att 9.217051 loss_ctc 14.874941 history loss 21.252703 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 05:59:58,533 DEBUG CV Batch 0/300 loss 10.914417 loss_att 9.217051 loss_ctc 14.874941 history loss 21.252703 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 05:59:58,541 DEBUG CV Batch 0/300 loss 10.914417 loss_att 9.217051 loss_ctc 14.874941 history loss 21.252703 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 05:59:58,542 DEBUG CV Batch 0/300 loss 10.914417 loss_att 9.217051 loss_ctc 14.874941 history loss 21.252703 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 05:59:58,544 DEBUG CV Batch 0/300 loss 10.914417 loss_att 9.217051 loss_ctc 14.874941 history loss 21.252703 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 05:59:58,545 DEBUG CV Batch 0/300 loss 10.914417 loss_att 9.217051 loss_ctc 14.874941 history loss 21.252703 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:01:24,225 DEBUG CV Batch 0/400 loss 30.028540 loss_att 26.599066 loss_ctc 38.030643 history loss 22.665283 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:01:24,248 DEBUG CV Batch 0/400 loss 30.028540 loss_att 26.599066 loss_ctc 38.030643 history loss 22.665283 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:01:24,254 DEBUG CV Batch 0/400 loss 30.028540 loss_att 26.599066 loss_ctc 38.030643 history loss 22.665283 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:01:24,281 DEBUG CV Batch 0/400 loss 30.028540 loss_att 26.599066 loss_ctc 38.030643 history loss 22.665283 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:01:24,288 DEBUG CV Batch 0/400 loss 30.028540 loss_att 26.599066 loss_ctc 38.030643 history loss 22.665283 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:01:24,292 DEBUG CV Batch 0/400 loss 30.028540 loss_att 26.599066 loss_ctc 38.030643 history loss 22.665283 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:01:24,293 DEBUG CV Batch 0/400 loss 30.028540 loss_att 26.599066 loss_ctc 38.030643 history loss 22.665283 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:01:24,314 DEBUG CV Batch 0/400 loss 30.028540 loss_att 26.599066 loss_ctc 38.030643 history loss 22.665283 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:03:21,948 DEBUG CV Batch 0/500 loss 6.079978 loss_att 5.218333 loss_ctc 8.090484 history loss 23.266222 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:03:21,950 DEBUG CV Batch 0/500 loss 6.079978 loss_att 5.218333 loss_ctc 8.090484 history loss 23.266222 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:03:21,951 DEBUG CV Batch 0/500 loss 6.079978 loss_att 5.218333 loss_ctc 8.090484 history loss 23.266222 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:03:21,953 DEBUG CV Batch 0/500 loss 6.079978 loss_att 5.218333 loss_ctc 8.090484 history loss 23.266222 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:03:21,956 DEBUG CV Batch 0/500 loss 6.079978 loss_att 5.218333 loss_ctc 8.090484 history loss 23.266222 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:03:21,963 DEBUG CV Batch 0/500 loss 6.079978 loss_att 5.218333 loss_ctc 8.090484 history loss 23.266222 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:03:21,966 DEBUG CV Batch 0/500 loss 6.079978 loss_att 5.218333 loss_ctc 8.090484 history loss 23.266222 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:03:21,968 DEBUG CV Batch 0/500 loss 6.079978 loss_att 5.218333 loss_ctc 8.090484 history loss 23.266222 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:04:36,745 DEBUG CV Batch 0/600 loss 18.645294 loss_att 16.930984 loss_ctc 22.645346 history loss 22.248496 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:04:36,747 DEBUG CV Batch 0/600 loss 18.645294 loss_att 16.930984 loss_ctc 22.645346 history loss 22.248496 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:04:36,767 DEBUG CV Batch 0/600 loss 18.645294 loss_att 16.930984 loss_ctc 22.645346 history loss 22.248496 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:04:36,801 DEBUG CV Batch 0/600 loss 18.645294 loss_att 16.930984 loss_ctc 22.645346 history loss 22.248496 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:04:36,805 DEBUG CV Batch 0/600 loss 18.645294 loss_att 16.930984 loss_ctc 22.645346 history loss 22.248496 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:04:36,807 DEBUG CV Batch 0/600 loss 18.645294 loss_att 16.930984 loss_ctc 22.645346 history loss 22.248496 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:04:36,815 DEBUG CV Batch 0/600 loss 18.645294 loss_att 16.930984 loss_ctc 22.645346 history loss 22.248496 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:04:36,927 DEBUG CV Batch 0/600 loss 18.645294 loss_att 16.930984 loss_ctc 22.645346 history loss 22.248496 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:05:48,636 DEBUG CV Batch 0/700 loss 33.339115 loss_att 29.671120 loss_ctc 41.897774 history loss 22.091986 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:05:48,637 DEBUG CV Batch 0/700 loss 33.339115 loss_att 29.671120 loss_ctc 41.897774 history loss 22.091986 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:05:48,640 DEBUG CV Batch 0/700 loss 33.339115 loss_att 29.671120 loss_ctc 41.897774 history loss 22.091986 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:05:48,646 DEBUG CV Batch 0/700 loss 33.339115 loss_att 29.671120 loss_ctc 41.897774 history loss 22.091986 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:05:48,648 DEBUG CV Batch 0/700 loss 33.339115 loss_att 29.671120 loss_ctc 41.897774 history loss 22.091986 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:05:48,652 DEBUG CV Batch 0/700 loss 33.339115 loss_att 29.671120 loss_ctc 41.897774 history loss 22.091986 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:05:48,662 DEBUG CV Batch 0/700 loss 33.339115 loss_att 29.671120 loss_ctc 41.897774 history loss 22.091986 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:05:48,666 DEBUG CV Batch 0/700 loss 33.339115 loss_att 29.671120 loss_ctc 41.897774 history loss 22.091986 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:07:35,009 DEBUG CV Batch 0/800 loss 16.059147 loss_att 14.405909 loss_ctc 19.916700 history loss 22.783706 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:07:35,037 DEBUG CV Batch 0/800 loss 16.059147 loss_att 14.405909 loss_ctc 19.916700 history loss 22.783706 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:07:35,039 DEBUG CV Batch 0/800 loss 16.059147 loss_att 14.405909 loss_ctc 19.916700 history loss 22.783706 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:07:35,042 DEBUG CV Batch 0/800 loss 16.059147 loss_att 14.405909 loss_ctc 19.916700 history loss 22.783706 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:07:35,047 DEBUG CV Batch 0/800 loss 16.059147 loss_att 14.405909 loss_ctc 19.916700 history loss 22.783706 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:07:35,047 DEBUG CV Batch 0/800 loss 16.059147 loss_att 14.405909 loss_ctc 19.916700 history loss 22.783706 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:07:35,053 DEBUG CV Batch 0/800 loss 16.059147 loss_att 14.405909 loss_ctc 19.916700 history loss 22.783706 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:07:35,059 DEBUG CV Batch 0/800 loss 16.059147 loss_att 14.405909 loss_ctc 19.916700 history loss 22.783706 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:38,739 DEBUG CV Batch 0/900 loss 31.803940 loss_att 28.138058 loss_ctc 40.357662 history loss 23.353162 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:38,758 DEBUG CV Batch 0/900 loss 31.803940 loss_att 28.138058 loss_ctc 40.357662 history loss 23.353162 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:38,767 DEBUG CV Batch 0/900 loss 31.803940 loss_att 28.138058 loss_ctc 40.357662 history loss 23.353162 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:38,779 DEBUG CV Batch 0/900 loss 31.803940 loss_att 28.138058 loss_ctc 40.357662 history loss 23.353162 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:38,780 DEBUG CV Batch 0/900 loss 31.803940 loss_att 28.138058 loss_ctc 40.357662 history loss 23.353162 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:38,798 DEBUG CV Batch 0/900 loss 31.803940 loss_att 28.138058 loss_ctc 40.357662 history loss 23.353162 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:38,808 DEBUG CV Batch 0/900 loss 31.803940 loss_att 28.138058 loss_ctc 40.357662 history loss 23.353162 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:38,818 DEBUG CV Batch 0/900 loss 31.803940 loss_att 28.138058 loss_ctc 40.357662 history loss 23.353162 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,422 INFO Epoch 0 CV info cv_loss 23.549949603682304\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,423 INFO Epoch 1 TRAIN info lr 0.00093728\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,425 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,436 INFO Epoch 0 CV info cv_loss 23.549949603682304\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,437 INFO Epoch 1 TRAIN info lr 0.00093744\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,441 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,461 INFO Epoch 0 CV info cv_loss 23.549949603682304\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,462 INFO Epoch 1 TRAIN info lr 0.00093728\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,464 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,470 INFO Epoch 0 CV info cv_loss 23.549949603682304\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,470 INFO Epoch 1 TRAIN info lr 0.00093744\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,471 INFO Epoch 0 CV info cv_loss 23.549949603682304\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,472 INFO Epoch 1 TRAIN info lr 0.00093728\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,473 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,473 INFO Epoch 0 CV info cv_loss 23.549949603682304\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,474 INFO Epoch 1 TRAIN info lr 0.00093744\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,475 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,474 INFO Epoch 0 CV info cv_loss 23.549949603682304\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,475 INFO Checkpoint: save to checkpoint /opt/ml/model/0.pt\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,476 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,483 INFO Epoch 0 CV info cv_loss 23.549949603682304\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,483 INFO Epoch 1 TRAIN info lr 0.00093728\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,486 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,813 INFO Epoch 1 TRAIN info lr 0.00093744\u001b[0m\n",
      "\u001b[34m2022-11-28 06:08:39,816 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 06:11:34,730 DEBUG TRAIN Batch 1/0 loss 14.439871 loss_att 12.532059 loss_ctc 18.891434 lr 0.00093744 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:11:34,731 DEBUG TRAIN Batch 1/0 loss 21.651093 loss_att 20.091717 loss_ctc 25.289633 lr 0.00093760 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:11:34,731 DEBUG TRAIN Batch 1/0 loss 20.024101 loss_att 17.608046 loss_ctc 25.661560 lr 0.00093744 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:11:34,733 DEBUG TRAIN Batch 1/0 loss 19.060741 loss_att 17.326714 loss_ctc 23.106804 lr 0.00093760 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:11:34,736 DEBUG TRAIN Batch 1/0 loss 26.137880 loss_att 24.458588 loss_ctc 30.056229 lr 0.00093744 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:11:34,736 DEBUG TRAIN Batch 1/0 loss 24.884399 loss_att 22.738617 loss_ctc 29.891226 lr 0.00093744 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:11:34,759 DEBUG TRAIN Batch 1/0 loss 17.018475 loss_att 15.904312 loss_ctc 19.618187 lr 0.00093760 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:11:34,799 DEBUG TRAIN Batch 1/0 loss 16.704891 loss_att 15.307764 loss_ctc 19.964855 lr 0.00093760 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:12:07,915 DEBUG TRAIN Batch 1/100 loss 49.852898 loss_att 45.910522 loss_ctc 59.051769 lr 0.00095360 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:12:07,917 DEBUG TRAIN Batch 1/100 loss 62.673523 loss_att 56.535931 loss_ctc 76.994568 lr 0.00095360 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:12:07,918 DEBUG TRAIN Batch 1/100 loss 50.918747 loss_att 46.508942 loss_ctc 61.208294 lr 0.00095344 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:12:07,918 DEBUG TRAIN Batch 1/100 loss 47.341442 loss_att 44.568558 loss_ctc 53.811501 lr 0.00095344 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:12:07,922 DEBUG TRAIN Batch 1/100 loss 43.353043 loss_att 40.185371 loss_ctc 50.744278 lr 0.00095344 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:12:07,926 DEBUG TRAIN Batch 1/100 loss 52.898190 loss_att 48.750359 loss_ctc 62.576454 lr 0.00095344 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:12:07,929 DEBUG TRAIN Batch 1/100 loss 58.672573 loss_att 53.464020 loss_ctc 70.825867 lr 0.00095360 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:12:07,931 DEBUG TRAIN Batch 1/100 loss 59.075050 loss_att 54.591583 loss_ctc 69.536476 lr 0.00095360 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:15:00,226 DEBUG TRAIN Batch 1/200 loss 46.399586 loss_att 43.783020 loss_ctc 52.504910 lr 0.00096944 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:15:00,228 DEBUG TRAIN Batch 1/200 loss 60.769444 loss_att 56.181671 loss_ctc 71.474251 lr 0.00096944 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:15:00,228 DEBUG TRAIN Batch 1/200 loss 56.752724 loss_att 52.963722 loss_ctc 65.593735 lr 0.00096960 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:15:00,230 DEBUG TRAIN Batch 1/200 loss 57.428410 loss_att 52.716415 loss_ctc 68.423065 lr 0.00096960 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:15:00,236 DEBUG TRAIN Batch 1/200 loss 59.549782 loss_att 56.359695 loss_ctc 66.993317 lr 0.00096944 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:15:00,239 DEBUG TRAIN Batch 1/200 loss 49.401611 loss_att 46.714584 loss_ctc 55.671337 lr 0.00096960 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:15:00,246 DEBUG TRAIN Batch 1/200 loss 42.240147 loss_att 39.025753 loss_ctc 49.740398 lr 0.00096944 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:15:00,273 DEBUG TRAIN Batch 1/200 loss 51.821644 loss_att 49.123608 loss_ctc 58.117065 lr 0.00096960 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:17:53,563 DEBUG TRAIN Batch 1/300 loss 36.233669 loss_att 33.882576 loss_ctc 41.719559 lr 0.00098560 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:17:53,564 DEBUG TRAIN Batch 1/300 loss 41.252171 loss_att 37.882454 loss_ctc 49.114838 lr 0.00098544 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:17:53,569 DEBUG TRAIN Batch 1/300 loss 39.818245 loss_att 37.163662 loss_ctc 46.012280 lr 0.00098544 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:17:53,569 DEBUG TRAIN Batch 1/300 loss 38.996414 loss_att 35.153023 loss_ctc 47.964333 lr 0.00098544 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:17:53,580 DEBUG TRAIN Batch 1/300 loss 37.884102 loss_att 34.845501 loss_ctc 44.974178 lr 0.00098560 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:17:53,583 DEBUG TRAIN Batch 1/300 loss 43.962234 loss_att 39.878437 loss_ctc 53.491085 lr 0.00098560 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:17:53,588 DEBUG TRAIN Batch 1/300 loss 35.883369 loss_att 33.553474 loss_ctc 41.319786 lr 0.00098560 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:17:53,591 DEBUG TRAIN Batch 1/300 loss 40.096783 loss_att 37.065544 loss_ctc 47.169670 lr 0.00098544 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:20:47,681 DEBUG TRAIN Batch 1/400 loss 66.416954 loss_att 62.434181 loss_ctc 75.710083 lr 0.00100144 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:20:47,684 DEBUG TRAIN Batch 1/400 loss 52.730728 loss_att 47.249969 loss_ctc 65.519173 lr 0.00100144 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:20:47,684 DEBUG TRAIN Batch 1/400 loss 60.047508 loss_att 56.241272 loss_ctc 68.928726 lr 0.00100160 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:20:47,686 DEBUG TRAIN Batch 1/400 loss 50.261646 loss_att 46.900230 loss_ctc 58.104942 lr 0.00100160 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:20:47,694 DEBUG TRAIN Batch 1/400 loss 43.211956 loss_att 40.196114 loss_ctc 50.248924 lr 0.00100144 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:20:47,694 DEBUG TRAIN Batch 1/400 loss 56.232330 loss_att 52.353195 loss_ctc 65.283646 lr 0.00100144 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:20:47,694 DEBUG TRAIN Batch 1/400 loss 52.545719 loss_att 48.564331 loss_ctc 61.835632 lr 0.00100160 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:20:47,701 DEBUG TRAIN Batch 1/400 loss 56.361954 loss_att 52.821648 loss_ctc 64.622665 lr 0.00100160 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:23:41,017 DEBUG TRAIN Batch 1/500 loss 10.678257 loss_att 9.945720 loss_ctc 12.387512 lr 0.00101760 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:23:41,019 DEBUG TRAIN Batch 1/500 loss 14.137329 loss_att 12.762378 loss_ctc 17.345549 lr 0.00101744 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:23:41,020 DEBUG TRAIN Batch 1/500 loss 17.743057 loss_att 16.041191 loss_ctc 21.714081 lr 0.00101744 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:23:41,023 DEBUG TRAIN Batch 1/500 loss 21.677578 loss_att 19.538902 loss_ctc 26.667816 lr 0.00101744 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:23:41,024 DEBUG TRAIN Batch 1/500 loss 23.080837 loss_att 20.384062 loss_ctc 29.373318 lr 0.00101744 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:23:41,030 DEBUG TRAIN Batch 1/500 loss 11.130624 loss_att 10.087462 loss_ctc 13.564667 lr 0.00101760 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:23:41,043 DEBUG TRAIN Batch 1/500 loss 18.767235 loss_att 16.855951 loss_ctc 23.226894 lr 0.00101760 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:23:41,094 DEBUG TRAIN Batch 1/500 loss 15.878915 loss_att 14.637695 loss_ctc 18.775093 lr 0.00101760 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:24:14,193 DEBUG TRAIN Batch 1/600 loss 44.243355 loss_att 40.339214 loss_ctc 53.353012 lr 0.00103360 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:24:14,195 DEBUG TRAIN Batch 1/600 loss 56.619186 loss_att 53.653137 loss_ctc 63.539970 lr 0.00103360 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:24:14,197 DEBUG TRAIN Batch 1/600 loss 62.109386 loss_att 56.681000 loss_ctc 74.775620 lr 0.00103344 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:24:14,198 DEBUG TRAIN Batch 1/600 loss 55.222061 loss_att 50.365196 loss_ctc 66.554741 lr 0.00103344 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:24:14,203 DEBUG TRAIN Batch 1/600 loss 50.736359 loss_att 47.726242 loss_ctc 57.759960 lr 0.00103344 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:24:14,213 DEBUG TRAIN Batch 1/600 loss 46.474754 loss_att 42.811691 loss_ctc 55.021904 lr 0.00103344 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:24:14,222 DEBUG TRAIN Batch 1/600 loss 42.513924 loss_att 39.688076 loss_ctc 49.107574 lr 0.00103360 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:24:14,247 DEBUG TRAIN Batch 1/600 loss 44.169174 loss_att 39.855103 loss_ctc 54.235344 lr 0.00103360 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:27:06,782 DEBUG TRAIN Batch 1/700 loss 51.688007 loss_att 48.171429 loss_ctc 59.893349 lr 0.00104960 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:27:06,785 DEBUG TRAIN Batch 1/700 loss 48.609550 loss_att 44.328125 loss_ctc 58.599541 lr 0.00104944 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:27:06,786 DEBUG TRAIN Batch 1/700 loss 40.558826 loss_att 37.454639 loss_ctc 47.801933 lr 0.00104944 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:27:06,788 DEBUG TRAIN Batch 1/700 loss 54.127670 loss_att 49.636425 loss_ctc 64.607239 lr 0.00104944 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:27:06,794 DEBUG TRAIN Batch 1/700 loss 42.723930 loss_att 38.522076 loss_ctc 52.528259 lr 0.00104960 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:27:06,794 DEBUG TRAIN Batch 1/700 loss 43.256859 loss_att 39.469791 loss_ctc 52.093353 lr 0.00104944 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:27:06,801 DEBUG TRAIN Batch 1/700 loss 49.281738 loss_att 45.893555 loss_ctc 57.187492 lr 0.00104960 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:27:06,802 DEBUG TRAIN Batch 1/700 loss 60.542694 loss_att 56.862137 loss_ctc 69.130669 lr 0.00104960 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:29:57,455 DEBUG TRAIN Batch 1/800 loss 32.907722 loss_att 30.086828 loss_ctc 39.489807 lr 0.00106544 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:29:57,461 DEBUG TRAIN Batch 1/800 loss 46.279396 loss_att 43.020275 loss_ctc 53.884010 lr 0.00106560 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:29:57,462 DEBUG TRAIN Batch 1/800 loss 43.921036 loss_att 38.362213 loss_ctc 56.891624 lr 0.00106560 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:29:57,465 DEBUG TRAIN Batch 1/800 loss 44.403351 loss_att 40.406223 loss_ctc 53.729977 lr 0.00106544 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:29:57,466 DEBUG TRAIN Batch 1/800 loss 34.569092 loss_att 31.217072 loss_ctc 42.390465 lr 0.00106560 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:29:57,467 DEBUG TRAIN Batch 1/800 loss 40.952999 loss_att 38.486111 loss_ctc 46.709072 lr 0.00106544 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:29:57,476 DEBUG TRAIN Batch 1/800 loss 45.256050 loss_att 41.488777 loss_ctc 54.046364 lr 0.00106544 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:29:57,481 DEBUG TRAIN Batch 1/800 loss 47.509071 loss_att 43.095970 loss_ctc 57.806297 lr 0.00106560 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:32:48,530 DEBUG TRAIN Batch 1/900 loss 45.229626 loss_att 41.900742 loss_ctc 52.997017 lr 0.00108144 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:32:48,530 DEBUG TRAIN Batch 1/900 loss 43.885590 loss_att 39.784019 loss_ctc 53.455914 lr 0.00108160 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:32:48,535 DEBUG TRAIN Batch 1/900 loss 46.700363 loss_att 42.983227 loss_ctc 55.373688 lr 0.00108144 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:32:48,537 DEBUG TRAIN Batch 1/900 loss 76.157524 loss_att 68.984062 loss_ctc 92.895599 lr 0.00108160 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:32:48,543 DEBUG TRAIN Batch 1/900 loss 55.432762 loss_att 49.300396 loss_ctc 69.741608 lr 0.00108144 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:32:48,544 DEBUG TRAIN Batch 1/900 loss 48.571781 loss_att 44.934845 loss_ctc 57.057964 lr 0.00108144 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:32:48,548 DEBUG TRAIN Batch 1/900 loss 49.014008 loss_att 43.604065 loss_ctc 61.637203 lr 0.00108160 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:32:48,555 DEBUG TRAIN Batch 1/900 loss 49.357887 loss_att 43.812500 loss_ctc 62.297119 lr 0.00108160 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:35:42,032 DEBUG TRAIN Batch 1/1000 loss 15.691056 loss_att 14.358711 loss_ctc 18.799860 lr 0.00109744 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:35:42,033 DEBUG TRAIN Batch 1/1000 loss 19.973303 loss_att 18.380209 loss_ctc 23.690521 lr 0.00109744 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:35:42,036 DEBUG TRAIN Batch 1/1000 loss 13.440657 loss_att 11.531940 loss_ctc 17.894329 lr 0.00109744 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:35:42,045 DEBUG TRAIN Batch 1/1000 loss 14.574090 loss_att 13.418506 loss_ctc 17.270452 lr 0.00109760 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:35:42,046 DEBUG TRAIN Batch 1/1000 loss 17.985779 loss_att 16.277483 loss_ctc 21.971802 lr 0.00109760 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:35:42,086 DEBUG TRAIN Batch 1/1000 loss 18.114471 loss_att 15.967506 loss_ctc 23.124058 lr 0.00109760 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:35:42,115 DEBUG TRAIN Batch 1/1000 loss 19.097858 loss_att 17.714994 loss_ctc 22.324543 lr 0.00109744 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:35:42,125 DEBUG TRAIN Batch 1/1000 loss 13.227272 loss_att 11.774801 loss_ctc 16.616367 lr 0.00109760 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:36:15,446 DEBUG TRAIN Batch 1/1100 loss 58.403549 loss_att 54.198212 loss_ctc 68.216003 lr 0.00111360 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:36:15,447 DEBUG TRAIN Batch 1/1100 loss 44.509800 loss_att 40.105316 loss_ctc 54.786926 lr 0.00111344 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:36:15,450 DEBUG TRAIN Batch 1/1100 loss 46.074028 loss_att 42.386902 loss_ctc 54.677315 lr 0.00111360 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:36:15,451 DEBUG TRAIN Batch 1/1100 loss 55.015514 loss_att 49.726559 loss_ctc 67.356415 lr 0.00111344 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:36:15,454 DEBUG TRAIN Batch 1/1100 loss 36.948586 loss_att 34.443245 loss_ctc 42.794384 lr 0.00111344 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:36:15,460 DEBUG TRAIN Batch 1/1100 loss 52.651718 loss_att 47.483887 loss_ctc 64.709984 lr 0.00111360 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:36:15,465 DEBUG TRAIN Batch 1/1100 loss 45.920410 loss_att 40.764538 loss_ctc 57.950775 lr 0.00111360 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:36:15,466 DEBUG TRAIN Batch 1/1100 loss 39.552536 loss_att 35.288071 loss_ctc 49.502960 lr 0.00111344 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:39:04,997 DEBUG TRAIN Batch 1/1200 loss 47.947937 loss_att 44.218349 loss_ctc 56.650311 lr 0.00112960 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:39:04,999 DEBUG TRAIN Batch 1/1200 loss 38.496487 loss_att 35.127937 loss_ctc 46.356438 lr 0.00112944 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:39:04,999 DEBUG TRAIN Batch 1/1200 loss 44.109802 loss_att 39.955070 loss_ctc 53.804169 lr 0.00112960 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:39:05,000 DEBUG TRAIN Batch 1/1200 loss 37.922630 loss_att 34.529556 loss_ctc 45.839809 lr 0.00112944 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:39:05,000 DEBUG TRAIN Batch 1/1200 loss 48.084312 loss_att 44.433022 loss_ctc 56.603985 lr 0.00112944 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:39:05,003 DEBUG TRAIN Batch 1/1200 loss 53.613869 loss_att 48.821823 loss_ctc 64.795311 lr 0.00112944 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:39:05,018 DEBUG TRAIN Batch 1/1200 loss 42.783539 loss_att 39.849426 loss_ctc 49.629803 lr 0.00112960 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:39:05,045 DEBUG TRAIN Batch 1/1200 loss 38.065453 loss_att 35.292351 loss_ctc 44.536018 lr 0.00112960 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:41:53,801 DEBUG TRAIN Batch 1/1300 loss 28.864101 loss_att 26.685493 loss_ctc 33.947521 lr 0.00114544 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:41:53,803 DEBUG TRAIN Batch 1/1300 loss 41.518581 loss_att 37.022568 loss_ctc 52.009285 lr 0.00114560 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:41:53,804 DEBUG TRAIN Batch 1/1300 loss 38.223412 loss_att 33.723366 loss_ctc 48.723522 lr 0.00114544 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:41:53,804 DEBUG TRAIN Batch 1/1300 loss 43.422356 loss_att 39.496181 loss_ctc 52.583420 lr 0.00114560 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:41:53,805 DEBUG TRAIN Batch 1/1300 loss 37.586937 loss_att 35.545441 loss_ctc 42.350426 lr 0.00114560 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:41:53,809 DEBUG TRAIN Batch 1/1300 loss 48.449047 loss_att 45.856140 loss_ctc 54.499168 lr 0.00114544 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:41:53,809 DEBUG TRAIN Batch 1/1300 loss 31.015024 loss_att 28.052383 loss_ctc 37.927856 lr 0.00114544 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:41:53,828 DEBUG TRAIN Batch 1/1300 loss 45.353580 loss_att 40.621906 loss_ctc 56.394157 lr 0.00114560 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:44:43,825 DEBUG TRAIN Batch 1/1400 loss 40.028515 loss_att 37.381989 loss_ctc 46.203747 lr 0.00116160 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:44:43,826 DEBUG TRAIN Batch 1/1400 loss 50.653912 loss_att 46.893730 loss_ctc 59.427666 lr 0.00116144 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:44:43,832 DEBUG TRAIN Batch 1/1400 loss 38.286133 loss_att 34.578068 loss_ctc 46.938278 lr 0.00116144 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:44:43,836 DEBUG TRAIN Batch 1/1400 loss 51.531387 loss_att 46.764690 loss_ctc 62.653679 lr 0.00116160 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:44:43,837 DEBUG TRAIN Batch 1/1400 loss 49.516235 loss_att 46.147537 loss_ctc 57.376526 lr 0.00116144 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:44:43,843 DEBUG TRAIN Batch 1/1400 loss 62.974655 loss_att 56.102135 loss_ctc 79.010544 lr 0.00116144 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:44:43,847 DEBUG TRAIN Batch 1/1400 loss 38.248459 loss_att 34.328804 loss_ctc 47.394314 lr 0.00116160 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:44:43,847 DEBUG TRAIN Batch 1/1400 loss 55.272240 loss_att 50.298271 loss_ctc 66.878166 lr 0.00116160 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:47:31,670 DEBUG TRAIN Batch 1/1500 loss 10.582036 loss_att 9.485529 loss_ctc 13.140552 lr 0.00117744 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:47:31,670 DEBUG TRAIN Batch 1/1500 loss 15.361760 loss_att 13.421172 loss_ctc 19.889799 lr 0.00117760 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:47:31,670 DEBUG TRAIN Batch 1/1500 loss 16.406603 loss_att 14.065611 loss_ctc 21.868919 lr 0.00117744 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:47:31,681 DEBUG TRAIN Batch 1/1500 loss 19.298542 loss_att 17.546375 loss_ctc 23.386932 lr 0.00117760 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:47:31,692 DEBUG TRAIN Batch 1/1500 loss 13.813285 loss_att 12.156284 loss_ctc 17.679621 lr 0.00117744 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:47:31,693 DEBUG TRAIN Batch 1/1500 loss 20.297466 loss_att 19.001263 loss_ctc 23.321945 lr 0.00117760 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:47:31,725 DEBUG TRAIN Batch 1/1500 loss 15.409836 loss_att 13.681725 loss_ctc 19.442097 lr 0.00117744 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:47:31,748 DEBUG TRAIN Batch 1/1500 loss 12.734877 loss_att 11.163145 loss_ctc 16.402250 lr 0.00117760 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:48:04,634 DEBUG TRAIN Batch 1/1600 loss 39.981247 loss_att 35.893585 loss_ctc 49.519127 lr 0.00119360 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:48:04,637 DEBUG TRAIN Batch 1/1600 loss 37.765949 loss_att 34.454281 loss_ctc 45.493179 lr 0.00119360 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:48:04,641 DEBUG TRAIN Batch 1/1600 loss 45.290031 loss_att 41.012451 loss_ctc 55.271053 lr 0.00119344 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:48:04,642 DEBUG TRAIN Batch 1/1600 loss 52.567535 loss_att 49.207806 loss_ctc 60.406898 lr 0.00119344 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:48:04,641 DEBUG TRAIN Batch 1/1600 loss 39.437820 loss_att 35.607880 loss_ctc 48.374352 lr 0.00119344 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:48:04,647 DEBUG TRAIN Batch 1/1600 loss 47.604385 loss_att 42.358898 loss_ctc 59.843853 lr 0.00119344 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:48:04,652 DEBUG TRAIN Batch 1/1600 loss 40.271622 loss_att 35.534172 loss_ctc 51.325665 lr 0.00119360 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:48:04,669 DEBUG TRAIN Batch 1/1600 loss 45.686283 loss_att 40.546165 loss_ctc 57.679893 lr 0.00119360 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:50:51,680 DEBUG TRAIN Batch 1/1700 loss 50.661263 loss_att 45.867096 loss_ctc 61.847637 lr 0.00120944 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:50:51,682 DEBUG TRAIN Batch 1/1700 loss 49.900612 loss_att 43.858238 loss_ctc 63.999493 lr 0.00120944 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:50:51,684 DEBUG TRAIN Batch 1/1700 loss 49.086014 loss_att 43.819969 loss_ctc 61.373444 lr 0.00120960 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:50:51,688 DEBUG TRAIN Batch 1/1700 loss 43.346581 loss_att 39.487949 loss_ctc 52.350048 lr 0.00120960 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:50:51,693 DEBUG TRAIN Batch 1/1700 loss 50.247627 loss_att 46.152233 loss_ctc 59.803539 lr 0.00120944 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:50:51,694 DEBUG TRAIN Batch 1/1700 loss 37.748764 loss_att 34.639339 loss_ctc 45.004082 lr 0.00120944 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:50:51,696 DEBUG TRAIN Batch 1/1700 loss 50.699852 loss_att 47.204742 loss_ctc 58.855110 lr 0.00120960 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:50:51,706 DEBUG TRAIN Batch 1/1700 loss 61.219051 loss_att 56.163940 loss_ctc 73.014305 lr 0.00120960 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:53:37,156 DEBUG TRAIN Batch 1/1800 loss 34.082031 loss_att 30.517859 loss_ctc 42.398438 lr 0.00122544 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:53:37,157 DEBUG TRAIN Batch 1/1800 loss 36.133320 loss_att 32.997215 loss_ctc 43.450897 lr 0.00122560 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:53:37,167 DEBUG TRAIN Batch 1/1800 loss 35.101353 loss_att 32.341003 loss_ctc 41.542164 lr 0.00122544 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:53:37,170 DEBUG TRAIN Batch 1/1800 loss 38.565224 loss_att 34.300758 loss_ctc 48.515640 lr 0.00122544 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:53:37,171 DEBUG TRAIN Batch 1/1800 loss 35.256618 loss_att 31.100548 loss_ctc 44.954113 lr 0.00122560 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:53:37,176 DEBUG TRAIN Batch 1/1800 loss 44.525230 loss_att 40.326481 loss_ctc 54.322311 lr 0.00122544 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:53:37,178 DEBUG TRAIN Batch 1/1800 loss 31.787292 loss_att 29.052259 loss_ctc 38.169029 lr 0.00122560 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:53:37,186 DEBUG TRAIN Batch 1/1800 loss 29.610950 loss_att 26.537771 loss_ctc 36.781700 lr 0.00122560 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:56:21,643 DEBUG TRAIN Batch 1/1900 loss 49.180302 loss_att 44.841362 loss_ctc 59.304497 lr 0.00124144 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:56:21,645 DEBUG TRAIN Batch 1/1900 loss 48.543556 loss_att 43.668427 loss_ctc 59.918865 lr 0.00124144 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:56:21,649 DEBUG TRAIN Batch 1/1900 loss 44.189369 loss_att 40.440594 loss_ctc 52.936516 lr 0.00124144 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:56:21,650 DEBUG TRAIN Batch 1/1900 loss 44.092934 loss_att 40.154961 loss_ctc 53.281536 lr 0.00124160 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:56:21,651 DEBUG TRAIN Batch 1/1900 loss 47.178665 loss_att 42.149147 loss_ctc 58.914207 lr 0.00124160 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:56:21,653 DEBUG TRAIN Batch 1/1900 loss 49.820797 loss_att 44.257904 loss_ctc 62.800880 lr 0.00124144 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:56:21,659 DEBUG TRAIN Batch 1/1900 loss 51.419216 loss_att 46.915253 loss_ctc 61.928467 lr 0.00124160 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:56:21,660 DEBUG TRAIN Batch 1/1900 loss 42.123119 loss_att 36.418442 loss_ctc 55.434036 lr 0.00124160 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:05,542 DEBUG TRAIN Batch 1/2000 loss 20.498802 loss_att 18.503645 loss_ctc 25.154171 lr 0.00125744 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:05,542 DEBUG TRAIN Batch 1/2000 loss 21.942554 loss_att 19.481663 loss_ctc 27.684629 lr 0.00125744 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:05,543 DEBUG TRAIN Batch 1/2000 loss 19.899086 loss_att 17.243881 loss_ctc 26.094566 lr 0.00125744 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:05,545 DEBUG TRAIN Batch 1/2000 loss 14.427807 loss_att 13.131060 loss_ctc 17.453548 lr 0.00125760 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:05,547 DEBUG TRAIN Batch 1/2000 loss 13.510954 loss_att 12.036724 loss_ctc 16.950825 lr 0.00125760 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:05,550 DEBUG TRAIN Batch 1/2000 loss 23.958324 loss_att 21.303783 loss_ctc 30.152256 lr 0.00125744 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:05,568 DEBUG TRAIN Batch 1/2000 loss 19.530384 loss_att 17.317480 loss_ctc 24.693829 lr 0.00125760 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:05,618 DEBUG TRAIN Batch 1/2000 loss 17.594387 loss_att 15.531856 loss_ctc 22.406956 lr 0.00125760 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:38,268 DEBUG TRAIN Batch 1/2100 loss 46.268757 loss_att 41.842800 loss_ctc 56.595989 lr 0.00127360 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:38,276 DEBUG TRAIN Batch 1/2100 loss 38.313568 loss_att 34.560501 loss_ctc 47.070721 lr 0.00127344 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:38,276 DEBUG TRAIN Batch 1/2100 loss 37.698044 loss_att 35.226585 loss_ctc 43.464783 lr 0.00127344 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:38,277 DEBUG TRAIN Batch 1/2100 loss 33.862602 loss_att 30.553673 loss_ctc 41.583427 lr 0.00127360 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:38,282 DEBUG TRAIN Batch 1/2100 loss 42.782623 loss_att 38.648838 loss_ctc 52.428123 lr 0.00127360 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:38,284 DEBUG TRAIN Batch 1/2100 loss 51.085266 loss_att 45.948853 loss_ctc 63.070236 lr 0.00127344 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:38,284 DEBUG TRAIN Batch 1/2100 loss 37.996208 loss_att 34.337448 loss_ctc 46.533310 lr 0.00127360 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 06:59:38,285 DEBUG TRAIN Batch 1/2100 loss 42.250275 loss_att 38.754097 loss_ctc 50.408031 lr 0.00127344 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:02:27,783 DEBUG TRAIN Batch 1/2200 loss 46.924263 loss_att 41.848526 loss_ctc 58.767639 lr 0.00128960 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:02:27,787 DEBUG TRAIN Batch 1/2200 loss 40.034515 loss_att 36.699158 loss_ctc 47.817017 lr 0.00128944 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:02:27,788 DEBUG TRAIN Batch 1/2200 loss 46.772884 loss_att 41.454967 loss_ctc 59.181358 lr 0.00128960 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:02:27,792 DEBUG TRAIN Batch 1/2200 loss 60.243416 loss_att 53.875511 loss_ctc 75.101868 lr 0.00128960 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:02:27,793 DEBUG TRAIN Batch 1/2200 loss 48.682602 loss_att 43.190636 loss_ctc 61.497185 lr 0.00128944 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:02:27,797 DEBUG TRAIN Batch 1/2200 loss 47.112053 loss_att 42.452946 loss_ctc 57.983299 lr 0.00128944 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:02:27,802 DEBUG TRAIN Batch 1/2200 loss 46.510986 loss_att 40.783436 loss_ctc 59.875271 lr 0.00128944 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:02:27,803 DEBUG TRAIN Batch 1/2200 loss 44.410637 loss_att 40.107582 loss_ctc 54.451096 lr 0.00128960 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:05:18,261 DEBUG TRAIN Batch 1/2300 loss 30.554321 loss_att 25.672066 loss_ctc 41.946251 lr 0.00130544 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:05:18,262 DEBUG TRAIN Batch 1/2300 loss 32.916252 loss_att 29.983257 loss_ctc 39.759914 lr 0.00130560 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:05:18,268 DEBUG TRAIN Batch 1/2300 loss 28.553761 loss_att 26.178772 loss_ctc 34.095398 lr 0.00130544 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:05:18,269 DEBUG TRAIN Batch 1/2300 loss 31.620203 loss_att 28.952246 loss_ctc 37.845440 lr 0.00130544 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:05:18,270 DEBUG TRAIN Batch 1/2300 loss 35.112442 loss_att 31.019415 loss_ctc 44.662834 lr 0.00130560 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:05:18,272 DEBUG TRAIN Batch 1/2300 loss 38.282757 loss_att 34.411594 loss_ctc 47.315472 lr 0.00130560 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:05:18,275 DEBUG TRAIN Batch 1/2300 loss 37.034286 loss_att 33.723976 loss_ctc 44.758347 lr 0.00130544 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:05:18,287 DEBUG TRAIN Batch 1/2300 loss 47.219887 loss_att 43.675026 loss_ctc 55.491222 lr 0.00130560 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:08:11,004 DEBUG TRAIN Batch 1/2400 loss 52.325500 loss_att 46.442795 loss_ctc 66.051819 lr 0.00132160 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:08:11,007 DEBUG TRAIN Batch 1/2400 loss 37.459709 loss_att 34.250664 loss_ctc 44.947487 lr 0.00132144 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:08:11,007 DEBUG TRAIN Batch 1/2400 loss 42.220631 loss_att 38.700893 loss_ctc 50.433350 lr 0.00132144 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:08:11,008 DEBUG TRAIN Batch 1/2400 loss 49.903702 loss_att 42.975708 loss_ctc 66.069016 lr 0.00132160 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:08:11,013 DEBUG TRAIN Batch 1/2400 loss 34.136219 loss_att 30.737518 loss_ctc 42.066521 lr 0.00132144 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:08:11,014 DEBUG TRAIN Batch 1/2400 loss 42.639214 loss_att 39.952412 loss_ctc 48.908417 lr 0.00132144 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:08:11,017 DEBUG TRAIN Batch 1/2400 loss 41.201366 loss_att 37.955929 loss_ctc 48.774055 lr 0.00132160 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:08:11,032 DEBUG TRAIN Batch 1/2400 loss 39.310184 loss_att 35.473167 loss_ctc 48.263222 lr 0.00132160 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:07,954 DEBUG TRAIN Batch 1/2500 loss 17.830742 loss_att 16.525429 loss_ctc 20.876472 lr 0.00133760 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:07,957 DEBUG TRAIN Batch 1/2500 loss 13.112217 loss_att 11.622341 loss_ctc 16.588596 lr 0.00133744 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:07,958 DEBUG TRAIN Batch 1/2500 loss 15.446413 loss_att 13.095848 loss_ctc 20.931065 lr 0.00133760 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:07,959 DEBUG TRAIN Batch 1/2500 loss 15.866385 loss_att 13.953711 loss_ctc 20.329288 lr 0.00133744 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:07,965 DEBUG TRAIN Batch 1/2500 loss 17.900188 loss_att 14.906482 loss_ctc 24.885504 lr 0.00133744 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:07,982 DEBUG TRAIN Batch 1/2500 loss 10.140722 loss_att 9.209261 loss_ctc 12.314133 lr 0.00133760 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:08,020 DEBUG TRAIN Batch 1/2500 loss 17.967432 loss_att 15.253036 loss_ctc 24.301022 lr 0.00133744 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:08,032 DEBUG TRAIN Batch 1/2500 loss 15.339610 loss_att 14.198692 loss_ctc 18.001753 lr 0.00133760 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:40,895 DEBUG TRAIN Batch 1/2600 loss 46.149788 loss_att 41.322128 loss_ctc 57.414318 lr 0.00135344 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:40,898 DEBUG TRAIN Batch 1/2600 loss 44.314323 loss_att 39.398235 loss_ctc 55.785202 lr 0.00135344 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:40,898 DEBUG TRAIN Batch 1/2600 loss 46.802467 loss_att 42.468044 loss_ctc 56.916119 lr 0.00135360 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:40,901 DEBUG TRAIN Batch 1/2600 loss 44.514225 loss_att 41.192593 loss_ctc 52.264702 lr 0.00135344 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:40,901 DEBUG TRAIN Batch 1/2600 loss 56.815155 loss_att 51.460686 loss_ctc 69.308922 lr 0.00135360 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:40,902 DEBUG TRAIN Batch 1/2600 loss 39.283051 loss_att 36.465725 loss_ctc 45.856808 lr 0.00135360 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:40,902 DEBUG TRAIN Batch 1/2600 loss 43.794418 loss_att 38.021614 loss_ctc 57.264290 lr 0.00135344 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:11:40,911 DEBUG TRAIN Batch 1/2600 loss 39.339668 loss_att 34.470711 loss_ctc 50.700562 lr 0.00135360 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:14:36,856 DEBUG TRAIN Batch 1/2700 loss 35.575092 loss_att 31.780081 loss_ctc 44.430122 lr 0.00136960 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:14:36,858 DEBUG TRAIN Batch 1/2700 loss 49.730072 loss_att 44.574749 loss_ctc 61.759155 lr 0.00136944 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:14:36,859 DEBUG TRAIN Batch 1/2700 loss 40.848888 loss_att 37.276089 loss_ctc 49.185421 lr 0.00136944 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:14:36,861 DEBUG TRAIN Batch 1/2700 loss 49.952095 loss_att 44.564201 loss_ctc 62.523842 lr 0.00136944 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:14:36,867 DEBUG TRAIN Batch 1/2700 loss 43.467140 loss_att 38.958961 loss_ctc 53.986229 lr 0.00136944 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:14:36,868 DEBUG TRAIN Batch 1/2700 loss 48.826736 loss_att 45.643990 loss_ctc 56.253143 lr 0.00136960 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:14:36,869 DEBUG TRAIN Batch 1/2700 loss 46.939987 loss_att 42.818569 loss_ctc 56.556625 lr 0.00136960 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:14:36,885 DEBUG TRAIN Batch 1/2700 loss 40.866562 loss_att 39.314857 loss_ctc 44.487213 lr 0.00136960 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:17:32,391 DEBUG TRAIN Batch 1/2800 loss 25.711473 loss_att 22.965645 loss_ctc 32.118408 lr 0.00138560 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:17:32,394 DEBUG TRAIN Batch 1/2800 loss 33.556416 loss_att 30.718285 loss_ctc 40.178726 lr 0.00138560 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:17:32,395 DEBUG TRAIN Batch 1/2800 loss 36.590183 loss_att 31.857641 loss_ctc 47.632778 lr 0.00138544 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:17:32,396 DEBUG TRAIN Batch 1/2800 loss 41.025509 loss_att 36.719234 loss_ctc 51.073483 lr 0.00138544 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:17:32,397 DEBUG TRAIN Batch 1/2800 loss 44.014862 loss_att 38.702705 loss_ctc 56.409882 lr 0.00138544 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:17:32,400 DEBUG TRAIN Batch 1/2800 loss 28.095814 loss_att 26.500271 loss_ctc 31.818741 lr 0.00138560 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:17:32,401 DEBUG TRAIN Batch 1/2800 loss 33.660172 loss_att 30.153296 loss_ctc 41.842880 lr 0.00138544 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:17:32,420 DEBUG TRAIN Batch 1/2800 loss 42.566685 loss_att 38.444313 loss_ctc 52.185555 lr 0.00138560 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:20:26,264 DEBUG TRAIN Batch 1/2900 loss 47.300587 loss_att 42.408333 loss_ctc 58.715843 lr 0.00140144 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:20:26,270 DEBUG TRAIN Batch 1/2900 loss 47.045219 loss_att 41.529366 loss_ctc 59.915550 lr 0.00140144 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:20:26,272 DEBUG TRAIN Batch 1/2900 loss 39.677887 loss_att 35.534008 loss_ctc 49.346935 lr 0.00140144 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:20:26,273 DEBUG TRAIN Batch 1/2900 loss 36.341991 loss_att 32.108440 loss_ctc 46.220272 lr 0.00140160 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:20:26,274 DEBUG TRAIN Batch 1/2900 loss 43.238102 loss_att 39.130268 loss_ctc 52.823044 lr 0.00140144 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:20:26,275 DEBUG TRAIN Batch 1/2900 loss 43.294224 loss_att 39.589687 loss_ctc 51.938141 lr 0.00140160 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:20:26,276 DEBUG TRAIN Batch 1/2900 loss 35.691162 loss_att 31.309525 loss_ctc 45.914989 lr 0.00140160 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:20:26,279 DEBUG TRAIN Batch 1/2900 loss 54.180550 loss_att 47.653320 loss_ctc 69.410751 lr 0.00140160 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:18,061 DEBUG TRAIN Batch 1/3000 loss 13.687499 loss_att 12.005885 loss_ctc 17.611265 lr 0.00141760 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:18,063 DEBUG TRAIN Batch 1/3000 loss 17.347336 loss_att 15.342476 loss_ctc 22.025343 lr 0.00141744 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:18,063 DEBUG TRAIN Batch 1/3000 loss 18.911289 loss_att 17.536148 loss_ctc 22.119953 lr 0.00141744 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:18,063 DEBUG TRAIN Batch 1/3000 loss 14.734155 loss_att 12.626471 loss_ctc 19.652082 lr 0.00141760 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:18,065 DEBUG TRAIN Batch 1/3000 loss 18.683392 loss_att 17.099255 loss_ctc 22.379713 lr 0.00141744 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:18,070 DEBUG TRAIN Batch 1/3000 loss 12.033228 loss_att 11.005447 loss_ctc 14.431384 lr 0.00141744 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:18,123 DEBUG TRAIN Batch 1/3000 loss 14.411827 loss_att 12.851692 loss_ctc 18.052143 lr 0.00141760 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:18,141 DEBUG TRAIN Batch 1/3000 loss 18.708729 loss_att 16.607782 loss_ctc 23.610939 lr 0.00141760 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:51,028 DEBUG TRAIN Batch 1/3100 loss 36.359661 loss_att 33.479294 loss_ctc 43.080517 lr 0.00143344 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:51,031 DEBUG TRAIN Batch 1/3100 loss 26.282280 loss_att 24.028881 loss_ctc 31.540213 lr 0.00143360 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:51,036 DEBUG TRAIN Batch 1/3100 loss 63.105595 loss_att 59.164856 loss_ctc 72.300652 lr 0.00143344 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:51,037 DEBUG TRAIN Batch 1/3100 loss 47.389587 loss_att 42.048416 loss_ctc 59.852325 lr 0.00143344 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:51,040 DEBUG TRAIN Batch 1/3100 loss 33.731052 loss_att 31.929718 loss_ctc 37.934170 lr 0.00143344 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:51,042 DEBUG TRAIN Batch 1/3100 loss 40.859665 loss_att 36.841972 loss_ctc 50.234276 lr 0.00143360 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:51,047 DEBUG TRAIN Batch 1/3100 loss 43.797894 loss_att 39.023632 loss_ctc 54.937836 lr 0.00143360 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:23:51,048 DEBUG TRAIN Batch 1/3100 loss 45.886856 loss_att 41.418312 loss_ctc 56.313457 lr 0.00143360 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:26:42,232 DEBUG TRAIN Batch 1/3200 loss 58.585587 loss_att 52.105133 loss_ctc 73.706650 lr 0.00144960 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:26:42,240 DEBUG TRAIN Batch 1/3200 loss 51.719444 loss_att 46.423859 loss_ctc 64.075813 lr 0.00144960 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:26:42,244 DEBUG TRAIN Batch 1/3200 loss 37.595070 loss_att 33.836876 loss_ctc 46.364189 lr 0.00144944 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:26:42,245 DEBUG TRAIN Batch 1/3200 loss 34.831726 loss_att 31.345047 loss_ctc 42.967316 lr 0.00144944 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:26:42,246 DEBUG TRAIN Batch 1/3200 loss 37.096752 loss_att 33.447678 loss_ctc 45.611259 lr 0.00144944 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:26:42,253 DEBUG TRAIN Batch 1/3200 loss 39.890007 loss_att 37.040359 loss_ctc 46.539185 lr 0.00144960 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:26:42,257 DEBUG TRAIN Batch 1/3200 loss 36.403450 loss_att 33.024803 loss_ctc 44.286964 lr 0.00144944 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:26:42,257 DEBUG TRAIN Batch 1/3200 loss 42.660355 loss_att 38.383160 loss_ctc 52.640472 lr 0.00144960 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:29:32,943 DEBUG TRAIN Batch 1/3300 loss 29.923489 loss_att 28.046597 loss_ctc 34.302902 lr 0.00146544 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:29:32,949 DEBUG TRAIN Batch 1/3300 loss 33.960766 loss_att 30.820889 loss_ctc 41.287148 lr 0.00146544 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:29:32,950 DEBUG TRAIN Batch 1/3300 loss 35.526031 loss_att 31.252853 loss_ctc 45.496777 lr 0.00146544 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:29:32,951 DEBUG TRAIN Batch 1/3300 loss 34.866447 loss_att 30.742834 loss_ctc 44.488213 lr 0.00146560 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:29:32,953 DEBUG TRAIN Batch 1/3300 loss 28.207499 loss_att 24.857252 loss_ctc 36.024742 lr 0.00146544 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:29:32,955 DEBUG TRAIN Batch 1/3300 loss 41.895241 loss_att 37.801018 loss_ctc 51.448425 lr 0.00146560 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:29:32,961 DEBUG TRAIN Batch 1/3300 loss 37.241356 loss_att 32.207596 loss_ctc 48.986797 lr 0.00146560 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:29:32,970 DEBUG TRAIN Batch 1/3300 loss 32.768982 loss_att 28.644184 loss_ctc 42.393509 lr 0.00146560 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:32:22,521 DEBUG TRAIN Batch 1/3400 loss 36.060066 loss_att 31.954315 loss_ctc 45.640144 lr 0.00148160 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:32:22,525 DEBUG TRAIN Batch 1/3400 loss 36.143299 loss_att 33.070862 loss_ctc 43.312317 lr 0.00148144 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:32:22,526 DEBUG TRAIN Batch 1/3400 loss 34.180984 loss_att 32.256096 loss_ctc 38.672386 lr 0.00148144 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:32:22,526 DEBUG TRAIN Batch 1/3400 loss 41.424084 loss_att 37.564137 loss_ctc 50.430626 lr 0.00148144 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:32:22,540 DEBUG TRAIN Batch 1/3400 loss 44.858780 loss_att 40.780991 loss_ctc 54.373611 lr 0.00148160 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:32:22,548 DEBUG TRAIN Batch 1/3400 loss 38.262829 loss_att 34.641670 loss_ctc 46.712200 lr 0.00148160 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:32:22,550 DEBUG TRAIN Batch 1/3400 loss 34.249901 loss_att 30.637985 loss_ctc 42.677704 lr 0.00148160 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:32:22,559 DEBUG TRAIN Batch 1/3400 loss 34.350040 loss_att 30.140217 loss_ctc 44.172955 lr 0.00148144 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:11,527 DEBUG TRAIN Batch 1/3500 loss 16.058353 loss_att 13.647479 loss_ctc 21.683727 lr 0.00149760 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:11,529 DEBUG TRAIN Batch 1/3500 loss 14.245539 loss_att 12.894684 loss_ctc 17.397533 lr 0.00149760 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:11,529 DEBUG TRAIN Batch 1/3500 loss 14.668140 loss_att 13.123556 loss_ctc 18.272169 lr 0.00149744 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:11,547 DEBUG TRAIN Batch 1/3500 loss 15.373869 loss_att 13.303912 loss_ctc 20.203766 lr 0.00149744 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:11,548 DEBUG TRAIN Batch 1/3500 loss 12.710453 loss_att 11.590820 loss_ctc 15.322927 lr 0.00149744 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:11,555 DEBUG TRAIN Batch 1/3500 loss 12.467617 loss_att 11.294291 loss_ctc 15.205376 lr 0.00149760 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:11,562 DEBUG TRAIN Batch 1/3500 loss 17.395546 loss_att 16.213394 loss_ctc 20.153896 lr 0.00149760 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:11,606 DEBUG TRAIN Batch 1/3500 loss 13.086704 loss_att 11.210526 loss_ctc 17.464453 lr 0.00149744 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:44,595 DEBUG TRAIN Batch 1/3600 loss 42.358543 loss_att 37.877522 loss_ctc 52.814262 lr 0.00151360 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:44,595 DEBUG TRAIN Batch 1/3600 loss 39.465729 loss_att 34.987015 loss_ctc 49.916058 lr 0.00151344 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:44,599 DEBUG TRAIN Batch 1/3600 loss 35.835640 loss_att 32.332539 loss_ctc 44.009544 lr 0.00151360 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:44,601 DEBUG TRAIN Batch 1/3600 loss 40.625706 loss_att 36.501534 loss_ctc 50.248772 lr 0.00151344 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:44,604 DEBUG TRAIN Batch 1/3600 loss 33.808338 loss_att 30.750210 loss_ctc 40.943970 lr 0.00151344 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:44,623 DEBUG TRAIN Batch 1/3600 loss 41.925362 loss_att 38.438904 loss_ctc 50.060425 lr 0.00151344 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:44,624 DEBUG TRAIN Batch 1/3600 loss 38.433216 loss_att 34.920433 loss_ctc 46.629711 lr 0.00151360 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:35:44,637 DEBUG TRAIN Batch 1/3600 loss 41.370094 loss_att 36.687691 loss_ctc 52.295700 lr 0.00151360 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:38:33,685 DEBUG TRAIN Batch 1/3700 loss 43.613613 loss_att 38.452934 loss_ctc 55.655197 lr 0.00152960 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:38:33,686 DEBUG TRAIN Batch 1/3700 loss 41.440536 loss_att 37.250366 loss_ctc 51.217602 lr 0.00152944 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:38:33,686 DEBUG TRAIN Batch 1/3700 loss 39.048897 loss_att 33.326458 loss_ctc 52.401257 lr 0.00152960 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:38:33,692 DEBUG TRAIN Batch 1/3700 loss 36.084637 loss_att 31.595970 loss_ctc 46.558197 lr 0.00152944 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:38:33,695 DEBUG TRAIN Batch 1/3700 loss 32.497551 loss_att 28.536659 loss_ctc 41.739632 lr 0.00152944 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:38:33,701 DEBUG TRAIN Batch 1/3700 loss 49.680550 loss_att 44.749249 loss_ctc 61.186913 lr 0.00152944 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:38:33,704 DEBUG TRAIN Batch 1/3700 loss 38.704617 loss_att 34.667854 loss_ctc 48.123730 lr 0.00152960 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:38:33,705 DEBUG TRAIN Batch 1/3700 loss 27.101559 loss_att 23.880032 loss_ctc 34.618454 lr 0.00152960 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:41:23,183 DEBUG TRAIN Batch 1/3800 loss 40.218239 loss_att 37.787071 loss_ctc 45.890961 lr 0.00154544 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:41:23,183 DEBUG TRAIN Batch 1/3800 loss 40.312916 loss_att 36.389313 loss_ctc 49.467983 lr 0.00154560 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:41:23,183 DEBUG TRAIN Batch 1/3800 loss 52.269066 loss_att 47.045532 loss_ctc 64.457298 lr 0.00154544 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:41:23,187 DEBUG TRAIN Batch 1/3800 loss 35.674305 loss_att 32.307808 loss_ctc 43.529461 lr 0.00154560 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:41:23,187 DEBUG TRAIN Batch 1/3800 loss 34.439938 loss_att 28.931555 loss_ctc 47.292828 lr 0.00154544 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:41:23,192 DEBUG TRAIN Batch 1/3800 loss 29.717793 loss_att 26.999182 loss_ctc 36.061218 lr 0.00154544 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:41:23,201 DEBUG TRAIN Batch 1/3800 loss 31.165371 loss_att 29.037510 loss_ctc 36.130379 lr 0.00154560 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:41:23,204 DEBUG TRAIN Batch 1/3800 loss 30.009758 loss_att 26.442200 loss_ctc 38.334061 lr 0.00154560 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:44:08,706 DEBUG TRAIN Batch 1/3900 loss 40.896423 loss_att 37.022469 loss_ctc 49.935650 lr 0.00156144 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:44:08,710 DEBUG TRAIN Batch 1/3900 loss 35.072350 loss_att 31.739086 loss_ctc 42.849960 lr 0.00156160 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:44:08,710 DEBUG TRAIN Batch 1/3900 loss 41.725330 loss_att 37.504631 loss_ctc 51.573635 lr 0.00156144 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:44:08,713 DEBUG TRAIN Batch 1/3900 loss 37.443108 loss_att 34.767010 loss_ctc 43.687328 lr 0.00156144 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:44:08,719 DEBUG TRAIN Batch 1/3900 loss 42.360992 loss_att 37.700623 loss_ctc 53.235191 lr 0.00156144 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:44:08,722 DEBUG TRAIN Batch 1/3900 loss 36.708767 loss_att 32.692421 loss_ctc 46.080242 lr 0.00156160 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:44:08,726 DEBUG TRAIN Batch 1/3900 loss 43.698593 loss_att 38.735359 loss_ctc 55.279472 lr 0.00156160 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:44:08,733 DEBUG TRAIN Batch 1/3900 loss 49.951164 loss_att 44.269508 loss_ctc 63.208366 lr 0.00156160 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:46:55,761 DEBUG TRAIN Batch 1/4000 loss 17.131134 loss_att 15.060049 loss_ctc 21.963665 lr 0.00157744 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:46:55,762 DEBUG TRAIN Batch 1/4000 loss 15.298813 loss_att 13.352615 loss_ctc 19.839943 lr 0.00157744 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:46:55,762 DEBUG TRAIN Batch 1/4000 loss 12.934624 loss_att 11.461399 loss_ctc 16.372150 lr 0.00157760 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:46:55,762 DEBUG TRAIN Batch 1/4000 loss 18.241446 loss_att 16.474003 loss_ctc 22.365477 lr 0.00157744 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:46:55,764 DEBUG TRAIN Batch 1/4000 loss 13.917959 loss_att 12.293837 loss_ctc 17.707579 lr 0.00157744 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:46:55,766 DEBUG TRAIN Batch 1/4000 loss 14.309572 loss_att 12.625888 loss_ctc 18.238167 lr 0.00157760 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:46:55,859 DEBUG TRAIN Batch 1/4000 loss 17.902227 loss_att 16.399961 loss_ctc 21.407513 lr 0.00157760 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:46:55,877 DEBUG TRAIN Batch 1/4000 loss 15.061485 loss_att 13.329775 loss_ctc 19.102144 lr 0.00157760 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:47:28,954 DEBUG TRAIN Batch 1/4100 loss 35.738087 loss_att 30.244457 loss_ctc 48.556557 lr 0.00159344 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:47:28,955 DEBUG TRAIN Batch 1/4100 loss 49.360382 loss_att 43.830582 loss_ctc 62.263260 lr 0.00159344 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:47:28,955 DEBUG TRAIN Batch 1/4100 loss 44.674274 loss_att 39.864311 loss_ctc 55.897518 lr 0.00159344 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:47:28,956 DEBUG TRAIN Batch 1/4100 loss 43.747192 loss_att 39.001461 loss_ctc 54.820564 lr 0.00159360 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:47:28,957 DEBUG TRAIN Batch 1/4100 loss 44.703247 loss_att 40.246407 loss_ctc 55.102535 lr 0.00159360 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:47:28,963 DEBUG TRAIN Batch 1/4100 loss 43.105541 loss_att 38.637253 loss_ctc 53.531551 lr 0.00159360 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:47:28,963 DEBUG TRAIN Batch 1/4100 loss 43.664021 loss_att 38.447662 loss_ctc 55.835522 lr 0.00159344 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:47:28,974 DEBUG TRAIN Batch 1/4100 loss 38.059036 loss_att 33.227406 loss_ctc 49.332836 lr 0.00159360 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:50:13,858 DEBUG TRAIN Batch 1/4200 loss 33.708561 loss_att 30.689911 loss_ctc 40.752075 lr 0.00160960 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:50:13,858 DEBUG TRAIN Batch 1/4200 loss 36.588486 loss_att 32.766579 loss_ctc 45.506268 lr 0.00160944 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:50:13,864 DEBUG TRAIN Batch 1/4200 loss 43.222660 loss_att 39.209808 loss_ctc 52.585979 lr 0.00160960 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:50:13,867 DEBUG TRAIN Batch 1/4200 loss 42.479439 loss_att 38.552238 loss_ctc 51.642906 lr 0.00160944 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:50:13,873 DEBUG TRAIN Batch 1/4200 loss 42.235817 loss_att 35.818569 loss_ctc 57.209396 lr 0.00160944 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:50:13,875 DEBUG TRAIN Batch 1/4200 loss 41.223412 loss_att 37.262711 loss_ctc 50.465042 lr 0.00160960 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:50:13,879 DEBUG TRAIN Batch 1/4200 loss 26.576090 loss_att 23.551914 loss_ctc 33.632500 lr 0.00160944 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:50:13,881 DEBUG TRAIN Batch 1/4200 loss 31.051651 loss_att 28.108322 loss_ctc 37.919415 lr 0.00160960 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:52:59,186 DEBUG TRAIN Batch 1/4300 loss 43.502686 loss_att 37.448814 loss_ctc 57.628384 lr 0.00162544 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:52:59,186 DEBUG TRAIN Batch 1/4300 loss 30.617073 loss_att 27.281693 loss_ctc 38.399628 lr 0.00162560 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:52:59,194 DEBUG TRAIN Batch 1/4300 loss 34.448082 loss_att 30.469069 loss_ctc 43.732445 lr 0.00162560 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:52:59,195 DEBUG TRAIN Batch 1/4300 loss 23.543911 loss_att 22.467758 loss_ctc 26.054932 lr 0.00162544 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:52:59,201 DEBUG TRAIN Batch 1/4300 loss 32.571487 loss_att 28.682190 loss_ctc 41.646511 lr 0.00162544 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:52:59,201 DEBUG TRAIN Batch 1/4300 loss 32.643723 loss_att 28.030773 loss_ctc 43.407280 lr 0.00162544 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:52:59,204 DEBUG TRAIN Batch 1/4300 loss 39.399261 loss_att 34.384937 loss_ctc 51.099346 lr 0.00162560 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:52:59,208 DEBUG TRAIN Batch 1/4300 loss 33.103310 loss_att 30.019058 loss_ctc 40.299896 lr 0.00162560 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:55:41,079 DEBUG TRAIN Batch 1/4400 loss 41.058922 loss_att 36.491783 loss_ctc 51.715576 lr 0.00164160 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:55:41,083 DEBUG TRAIN Batch 1/4400 loss 43.415333 loss_att 38.755421 loss_ctc 54.288464 lr 0.00164144 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:55:41,085 DEBUG TRAIN Batch 1/4400 loss 34.828468 loss_att 31.624119 loss_ctc 42.305283 lr 0.00164160 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:55:41,089 DEBUG TRAIN Batch 1/4400 loss 30.738995 loss_att 26.831884 loss_ctc 39.855587 lr 0.00164144 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:55:41,089 DEBUG TRAIN Batch 1/4400 loss 33.367615 loss_att 30.425064 loss_ctc 40.233559 lr 0.00164160 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:55:41,091 DEBUG TRAIN Batch 1/4400 loss 42.278431 loss_att 37.639664 loss_ctc 53.102219 lr 0.00164144 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:55:41,098 DEBUG TRAIN Batch 1/4400 loss 40.527603 loss_att 36.265606 loss_ctc 50.472271 lr 0.00164144 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:55:41,108 DEBUG TRAIN Batch 1/4400 loss 33.712357 loss_att 29.144863 loss_ctc 44.369843 lr 0.00164160 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:23,279 DEBUG TRAIN Batch 1/4500 loss 11.505489 loss_att 10.536251 loss_ctc 13.767046 lr 0.00165760 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:23,282 DEBUG TRAIN Batch 1/4500 loss 17.700432 loss_att 15.931532 loss_ctc 21.827862 lr 0.00165744 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:23,292 DEBUG TRAIN Batch 1/4500 loss 18.625572 loss_att 16.379242 loss_ctc 23.867010 lr 0.00165744 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:23,302 DEBUG TRAIN Batch 1/4500 loss 17.304464 loss_att 15.084576 loss_ctc 22.484203 lr 0.00165760 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:23,305 DEBUG TRAIN Batch 1/4500 loss 10.755384 loss_att 8.698366 loss_ctc 15.555092 lr 0.00165760 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:23,322 DEBUG TRAIN Batch 1/4500 loss 15.111140 loss_att 13.665927 loss_ctc 18.483305 lr 0.00165744 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:23,337 DEBUG TRAIN Batch 1/4500 loss 22.403748 loss_att 19.193571 loss_ctc 29.894161 lr 0.00165760 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:23,375 DEBUG TRAIN Batch 1/4500 loss 15.422865 loss_att 13.993376 loss_ctc 18.758339 lr 0.00165744 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:56,324 DEBUG TRAIN Batch 1/4600 loss 37.453033 loss_att 34.892944 loss_ctc 43.426571 lr 0.00167344 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:56,326 DEBUG TRAIN Batch 1/4600 loss 52.068615 loss_att 46.883247 loss_ctc 64.167809 lr 0.00167344 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:56,328 DEBUG TRAIN Batch 1/4600 loss 39.302490 loss_att 36.183067 loss_ctc 46.581139 lr 0.00167360 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:56,332 DEBUG TRAIN Batch 1/4600 loss 53.974594 loss_att 50.482765 loss_ctc 62.122189 lr 0.00167360 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:56,337 DEBUG TRAIN Batch 1/4600 loss 38.387840 loss_att 35.384804 loss_ctc 45.394928 lr 0.00167344 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:56,339 DEBUG TRAIN Batch 1/4600 loss 39.425240 loss_att 34.049053 loss_ctc 51.969677 lr 0.00167360 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:56,344 DEBUG TRAIN Batch 1/4600 loss 35.748344 loss_att 33.296661 loss_ctc 41.468929 lr 0.00167344 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 07:58:56,345 DEBUG TRAIN Batch 1/4600 loss 45.992702 loss_att 41.244282 loss_ctc 57.072353 lr 0.00167360 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:01:41,499 DEBUG TRAIN Batch 1/4700 loss 29.820250 loss_att 26.251890 loss_ctc 38.146420 lr 0.00168960 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:01:41,503 DEBUG TRAIN Batch 1/4700 loss 42.497528 loss_att 37.735779 loss_ctc 53.608269 lr 0.00168944 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:01:41,505 DEBUG TRAIN Batch 1/4700 loss 35.459606 loss_att 32.634682 loss_ctc 42.051102 lr 0.00168960 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:01:41,507 DEBUG TRAIN Batch 1/4700 loss 52.265530 loss_att 45.969688 loss_ctc 66.955833 lr 0.00168944 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:01:41,509 DEBUG TRAIN Batch 1/4700 loss 39.267956 loss_att 36.145790 loss_ctc 46.553001 lr 0.00168944 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:01:41,514 DEBUG TRAIN Batch 1/4700 loss 32.166981 loss_att 29.886101 loss_ctc 37.489029 lr 0.00168944 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:01:41,519 DEBUG TRAIN Batch 1/4700 loss 36.773903 loss_att 33.174294 loss_ctc 45.172985 lr 0.00168960 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:01:41,521 DEBUG TRAIN Batch 1/4700 loss 42.157600 loss_att 37.491455 loss_ctc 53.045273 lr 0.00168960 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:04:34,208 DEBUG TRAIN Batch 1/4800 loss 26.596216 loss_att 23.797522 loss_ctc 33.126507 lr 0.00170544 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:04:34,208 DEBUG TRAIN Batch 1/4800 loss 41.492714 loss_att 37.065796 loss_ctc 51.822186 lr 0.00170544 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:04:34,211 DEBUG TRAIN Batch 1/4800 loss 21.667839 loss_att 18.312788 loss_ctc 29.496292 lr 0.00170560 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:04:34,211 DEBUG TRAIN Batch 1/4800 loss 39.410473 loss_att 34.891129 loss_ctc 49.955612 lr 0.00170544 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:04:34,212 DEBUG TRAIN Batch 1/4800 loss 29.191977 loss_att 25.233999 loss_ctc 38.427258 lr 0.00170560 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:04:34,220 DEBUG TRAIN Batch 1/4800 loss 30.233505 loss_att 26.802450 loss_ctc 38.239300 lr 0.00170544 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:04:34,230 DEBUG TRAIN Batch 1/4800 loss 36.140453 loss_att 31.720266 loss_ctc 46.454220 lr 0.00170560 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:04:34,240 DEBUG TRAIN Batch 1/4800 loss 35.358589 loss_att 31.618931 loss_ctc 44.084465 lr 0.00170560 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:07:31,027 DEBUG TRAIN Batch 1/4900 loss 33.916592 loss_att 30.724318 loss_ctc 41.365223 lr 0.00172160 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:07:31,031 DEBUG TRAIN Batch 1/4900 loss 33.537697 loss_att 29.640944 loss_ctc 42.630112 lr 0.00172144 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:07:31,031 DEBUG TRAIN Batch 1/4900 loss 39.810715 loss_att 34.878048 loss_ctc 51.320267 lr 0.00172144 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:07:31,033 DEBUG TRAIN Batch 1/4900 loss 35.324032 loss_att 31.498417 loss_ctc 44.250469 lr 0.00172144 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:07:31,036 DEBUG TRAIN Batch 1/4900 loss 32.279129 loss_att 28.757565 loss_ctc 40.496109 lr 0.00172160 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:07:31,040 DEBUG TRAIN Batch 1/4900 loss 42.076782 loss_att 37.196465 loss_ctc 53.464184 lr 0.00172144 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:07:31,040 DEBUG TRAIN Batch 1/4900 loss 42.467339 loss_att 37.811661 loss_ctc 53.330578 lr 0.00172160 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:07:31,047 DEBUG TRAIN Batch 1/4900 loss 35.510284 loss_att 31.533346 loss_ctc 44.789806 lr 0.00172160 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:10:29,598 DEBUG TRAIN Batch 1/5000 loss 13.471559 loss_att 11.655190 loss_ctc 17.709753 lr 0.00173744 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:10:29,602 DEBUG TRAIN Batch 1/5000 loss 15.999807 loss_att 14.273239 loss_ctc 20.028467 lr 0.00173744 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:10:29,603 DEBUG TRAIN Batch 1/5000 loss 12.345594 loss_att 11.483124 loss_ctc 14.358027 lr 0.00173760 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:10:29,604 DEBUG TRAIN Batch 1/5000 loss 15.107536 loss_att 13.067812 loss_ctc 19.866890 lr 0.00173760 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:10:29,605 DEBUG TRAIN Batch 1/5000 loss 22.028942 loss_att 19.347389 loss_ctc 28.285902 lr 0.00173744 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:10:29,610 DEBUG TRAIN Batch 1/5000 loss 16.270231 loss_att 14.913272 loss_ctc 19.436468 lr 0.00173744 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:10:29,626 DEBUG TRAIN Batch 1/5000 loss 11.285141 loss_att 9.509637 loss_ctc 15.427985 lr 0.00173760 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:10:29,668 DEBUG TRAIN Batch 1/5000 loss 14.923029 loss_att 12.811648 loss_ctc 19.849585 lr 0.00173760 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:11:02,764 DEBUG TRAIN Batch 1/5100 loss 35.116623 loss_att 30.533834 loss_ctc 45.809795 lr 0.00175344 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:11:02,769 DEBUG TRAIN Batch 1/5100 loss 36.939198 loss_att 33.553566 loss_ctc 44.839001 lr 0.00175360 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:11:02,770 DEBUG TRAIN Batch 1/5100 loss 31.617958 loss_att 28.573055 loss_ctc 38.722733 lr 0.00175360 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:11:02,775 DEBUG TRAIN Batch 1/5100 loss 34.107147 loss_att 30.271191 loss_ctc 43.057720 lr 0.00175344 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:11:02,779 DEBUG TRAIN Batch 1/5100 loss 32.021008 loss_att 28.945129 loss_ctc 39.198055 lr 0.00175344 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:11:02,779 DEBUG TRAIN Batch 1/5100 loss 43.702072 loss_att 38.580063 loss_ctc 55.653419 lr 0.00175360 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:11:02,780 DEBUG TRAIN Batch 1/5100 loss 34.347958 loss_att 30.106817 loss_ctc 44.243954 lr 0.00175344 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:11:02,790 DEBUG TRAIN Batch 1/5100 loss 34.572205 loss_att 31.488262 loss_ctc 41.768074 lr 0.00175360 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:13:59,332 DEBUG TRAIN Batch 1/5200 loss 31.568268 loss_att 27.586477 loss_ctc 40.859108 lr 0.00176960 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:13:59,334 DEBUG TRAIN Batch 1/5200 loss 31.415604 loss_att 28.524014 loss_ctc 38.162651 lr 0.00176944 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:13:59,335 DEBUG TRAIN Batch 1/5200 loss 40.511318 loss_att 35.834675 loss_ctc 51.423485 lr 0.00176960 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:13:59,337 DEBUG TRAIN Batch 1/5200 loss 32.513187 loss_att 29.249931 loss_ctc 40.127449 lr 0.00176944 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:13:59,341 DEBUG TRAIN Batch 1/5200 loss 36.401440 loss_att 33.680298 loss_ctc 42.750771 lr 0.00176944 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:13:59,341 DEBUG TRAIN Batch 1/5200 loss 33.140747 loss_att 30.874466 loss_ctc 38.428741 lr 0.00176944 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:13:59,341 DEBUG TRAIN Batch 1/5200 loss 34.312492 loss_att 30.376663 loss_ctc 43.496086 lr 0.00176960 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:13:59,354 DEBUG TRAIN Batch 1/5200 loss 32.456638 loss_att 28.995537 loss_ctc 40.532539 lr 0.00176960 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:17:00,861 DEBUG TRAIN Batch 1/5300 loss 31.139051 loss_att 28.069641 loss_ctc 38.301010 lr 0.00178560 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:17:00,864 DEBUG TRAIN Batch 1/5300 loss 26.588264 loss_att 22.656340 loss_ctc 35.762753 lr 0.00178560 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:17:00,866 DEBUG TRAIN Batch 1/5300 loss 29.125523 loss_att 25.584442 loss_ctc 37.388042 lr 0.00178544 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:17:00,869 DEBUG TRAIN Batch 1/5300 loss 26.473089 loss_att 23.597294 loss_ctc 33.183281 lr 0.00178544 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:17:00,869 DEBUG TRAIN Batch 1/5300 loss 36.332592 loss_att 31.428303 loss_ctc 47.775936 lr 0.00178560 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:17:00,870 DEBUG TRAIN Batch 1/5300 loss 21.223312 loss_att 17.786615 loss_ctc 29.242270 lr 0.00178544 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:17:00,880 DEBUG TRAIN Batch 1/5300 loss 44.400978 loss_att 39.104568 loss_ctc 56.759274 lr 0.00178544 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:17:00,891 DEBUG TRAIN Batch 1/5300 loss 32.117767 loss_att 28.864502 loss_ctc 39.708717 lr 0.00178560 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:19:58,069 DEBUG TRAIN Batch 1/5400 loss 39.979279 loss_att 35.587715 loss_ctc 50.226257 lr 0.00180160 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:19:58,070 DEBUG TRAIN Batch 1/5400 loss 39.098103 loss_att 33.832912 loss_ctc 51.383545 lr 0.00180144 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:19:58,069 DEBUG TRAIN Batch 1/5400 loss 42.026344 loss_att 36.622967 loss_ctc 54.634232 lr 0.00180144 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:19:58,076 DEBUG TRAIN Batch 1/5400 loss 33.990704 loss_att 29.001923 loss_ctc 45.631195 lr 0.00180144 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:19:58,078 DEBUG TRAIN Batch 1/5400 loss 33.190205 loss_att 29.228725 loss_ctc 42.433651 lr 0.00180160 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:19:58,077 DEBUG TRAIN Batch 1/5400 loss 34.319324 loss_att 29.940166 loss_ctc 44.537354 lr 0.00180144 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:19:58,080 DEBUG TRAIN Batch 1/5400 loss 33.933952 loss_att 31.574947 loss_ctc 39.438305 lr 0.00180160 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:19:58,086 DEBUG TRAIN Batch 1/5400 loss 31.443584 loss_att 28.663269 loss_ctc 37.930992 lr 0.00180160 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:22:56,555 DEBUG TRAIN Batch 1/5500 loss 17.830307 loss_att 15.969391 loss_ctc 22.172445 lr 0.00181744 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:22:56,556 DEBUG TRAIN Batch 1/5500 loss 11.201794 loss_att 9.718282 loss_ctc 14.663322 lr 0.00181760 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:22:56,559 DEBUG TRAIN Batch 1/5500 loss 15.688269 loss_att 14.627335 loss_ctc 18.163780 lr 0.00181744 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:22:56,573 DEBUG TRAIN Batch 1/5500 loss 21.501228 loss_att 18.720016 loss_ctc 27.990721 lr 0.00181760 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:22:56,584 DEBUG TRAIN Batch 1/5500 loss 15.731104 loss_att 13.484177 loss_ctc 20.973930 lr 0.00181760 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:22:56,632 DEBUG TRAIN Batch 1/5500 loss 8.705458 loss_att 7.914283 loss_ctc 10.551532 lr 0.00181744 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:22:56,650 DEBUG TRAIN Batch 1/5500 loss 8.934612 loss_att 7.719922 loss_ctc 11.768888 lr 0.00181760 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:22:56,649 DEBUG TRAIN Batch 1/5500 loss 9.957773 loss_att 8.730594 loss_ctc 12.821194 lr 0.00181744 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:23:29,586 DEBUG TRAIN Batch 1/5600 loss 38.018406 loss_att 33.527603 loss_ctc 48.496941 lr 0.00183360 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:23:29,594 DEBUG TRAIN Batch 1/5600 loss 47.085915 loss_att 41.809425 loss_ctc 59.397720 lr 0.00183360 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:23:29,595 DEBUG TRAIN Batch 1/5600 loss 35.726082 loss_att 30.975439 loss_ctc 46.810909 lr 0.00183360 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:23:29,595 DEBUG TRAIN Batch 1/5600 loss 28.423656 loss_att 24.905575 loss_ctc 36.632511 lr 0.00183344 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:23:29,596 DEBUG TRAIN Batch 1/5600 loss 31.082693 loss_att 27.248447 loss_ctc 40.029263 lr 0.00183344 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:23:29,598 DEBUG TRAIN Batch 1/5600 loss 34.998821 loss_att 31.045887 loss_ctc 44.222332 lr 0.00183344 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:23:29,599 DEBUG TRAIN Batch 1/5600 loss 32.856449 loss_att 29.526846 loss_ctc 40.625519 lr 0.00183344 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:23:29,607 DEBUG TRAIN Batch 1/5600 loss 32.373283 loss_att 29.314077 loss_ctc 39.511429 lr 0.00183360 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:26:27,834 DEBUG TRAIN Batch 1/5700 loss 34.958382 loss_att 30.233639 loss_ctc 45.982780 lr 0.00184944 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:26:27,835 DEBUG TRAIN Batch 1/5700 loss 34.987328 loss_att 30.916609 loss_ctc 44.485672 lr 0.00184960 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:26:27,836 DEBUG TRAIN Batch 1/5700 loss 35.248486 loss_att 31.464703 loss_ctc 44.077312 lr 0.00184960 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:26:27,843 DEBUG TRAIN Batch 1/5700 loss 38.325409 loss_att 34.345669 loss_ctc 47.611477 lr 0.00184944 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:26:27,851 DEBUG TRAIN Batch 1/5700 loss 30.635624 loss_att 27.881805 loss_ctc 37.061199 lr 0.00184960 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:26:27,851 DEBUG TRAIN Batch 1/5700 loss 44.521446 loss_att 39.195618 loss_ctc 56.948376 lr 0.00184944 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:26:27,854 DEBUG TRAIN Batch 1/5700 loss 37.197392 loss_att 32.812912 loss_ctc 47.427841 lr 0.00184960 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:26:27,855 DEBUG TRAIN Batch 1/5700 loss 37.198605 loss_att 32.637436 loss_ctc 47.841331 lr 0.00184944 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:28:56,589 DEBUG TRAIN Batch 1/5800 loss 31.808466 loss_att 28.337055 loss_ctc 39.908428 lr 0.00186560 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:28:56,590 DEBUG TRAIN Batch 1/5800 loss 31.244276 loss_att 27.609409 loss_ctc 39.725632 lr 0.00186560 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:28:56,590 DEBUG TRAIN Batch 1/5800 loss 30.636974 loss_att 26.672062 loss_ctc 39.888435 lr 0.00186544 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:28:56,593 DEBUG TRAIN Batch 1/5800 loss 30.228382 loss_att 25.928556 loss_ctc 40.261303 lr 0.00186544 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:28:56,594 DEBUG TRAIN Batch 1/5800 loss 38.945240 loss_att 33.268723 loss_ctc 52.190453 lr 0.00186544 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:28:56,596 DEBUG TRAIN Batch 1/5800 loss 28.123444 loss_att 23.909142 loss_ctc 37.956818 lr 0.00186560 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:28:56,596 DEBUG TRAIN Batch 1/5800 loss 40.486919 loss_att 35.363533 loss_ctc 52.441490 lr 0.00186544 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:28:56,609 DEBUG TRAIN Batch 1/5800 loss 38.146183 loss_att 33.732086 loss_ctc 48.445747 lr 0.00186560 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:29:52,526 DEBUG CV Batch 1/0 loss 5.939364 loss_att 5.040916 loss_ctc 8.035742 history loss 5.482490 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:29:52,529 DEBUG CV Batch 1/0 loss 5.939364 loss_att 5.040916 loss_ctc 8.035742 history loss 5.482490 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:29:52,532 DEBUG CV Batch 1/0 loss 5.939364 loss_att 5.040916 loss_ctc 8.035742 history loss 5.482490 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:29:52,534 DEBUG CV Batch 1/0 loss 5.939364 loss_att 5.040916 loss_ctc 8.035742 history loss 5.482490 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:29:52,535 DEBUG CV Batch 1/0 loss 5.939364 loss_att 5.040916 loss_ctc 8.035742 history loss 5.482490 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:29:52,535 DEBUG CV Batch 1/0 loss 5.939364 loss_att 5.040916 loss_ctc 8.035742 history loss 5.482490 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:29:52,536 DEBUG CV Batch 1/0 loss 5.939364 loss_att 5.040916 loss_ctc 8.035742 history loss 5.482490 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:29:52,540 DEBUG CV Batch 1/0 loss 5.939364 loss_att 5.040916 loss_ctc 8.035742 history loss 5.482490 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:31:09,805 DEBUG CV Batch 1/100 loss 7.680908 loss_att 6.499129 loss_ctc 10.438391 history loss 10.203332 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:31:09,819 DEBUG CV Batch 1/100 loss 7.680908 loss_att 6.499129 loss_ctc 10.438391 history loss 10.203332 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:31:09,824 DEBUG CV Batch 1/100 loss 7.680908 loss_att 6.499129 loss_ctc 10.438391 history loss 10.203332 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:31:09,841 DEBUG CV Batch 1/100 loss 7.680908 loss_att 6.499129 loss_ctc 10.438391 history loss 10.203332 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:31:09,843 DEBUG CV Batch 1/100 loss 7.680908 loss_att 6.499129 loss_ctc 10.438391 history loss 10.203332 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:31:09,848 DEBUG CV Batch 1/100 loss 7.680908 loss_att 6.499129 loss_ctc 10.438391 history loss 10.203332 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:31:09,876 DEBUG CV Batch 1/100 loss 7.680908 loss_att 6.499129 loss_ctc 10.438391 history loss 10.203332 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:31:09,903 DEBUG CV Batch 1/100 loss 7.680908 loss_att 6.499129 loss_ctc 10.438391 history loss 10.203332 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:32:25,614 DEBUG CV Batch 1/200 loss 18.644094 loss_att 16.731163 loss_ctc 23.107601 history loss 10.817689 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:32:25,635 DEBUG CV Batch 1/200 loss 18.644094 loss_att 16.731163 loss_ctc 23.107601 history loss 10.817689 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:32:25,648 DEBUG CV Batch 1/200 loss 18.644094 loss_att 16.731163 loss_ctc 23.107601 history loss 10.817689 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:32:25,649 DEBUG CV Batch 1/200 loss 18.644094 loss_att 16.731163 loss_ctc 23.107601 history loss 10.817689 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:32:25,663 DEBUG CV Batch 1/200 loss 18.644094 loss_att 16.731163 loss_ctc 23.107601 history loss 10.817689 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:32:25,681 DEBUG CV Batch 1/200 loss 18.644094 loss_att 16.731163 loss_ctc 23.107601 history loss 10.817689 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:32:25,682 DEBUG CV Batch 1/200 loss 18.644094 loss_att 16.731163 loss_ctc 23.107601 history loss 10.817689 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:32:25,692 DEBUG CV Batch 1/200 loss 18.644094 loss_att 16.731163 loss_ctc 23.107601 history loss 10.817689 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:34:12,871 DEBUG CV Batch 1/300 loss 6.649505 loss_att 5.320941 loss_ctc 9.749486 history loss 13.302638 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:34:12,879 DEBUG CV Batch 1/300 loss 6.649505 loss_att 5.320941 loss_ctc 9.749486 history loss 13.302638 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:34:12,880 DEBUG CV Batch 1/300 loss 6.649505 loss_att 5.320941 loss_ctc 9.749486 history loss 13.302638 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:34:12,881 DEBUG CV Batch 1/300 loss 6.649505 loss_att 5.320941 loss_ctc 9.749486 history loss 13.302638 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:34:12,884 DEBUG CV Batch 1/300 loss 6.649505 loss_att 5.320941 loss_ctc 9.749486 history loss 13.302638 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:34:12,887 DEBUG CV Batch 1/300 loss 6.649505 loss_att 5.320941 loss_ctc 9.749486 history loss 13.302638 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:34:12,890 DEBUG CV Batch 1/300 loss 6.649505 loss_att 5.320941 loss_ctc 9.749486 history loss 13.302638 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:34:12,933 DEBUG CV Batch 1/300 loss 6.649505 loss_att 5.320941 loss_ctc 9.749486 history loss 13.302638 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:35:24,710 DEBUG CV Batch 1/400 loss 24.381451 loss_att 19.804350 loss_ctc 35.061348 history loss 14.569593 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:35:24,714 DEBUG CV Batch 1/400 loss 24.381451 loss_att 19.804350 loss_ctc 35.061348 history loss 14.569593 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:35:24,754 DEBUG CV Batch 1/400 loss 24.381451 loss_att 19.804350 loss_ctc 35.061348 history loss 14.569593 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:35:24,765 DEBUG CV Batch 1/400 loss 24.381451 loss_att 19.804350 loss_ctc 35.061348 history loss 14.569593 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:35:24,774 DEBUG CV Batch 1/400 loss 24.381451 loss_att 19.804350 loss_ctc 35.061348 history loss 14.569593 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:35:24,781 DEBUG CV Batch 1/400 loss 24.381451 loss_att 19.804350 loss_ctc 35.061348 history loss 14.569593 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:35:24,788 DEBUG CV Batch 1/400 loss 24.381451 loss_att 19.804350 loss_ctc 35.061348 history loss 14.569593 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:35:24,801 DEBUG CV Batch 1/400 loss 24.381451 loss_att 19.804350 loss_ctc 35.061348 history loss 14.569593 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:37:09,075 DEBUG CV Batch 1/500 loss 4.779285 loss_att 3.573652 loss_ctc 7.592427 history loss 14.976109 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:37:09,077 DEBUG CV Batch 1/500 loss 4.779285 loss_att 3.573652 loss_ctc 7.592427 history loss 14.976109 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:37:09,081 DEBUG CV Batch 1/500 loss 4.779285 loss_att 3.573652 loss_ctc 7.592427 history loss 14.976109 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:37:09,082 DEBUG CV Batch 1/500 loss 4.779285 loss_att 3.573652 loss_ctc 7.592427 history loss 14.976109 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:37:09,086 DEBUG CV Batch 1/500 loss 4.779285 loss_att 3.573652 loss_ctc 7.592427 history loss 14.976109 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:37:09,089 DEBUG CV Batch 1/500 loss 4.779285 loss_att 3.573652 loss_ctc 7.592427 history loss 14.976109 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:37:09,096 DEBUG CV Batch 1/500 loss 4.779285 loss_att 3.573652 loss_ctc 7.592427 history loss 14.976109 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:37:09,098 DEBUG CV Batch 1/500 loss 4.779285 loss_att 3.573652 loss_ctc 7.592427 history loss 14.976109 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:38:19,936 DEBUG CV Batch 1/600 loss 11.140608 loss_att 9.912762 loss_ctc 14.005584 history loss 14.150780 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:38:19,945 DEBUG CV Batch 1/600 loss 11.140608 loss_att 9.912762 loss_ctc 14.005584 history loss 14.150780 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:38:19,950 DEBUG CV Batch 1/600 loss 11.140608 loss_att 9.912762 loss_ctc 14.005584 history loss 14.150780 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:38:19,963 DEBUG CV Batch 1/600 loss 11.140608 loss_att 9.912762 loss_ctc 14.005584 history loss 14.150780 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:38:19,975 DEBUG CV Batch 1/600 loss 11.140608 loss_att 9.912762 loss_ctc 14.005584 history loss 14.150780 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:38:20,004 DEBUG CV Batch 1/600 loss 11.140608 loss_att 9.912762 loss_ctc 14.005584 history loss 14.150780 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:38:20,011 DEBUG CV Batch 1/600 loss 11.140608 loss_att 9.912762 loss_ctc 14.005584 history loss 14.150780 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:38:20,052 DEBUG CV Batch 1/600 loss 11.140608 loss_att 9.912762 loss_ctc 14.005584 history loss 14.150780 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:39:29,000 DEBUG CV Batch 1/700 loss 20.920900 loss_att 18.039122 loss_ctc 27.645050 history loss 14.045100 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:39:29,004 DEBUG CV Batch 1/700 loss 20.920900 loss_att 18.039122 loss_ctc 27.645050 history loss 14.045100 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:39:29,024 DEBUG CV Batch 1/700 loss 20.920900 loss_att 18.039122 loss_ctc 27.645050 history loss 14.045100 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:39:29,025 DEBUG CV Batch 1/700 loss 20.920900 loss_att 18.039122 loss_ctc 27.645050 history loss 14.045100 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:39:29,027 DEBUG CV Batch 1/700 loss 20.920900 loss_att 18.039122 loss_ctc 27.645050 history loss 14.045100 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:39:29,029 DEBUG CV Batch 1/700 loss 20.920900 loss_att 18.039122 loss_ctc 27.645050 history loss 14.045100 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:39:29,037 DEBUG CV Batch 1/700 loss 20.920900 loss_att 18.039122 loss_ctc 27.645050 history loss 14.045100 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:39:29,050 DEBUG CV Batch 1/700 loss 20.920900 loss_att 18.039122 loss_ctc 27.645050 history loss 14.045100 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:41:10,319 DEBUG CV Batch 1/800 loss 10.218553 loss_att 8.856895 loss_ctc 13.395752 history loss 14.620690 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:41:10,323 DEBUG CV Batch 1/800 loss 10.218553 loss_att 8.856895 loss_ctc 13.395752 history loss 14.620690 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:41:10,324 DEBUG CV Batch 1/800 loss 10.218553 loss_att 8.856895 loss_ctc 13.395752 history loss 14.620690 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:41:10,336 DEBUG CV Batch 1/800 loss 10.218553 loss_att 8.856895 loss_ctc 13.395752 history loss 14.620690 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:41:10,346 DEBUG CV Batch 1/800 loss 10.218553 loss_att 8.856895 loss_ctc 13.395752 history loss 14.620690 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:41:10,349 DEBUG CV Batch 1/800 loss 10.218553 loss_att 8.856895 loss_ctc 13.395752 history loss 14.620690 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:41:10,353 DEBUG CV Batch 1/800 loss 10.218553 loss_att 8.856895 loss_ctc 13.395752 history loss 14.620690 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:41:10,354 DEBUG CV Batch 1/800 loss 10.218553 loss_att 8.856895 loss_ctc 13.395752 history loss 14.620690 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:09,681 DEBUG CV Batch 1/900 loss 21.966124 loss_att 18.927422 loss_ctc 29.056427 history loss 15.132375 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:09,683 DEBUG CV Batch 1/900 loss 21.966124 loss_att 18.927422 loss_ctc 29.056427 history loss 15.132375 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:09,688 DEBUG CV Batch 1/900 loss 21.966124 loss_att 18.927422 loss_ctc 29.056427 history loss 15.132375 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:09,709 DEBUG CV Batch 1/900 loss 21.966124 loss_att 18.927422 loss_ctc 29.056427 history loss 15.132375 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:09,757 DEBUG CV Batch 1/900 loss 21.966124 loss_att 18.927422 loss_ctc 29.056427 history loss 15.132375 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:09,799 DEBUG CV Batch 1/900 loss 21.966124 loss_att 18.927422 loss_ctc 29.056427 history loss 15.132375 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:09,825 DEBUG CV Batch 1/900 loss 21.966124 loss_att 18.927422 loss_ctc 29.056427 history loss 15.132375 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:09,925 DEBUG CV Batch 1/900 loss 21.966124 loss_att 18.927422 loss_ctc 29.056427 history loss 15.132375 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,370 INFO Epoch 1 CV info cv_loss 15.298559207774924\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,371 INFO Epoch 2 TRAIN info lr 0.0018747199999999999\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,373 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,374 INFO Epoch 1 CV info cv_loss 15.298559207774924\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,375 INFO Checkpoint: save to checkpoint /opt/ml/model/1.pt\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,375 INFO Epoch 1 CV info cv_loss 15.298559207774924\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,375 INFO Epoch 2 TRAIN info lr 0.0018747199999999999\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,378 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,400 INFO Epoch 1 CV info cv_loss 15.298559207774924\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,401 INFO Epoch 2 TRAIN info lr 0.0018747199999999999\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,403 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,459 INFO Epoch 1 CV info cv_loss 15.298559207774924\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,459 INFO Epoch 2 TRAIN info lr 0.00187488\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,461 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,501 INFO Epoch 1 CV info cv_loss 15.298559207774924\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,502 INFO Epoch 2 TRAIN info lr 0.00187456\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,504 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,537 INFO Epoch 1 CV info cv_loss 15.298559207774924\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,538 INFO Epoch 2 TRAIN info lr 0.00187488\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,540 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,640 INFO Epoch 1 CV info cv_loss 15.298559207774924\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,641 INFO Epoch 2 TRAIN info lr 0.0018747199999999999\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,644 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,725 INFO Epoch 2 TRAIN info lr 0.00187488\u001b[0m\n",
      "\u001b[34m2022-11-28 08:42:10,729 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:01,851 DEBUG TRAIN Batch 2/0 loss 15.314997 loss_att 13.887053 loss_ctc 18.646862 lr 0.00187488 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:01,856 DEBUG TRAIN Batch 2/0 loss 8.135610 loss_att 7.507261 loss_ctc 9.601757 lr 0.00187488 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:01,859 DEBUG TRAIN Batch 2/0 loss 11.120716 loss_att 9.434166 loss_ctc 15.055998 lr 0.00187472 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:01,859 DEBUG TRAIN Batch 2/0 loss 12.454308 loss_att 10.703493 loss_ctc 16.539541 lr 0.00187488 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:01,860 DEBUG TRAIN Batch 2/0 loss 14.083839 loss_att 12.388052 loss_ctc 18.040676 lr 0.00187504 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:01,864 DEBUG TRAIN Batch 2/0 loss 16.991892 loss_att 14.931224 loss_ctc 21.800116 lr 0.00187504 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:01,880 DEBUG TRAIN Batch 2/0 loss 18.839125 loss_att 16.325741 loss_ctc 24.703688 lr 0.00187504 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:01,899 DEBUG TRAIN Batch 2/0 loss 13.026196 loss_att 11.134405 loss_ctc 17.440376 lr 0.00187488 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:34,855 DEBUG TRAIN Batch 2/100 loss 32.059761 loss_att 29.170670 loss_ctc 38.800980 lr 0.00189088 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:34,859 DEBUG TRAIN Batch 2/100 loss 28.254873 loss_att 24.712875 loss_ctc 36.519531 lr 0.00189088 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:34,862 DEBUG TRAIN Batch 2/100 loss 29.916290 loss_att 26.324978 loss_ctc 38.296017 lr 0.00189072 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:34,863 DEBUG TRAIN Batch 2/100 loss 31.366295 loss_att 27.896358 loss_ctc 39.462811 lr 0.00189088 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:34,863 DEBUG TRAIN Batch 2/100 loss 33.223579 loss_att 29.278133 loss_ctc 42.429611 lr 0.00189088 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:34,867 DEBUG TRAIN Batch 2/100 loss 43.691463 loss_att 36.740734 loss_ctc 59.909832 lr 0.00189104 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:34,870 DEBUG TRAIN Batch 2/100 loss 35.531837 loss_att 30.931129 loss_ctc 46.266823 lr 0.00189104 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:45:34,874 DEBUG TRAIN Batch 2/100 loss 31.776175 loss_att 28.055859 loss_ctc 40.456909 lr 0.00189104 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:48:25,241 DEBUG TRAIN Batch 2/200 loss 36.691280 loss_att 32.109707 loss_ctc 47.381615 lr 0.00190688 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:48:25,245 DEBUG TRAIN Batch 2/200 loss 35.303764 loss_att 31.453564 loss_ctc 44.287560 lr 0.00190688 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:48:25,244 DEBUG TRAIN Batch 2/200 loss 47.046066 loss_att 41.205273 loss_ctc 60.674591 lr 0.00190704 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:48:25,245 DEBUG TRAIN Batch 2/200 loss 34.545349 loss_att 30.461891 loss_ctc 44.073418 lr 0.00190672 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:48:25,252 DEBUG TRAIN Batch 2/200 loss 31.586964 loss_att 28.604046 loss_ctc 38.547108 lr 0.00190688 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:48:25,253 DEBUG TRAIN Batch 2/200 loss 33.786152 loss_att 29.994064 loss_ctc 42.634354 lr 0.00190688 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:48:25,256 DEBUG TRAIN Batch 2/200 loss 37.447044 loss_att 33.129391 loss_ctc 47.521568 lr 0.00190704 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:48:25,257 DEBUG TRAIN Batch 2/200 loss 38.278488 loss_att 33.365974 loss_ctc 49.741020 lr 0.00190704 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:51:14,646 DEBUG TRAIN Batch 2/300 loss 38.937866 loss_att 33.241879 loss_ctc 52.228512 lr 0.00192288 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:51:14,650 DEBUG TRAIN Batch 2/300 loss 30.990940 loss_att 26.939383 loss_ctc 40.444572 lr 0.00192304 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:51:14,651 DEBUG TRAIN Batch 2/300 loss 35.379585 loss_att 29.898626 loss_ctc 48.168484 lr 0.00192272 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:51:14,653 DEBUG TRAIN Batch 2/300 loss 37.043884 loss_att 32.758194 loss_ctc 47.043835 lr 0.00192288 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:51:14,653 DEBUG TRAIN Batch 2/300 loss 22.792635 loss_att 19.558186 loss_ctc 30.339682 lr 0.00192288 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:51:14,655 DEBUG TRAIN Batch 2/300 loss 27.857876 loss_att 24.507267 loss_ctc 35.675961 lr 0.00192288 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:51:14,669 DEBUG TRAIN Batch 2/300 loss 30.717613 loss_att 28.376572 loss_ctc 36.180046 lr 0.00192304 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:51:14,687 DEBUG TRAIN Batch 2/300 loss 47.630947 loss_att 41.558098 loss_ctc 61.800926 lr 0.00192304 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:54:02,171 DEBUG TRAIN Batch 2/400 loss 40.797619 loss_att 36.138947 loss_ctc 51.667858 lr 0.00193888 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:54:02,174 DEBUG TRAIN Batch 2/400 loss 33.286991 loss_att 29.118557 loss_ctc 43.013336 lr 0.00193904 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:54:02,174 DEBUG TRAIN Batch 2/400 loss 31.877153 loss_att 29.134512 loss_ctc 38.276653 lr 0.00193888 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:54:02,175 DEBUG TRAIN Batch 2/400 loss 43.502335 loss_att 38.090984 loss_ctc 56.128815 lr 0.00193888 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:54:02,178 DEBUG TRAIN Batch 2/400 loss 41.183182 loss_att 36.923172 loss_ctc 51.123199 lr 0.00193872 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:54:02,187 DEBUG TRAIN Batch 2/400 loss 30.347860 loss_att 26.746227 loss_ctc 38.751671 lr 0.00193904 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:54:02,188 DEBUG TRAIN Batch 2/400 loss 34.460476 loss_att 30.857849 loss_ctc 42.866608 lr 0.00193904 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:54:02,190 DEBUG TRAIN Batch 2/400 loss 30.874336 loss_att 27.265648 loss_ctc 39.294609 lr 0.00193888 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:56:49,708 DEBUG TRAIN Batch 2/500 loss 12.169599 loss_att 11.030426 loss_ctc 14.827667 lr 0.00195488 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:56:49,710 DEBUG TRAIN Batch 2/500 loss 21.987995 loss_att 18.628414 loss_ctc 29.827023 lr 0.00195504 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:56:49,714 DEBUG TRAIN Batch 2/500 loss 12.057602 loss_att 11.131857 loss_ctc 14.217672 lr 0.00195472 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:56:49,718 DEBUG TRAIN Batch 2/500 loss 17.683100 loss_att 15.276718 loss_ctc 23.297989 lr 0.00195488 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:56:49,722 DEBUG TRAIN Batch 2/500 loss 14.473196 loss_att 12.761863 loss_ctc 18.466309 lr 0.00195504 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:56:49,740 DEBUG TRAIN Batch 2/500 loss 13.500072 loss_att 11.946161 loss_ctc 17.125866 lr 0.00195504 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 08:56:49,744 DEBUG TRAIN Batch 2/500 loss 14.897219 loss_att 12.513923 loss_ctc 20.458242 lr 0.00195488 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:56:49,815 DEBUG TRAIN Batch 2/500 loss 13.449705 loss_att 11.542786 loss_ctc 17.899185 lr 0.00195488 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:57:22,868 DEBUG TRAIN Batch 2/600 loss 33.558990 loss_att 29.913717 loss_ctc 42.064632 lr 0.00197088 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 08:57:22,868 DEBUG TRAIN Batch 2/600 loss 36.400894 loss_att 31.152031 loss_ctc 48.648235 lr 0.00197088 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 08:57:22,871 DEBUG TRAIN Batch 2/600 loss 29.591372 loss_att 26.305752 loss_ctc 37.257816 lr 0.00197104 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 08:57:22,876 DEBUG TRAIN Batch 2/600 loss 29.701340 loss_att 26.342474 loss_ctc 37.538696 lr 0.00197072 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 08:57:22,879 DEBUG TRAIN Batch 2/600 loss 39.928764 loss_att 33.900524 loss_ctc 53.994652 lr 0.00197088 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 08:57:22,885 DEBUG TRAIN Batch 2/600 loss 41.535187 loss_att 36.481903 loss_ctc 53.326183 lr 0.00197104 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 08:57:22,888 DEBUG TRAIN Batch 2/600 loss 37.216599 loss_att 34.412170 loss_ctc 43.760265 lr 0.00197088 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 08:57:22,894 DEBUG TRAIN Batch 2/600 loss 29.900286 loss_att 26.581993 loss_ctc 37.642967 lr 0.00197104 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:00:07,290 DEBUG TRAIN Batch 2/700 loss 28.711493 loss_att 24.946209 loss_ctc 37.497158 lr 0.00198688 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:00:07,294 DEBUG TRAIN Batch 2/700 loss 23.620594 loss_att 20.632788 loss_ctc 30.592140 lr 0.00198688 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:00:07,294 DEBUG TRAIN Batch 2/700 loss 38.149067 loss_att 33.619713 loss_ctc 48.717556 lr 0.00198672 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:00:07,300 DEBUG TRAIN Batch 2/700 loss 28.051960 loss_att 25.404297 loss_ctc 34.229839 lr 0.00198688 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:00:07,306 DEBUG TRAIN Batch 2/700 loss 31.680910 loss_att 27.505718 loss_ctc 41.423027 lr 0.00198704 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:00:07,322 DEBUG TRAIN Batch 2/700 loss 31.943142 loss_att 28.362278 loss_ctc 40.298492 lr 0.00198704 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:00:07,325 DEBUG TRAIN Batch 2/700 loss 32.616089 loss_att 28.296959 loss_ctc 42.694054 lr 0.00198688 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:00:07,326 DEBUG TRAIN Batch 2/700 loss 35.450989 loss_att 30.913900 loss_ctc 46.037529 lr 0.00198704 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:02:59,710 DEBUG TRAIN Batch 2/800 loss 24.582800 loss_att 21.421545 loss_ctc 31.959061 lr 0.00200288 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:02:59,713 DEBUG TRAIN Batch 2/800 loss 27.393845 loss_att 23.093075 loss_ctc 37.428974 lr 0.00200272 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:02:59,713 DEBUG TRAIN Batch 2/800 loss 29.312950 loss_att 24.760374 loss_ctc 39.935623 lr 0.00200288 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:02:59,713 DEBUG TRAIN Batch 2/800 loss 30.247206 loss_att 27.062897 loss_ctc 37.677261 lr 0.00200288 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:02:59,721 DEBUG TRAIN Batch 2/800 loss 26.260086 loss_att 23.468002 loss_ctc 32.774948 lr 0.00200304 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:02:59,721 DEBUG TRAIN Batch 2/800 loss 29.593956 loss_att 25.147787 loss_ctc 39.968353 lr 0.00200304 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:02:59,723 DEBUG TRAIN Batch 2/800 loss 29.885220 loss_att 26.199989 loss_ctc 38.484085 lr 0.00200288 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:02:59,735 DEBUG TRAIN Batch 2/800 loss 21.046043 loss_att 19.285027 loss_ctc 25.155081 lr 0.00200304 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:05:49,706 DEBUG TRAIN Batch 2/900 loss 38.055534 loss_att 34.242714 loss_ctc 46.952110 lr 0.00201888 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:05:49,707 DEBUG TRAIN Batch 2/900 loss 34.654263 loss_att 30.378790 loss_ctc 44.630371 lr 0.00201872 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:05:49,707 DEBUG TRAIN Batch 2/900 loss 35.289055 loss_att 30.283199 loss_ctc 46.969383 lr 0.00201888 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:05:49,709 DEBUG TRAIN Batch 2/900 loss 32.826664 loss_att 29.210102 loss_ctc 41.265305 lr 0.00201888 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:05:49,713 DEBUG TRAIN Batch 2/900 loss 20.527233 loss_att 17.665270 loss_ctc 27.205153 lr 0.00201888 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:05:49,715 DEBUG TRAIN Batch 2/900 loss 35.483658 loss_att 30.023840 loss_ctc 48.223232 lr 0.00201904 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:05:49,726 DEBUG TRAIN Batch 2/900 loss 32.891994 loss_att 29.438980 loss_ctc 40.949028 lr 0.00201904 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:05:49,740 DEBUG TRAIN Batch 2/900 loss 32.761055 loss_att 29.471258 loss_ctc 40.437241 lr 0.00201904 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:08:43,040 DEBUG TRAIN Batch 2/1000 loss 16.429892 loss_att 13.979267 loss_ctc 22.148014 lr 0.00203488 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:08:43,042 DEBUG TRAIN Batch 2/1000 loss 11.535755 loss_att 10.252991 loss_ctc 14.528870 lr 0.00203488 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:08:43,043 DEBUG TRAIN Batch 2/1000 loss 13.284981 loss_att 11.925804 loss_ctc 16.456392 lr 0.00203504 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:08:43,046 DEBUG TRAIN Batch 2/1000 loss 26.615242 loss_att 22.599060 loss_ctc 35.986336 lr 0.00203488 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:08:43,048 DEBUG TRAIN Batch 2/1000 loss 14.531614 loss_att 12.879389 loss_ctc 18.386810 lr 0.00203488 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:08:43,048 DEBUG TRAIN Batch 2/1000 loss 15.420906 loss_att 13.266874 loss_ctc 20.446981 lr 0.00203472 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:08:43,049 DEBUG TRAIN Batch 2/1000 loss 12.017130 loss_att 10.940384 loss_ctc 14.529535 lr 0.00203504 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:08:43,109 DEBUG TRAIN Batch 2/1000 loss 18.300255 loss_att 16.496315 loss_ctc 22.509449 lr 0.00203504 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:09:16,133 DEBUG TRAIN Batch 2/1100 loss 32.154755 loss_att 28.013367 loss_ctc 41.817989 lr 0.00205072 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:09:16,138 DEBUG TRAIN Batch 2/1100 loss 35.712029 loss_att 31.625051 loss_ctc 45.248314 lr 0.00205088 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:09:16,141 DEBUG TRAIN Batch 2/1100 loss 37.340305 loss_att 32.632652 loss_ctc 48.324829 lr 0.00205104 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:09:16,142 DEBUG TRAIN Batch 2/1100 loss 40.613461 loss_att 36.388271 loss_ctc 50.472237 lr 0.00205088 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:09:16,142 DEBUG TRAIN Batch 2/1100 loss 33.252113 loss_att 29.434261 loss_ctc 42.160435 lr 0.00205088 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:09:16,143 DEBUG TRAIN Batch 2/1100 loss 25.673988 loss_att 22.523321 loss_ctc 33.025543 lr 0.00205104 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:09:16,146 DEBUG TRAIN Batch 2/1100 loss 40.169685 loss_att 35.104275 loss_ctc 51.988976 lr 0.00205088 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:09:16,177 DEBUG TRAIN Batch 2/1100 loss 36.563862 loss_att 33.113075 loss_ctc 44.615704 lr 0.00205104 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:12:08,176 DEBUG TRAIN Batch 2/1200 loss 34.119656 loss_att 29.362564 loss_ctc 45.219536 lr 0.00206688 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:12:08,179 DEBUG TRAIN Batch 2/1200 loss 36.657722 loss_att 32.610291 loss_ctc 46.101730 lr 0.00206672 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:12:08,179 DEBUG TRAIN Batch 2/1200 loss 26.738434 loss_att 24.002071 loss_ctc 33.123283 lr 0.00206688 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:12:08,182 DEBUG TRAIN Batch 2/1200 loss 32.306046 loss_att 27.646393 loss_ctc 43.178570 lr 0.00206704 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:12:08,184 DEBUG TRAIN Batch 2/1200 loss 27.259586 loss_att 24.769463 loss_ctc 33.069878 lr 0.00206704 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:12:08,186 DEBUG TRAIN Batch 2/1200 loss 39.011921 loss_att 35.151138 loss_ctc 48.020409 lr 0.00206688 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:12:08,192 DEBUG TRAIN Batch 2/1200 loss 24.310223 loss_att 21.882925 loss_ctc 29.973915 lr 0.00206704 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:12:08,207 DEBUG TRAIN Batch 2/1200 loss 39.822311 loss_att 35.227917 loss_ctc 50.542564 lr 0.00206688 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:15:00,303 DEBUG TRAIN Batch 2/1300 loss 26.516045 loss_att 22.533249 loss_ctc 35.809231 lr 0.00208304 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:15:00,304 DEBUG TRAIN Batch 2/1300 loss 33.921066 loss_att 30.982101 loss_ctc 40.778648 lr 0.00208288 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:15:00,305 DEBUG TRAIN Batch 2/1300 loss 28.641895 loss_att 23.616928 loss_ctc 40.366821 lr 0.00208288 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:15:00,307 DEBUG TRAIN Batch 2/1300 loss 31.857328 loss_att 27.487453 loss_ctc 42.053703 lr 0.00208304 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:15:00,309 DEBUG TRAIN Batch 2/1300 loss 29.880306 loss_att 27.420536 loss_ctc 35.619774 lr 0.00208272 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:15:00,309 DEBUG TRAIN Batch 2/1300 loss 28.760143 loss_att 25.955322 loss_ctc 35.304726 lr 0.00208288 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:15:00,311 DEBUG TRAIN Batch 2/1300 loss 30.466110 loss_att 26.486639 loss_ctc 39.751537 lr 0.00208288 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:15:00,328 DEBUG TRAIN Batch 2/1300 loss 31.509731 loss_att 28.177490 loss_ctc 39.284958 lr 0.00208304 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:17:53,356 DEBUG TRAIN Batch 2/1400 loss 30.428288 loss_att 26.023026 loss_ctc 40.707226 lr 0.00209888 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:17:53,358 DEBUG TRAIN Batch 2/1400 loss 28.011177 loss_att 25.356781 loss_ctc 34.204765 lr 0.00209872 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:17:53,360 DEBUG TRAIN Batch 2/1400 loss 38.056244 loss_att 34.678272 loss_ctc 45.938171 lr 0.00209888 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:17:53,363 DEBUG TRAIN Batch 2/1400 loss 38.120155 loss_att 33.478249 loss_ctc 48.951263 lr 0.00209888 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:17:53,368 DEBUG TRAIN Batch 2/1400 loss 29.275055 loss_att 25.282034 loss_ctc 38.592102 lr 0.00209904 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:17:53,369 DEBUG TRAIN Batch 2/1400 loss 42.324089 loss_att 37.479103 loss_ctc 53.629051 lr 0.00209888 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:17:53,372 DEBUG TRAIN Batch 2/1400 loss 43.414829 loss_att 39.039879 loss_ctc 53.623043 lr 0.00209904 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:17:53,373 DEBUG TRAIN Batch 2/1400 loss 50.429008 loss_att 45.815441 loss_ctc 61.194000 lr 0.00209904 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:20:45,089 DEBUG TRAIN Batch 2/1500 loss 15.724847 loss_att 14.394638 loss_ctc 18.828669 lr 0.00211488 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:20:45,092 DEBUG TRAIN Batch 2/1500 loss 15.987589 loss_att 13.679062 loss_ctc 21.374149 lr 0.00211504 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:20:45,093 DEBUG TRAIN Batch 2/1500 loss 10.453116 loss_att 8.645026 loss_ctc 14.671992 lr 0.00211488 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:20:45,116 DEBUG TRAIN Batch 2/1500 loss 19.566530 loss_att 17.286106 loss_ctc 24.887518 lr 0.00211504 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:20:45,117 DEBUG TRAIN Batch 2/1500 loss 11.999356 loss_att 10.505423 loss_ctc 15.485202 lr 0.00211488 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:20:45,120 DEBUG TRAIN Batch 2/1500 loss 13.765836 loss_att 11.993507 loss_ctc 17.901268 lr 0.00211504 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:20:45,171 DEBUG TRAIN Batch 2/1500 loss 11.854803 loss_att 10.190161 loss_ctc 15.738968 lr 0.00211472 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:20:45,194 DEBUG TRAIN Batch 2/1500 loss 14.822819 loss_att 13.404781 loss_ctc 18.131573 lr 0.00211488 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:21:17,929 DEBUG TRAIN Batch 2/1600 loss 40.228195 loss_att 34.839890 loss_ctc 52.800911 lr 0.00213088 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:21:17,930 DEBUG TRAIN Batch 2/1600 loss 23.984608 loss_att 21.396307 loss_ctc 30.023975 lr 0.00213088 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:21:17,932 DEBUG TRAIN Batch 2/1600 loss 29.899996 loss_att 26.849895 loss_ctc 37.016899 lr 0.00213072 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:21:17,933 DEBUG TRAIN Batch 2/1600 loss 31.650311 loss_att 27.188276 loss_ctc 42.061722 lr 0.00213104 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:21:17,934 DEBUG TRAIN Batch 2/1600 loss 30.651043 loss_att 27.931286 loss_ctc 36.997143 lr 0.00213088 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:21:17,946 DEBUG TRAIN Batch 2/1600 loss 34.913357 loss_att 30.574413 loss_ctc 45.037563 lr 0.00213104 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:21:17,946 DEBUG TRAIN Batch 2/1600 loss 49.860214 loss_att 45.208935 loss_ctc 60.713196 lr 0.00213104 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:21:17,952 DEBUG TRAIN Batch 2/1600 loss 33.983681 loss_att 30.781065 loss_ctc 41.456444 lr 0.00213088 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:24:10,396 DEBUG TRAIN Batch 2/1700 loss 33.704056 loss_att 30.504034 loss_ctc 41.170769 lr 0.00214688 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:24:10,397 DEBUG TRAIN Batch 2/1700 loss 35.619270 loss_att 30.679150 loss_ctc 47.146210 lr 0.00214672 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:24:10,402 DEBUG TRAIN Batch 2/1700 loss 42.047523 loss_att 36.532784 loss_ctc 54.915253 lr 0.00214688 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:24:10,404 DEBUG TRAIN Batch 2/1700 loss 27.797518 loss_att 25.731251 loss_ctc 32.618809 lr 0.00214688 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:24:10,405 DEBUG TRAIN Batch 2/1700 loss 26.562698 loss_att 23.245230 loss_ctc 34.303459 lr 0.00214704 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:24:10,406 DEBUG TRAIN Batch 2/1700 loss 40.867558 loss_att 35.805855 loss_ctc 52.678192 lr 0.00214688 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:24:10,408 DEBUG TRAIN Batch 2/1700 loss 36.028694 loss_att 32.050781 loss_ctc 45.310486 lr 0.00214704 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:24:10,420 DEBUG TRAIN Batch 2/1700 loss 27.913193 loss_att 25.604313 loss_ctc 33.300575 lr 0.00214704 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:27:03,748 DEBUG TRAIN Batch 2/1800 loss 30.162151 loss_att 25.927025 loss_ctc 40.044117 lr 0.00216304 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:27:03,748 DEBUG TRAIN Batch 2/1800 loss 24.570374 loss_att 21.784241 loss_ctc 31.071346 lr 0.00216288 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:27:03,753 DEBUG TRAIN Batch 2/1800 loss 27.234283 loss_att 24.202414 loss_ctc 34.308647 lr 0.00216272 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:27:03,756 DEBUG TRAIN Batch 2/1800 loss 25.183681 loss_att 21.903059 loss_ctc 32.838463 lr 0.00216288 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:27:03,756 DEBUG TRAIN Batch 2/1800 loss 25.145245 loss_att 22.077450 loss_ctc 32.303432 lr 0.00216304 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:27:03,768 DEBUG TRAIN Batch 2/1800 loss 30.280167 loss_att 26.230743 loss_ctc 39.728821 lr 0.00216288 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:27:03,771 DEBUG TRAIN Batch 2/1800 loss 31.839252 loss_att 27.429256 loss_ctc 42.129246 lr 0.00216304 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:27:03,779 DEBUG TRAIN Batch 2/1800 loss 37.567150 loss_att 33.395805 loss_ctc 47.300285 lr 0.00216288 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:29:55,009 DEBUG TRAIN Batch 2/1900 loss 45.353432 loss_att 39.783485 loss_ctc 58.349976 lr 0.00217872 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:29:55,015 DEBUG TRAIN Batch 2/1900 loss 32.227570 loss_att 29.160629 loss_ctc 39.383766 lr 0.00217888 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:29:55,017 DEBUG TRAIN Batch 2/1900 loss 34.369701 loss_att 30.178074 loss_ctc 44.150169 lr 0.00217904 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:29:55,017 DEBUG TRAIN Batch 2/1900 loss 37.855492 loss_att 35.812271 loss_ctc 42.623005 lr 0.00217904 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:29:55,022 DEBUG TRAIN Batch 2/1900 loss 42.038067 loss_att 37.464561 loss_ctc 52.709576 lr 0.00217888 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:29:55,023 DEBUG TRAIN Batch 2/1900 loss 38.906933 loss_att 33.763424 loss_ctc 50.908459 lr 0.00217888 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:29:55,028 DEBUG TRAIN Batch 2/1900 loss 35.176510 loss_att 29.672726 loss_ctc 48.018669 lr 0.00217904 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:29:55,050 DEBUG TRAIN Batch 2/1900 loss 37.961540 loss_att 32.794403 loss_ctc 50.018188 lr 0.00217888 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:32:51,383 DEBUG TRAIN Batch 2/2000 loss 16.553438 loss_att 13.989276 loss_ctc 22.536484 lr 0.00219488 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:32:51,384 DEBUG TRAIN Batch 2/2000 loss 15.374689 loss_att 13.500881 loss_ctc 19.746908 lr 0.00219488 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:32:51,386 DEBUG TRAIN Batch 2/2000 loss 15.928239 loss_att 13.174033 loss_ctc 22.354719 lr 0.00219488 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:32:51,388 DEBUG TRAIN Batch 2/2000 loss 15.055850 loss_att 13.766261 loss_ctc 18.064890 lr 0.00219504 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:32:51,406 DEBUG TRAIN Batch 2/2000 loss 11.968014 loss_att 10.357792 loss_ctc 15.725197 lr 0.00219504 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:32:51,410 DEBUG TRAIN Batch 2/2000 loss 11.436381 loss_att 10.645615 loss_ctc 13.281505 lr 0.00219504 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:32:51,415 DEBUG TRAIN Batch 2/2000 loss 10.816298 loss_att 9.238710 loss_ctc 14.497333 lr 0.00219472 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:32:51,462 DEBUG TRAIN Batch 2/2000 loss 16.798475 loss_att 14.803419 loss_ctc 21.453604 lr 0.00219488 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:33:24,026 DEBUG TRAIN Batch 2/2100 loss 25.825657 loss_att 23.216528 loss_ctc 31.913620 lr 0.00221088 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:33:24,028 DEBUG TRAIN Batch 2/2100 loss 28.444580 loss_att 25.804327 loss_ctc 34.605171 lr 0.00221104 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:33:24,029 DEBUG TRAIN Batch 2/2100 loss 23.625694 loss_att 20.252859 loss_ctc 31.495642 lr 0.00221072 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:33:24,035 DEBUG TRAIN Batch 2/2100 loss 24.024162 loss_att 20.536644 loss_ctc 32.161709 lr 0.00221104 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:33:24,035 DEBUG TRAIN Batch 2/2100 loss 24.522150 loss_att 20.702621 loss_ctc 33.434383 lr 0.00221088 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:33:24,044 DEBUG TRAIN Batch 2/2100 loss 32.846699 loss_att 28.800499 loss_ctc 42.287830 lr 0.00221088 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:33:24,048 DEBUG TRAIN Batch 2/2100 loss 32.111794 loss_att 28.214035 loss_ctc 41.206558 lr 0.00221088 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:33:24,055 DEBUG TRAIN Batch 2/2100 loss 24.229973 loss_att 21.662804 loss_ctc 30.220032 lr 0.00221104 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:36:18,493 DEBUG TRAIN Batch 2/2200 loss 32.972137 loss_att 29.180771 loss_ctc 41.818668 lr 0.00222688 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:36:18,493 DEBUG TRAIN Batch 2/2200 loss 27.769531 loss_att 23.785652 loss_ctc 37.065254 lr 0.00222688 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:36:18,494 DEBUG TRAIN Batch 2/2200 loss 32.976978 loss_att 28.695450 loss_ctc 42.967205 lr 0.00222672 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:36:18,495 DEBUG TRAIN Batch 2/2200 loss 29.901070 loss_att 25.944624 loss_ctc 39.132782 lr 0.00222688 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:36:18,498 DEBUG TRAIN Batch 2/2200 loss 34.978554 loss_att 30.733847 loss_ctc 44.882866 lr 0.00222704 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:36:18,502 DEBUG TRAIN Batch 2/2200 loss 29.984917 loss_att 26.786335 loss_ctc 37.448269 lr 0.00222688 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:36:18,508 DEBUG TRAIN Batch 2/2200 loss 28.036875 loss_att 25.223623 loss_ctc 34.601128 lr 0.00222704 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:36:18,510 DEBUG TRAIN Batch 2/2200 loss 26.238426 loss_att 23.423798 loss_ctc 32.805889 lr 0.00222704 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:39:09,593 DEBUG TRAIN Batch 2/2300 loss 24.277184 loss_att 19.963978 loss_ctc 34.341331 lr 0.00224288 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:39:09,596 DEBUG TRAIN Batch 2/2300 loss 26.359505 loss_att 23.692236 loss_ctc 32.583130 lr 0.00224304 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:39:09,600 DEBUG TRAIN Batch 2/2300 loss 28.033855 loss_att 23.907207 loss_ctc 37.662701 lr 0.00224288 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:39:09,601 DEBUG TRAIN Batch 2/2300 loss 29.820116 loss_att 25.510727 loss_ctc 39.875359 lr 0.00224288 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:39:09,615 DEBUG TRAIN Batch 2/2300 loss 27.838631 loss_att 25.013363 loss_ctc 34.430923 lr 0.00224272 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:39:09,616 DEBUG TRAIN Batch 2/2300 loss 21.924650 loss_att 19.431160 loss_ctc 27.742794 lr 0.00224304 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:39:09,620 DEBUG TRAIN Batch 2/2300 loss 35.260414 loss_att 30.811806 loss_ctc 45.640495 lr 0.00224304 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:39:09,631 DEBUG TRAIN Batch 2/2300 loss 31.992378 loss_att 28.264273 loss_ctc 40.691296 lr 0.00224288 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:42:01,845 DEBUG TRAIN Batch 2/2400 loss 32.866093 loss_att 29.679546 loss_ctc 40.301369 lr 0.00225888 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:42:01,848 DEBUG TRAIN Batch 2/2400 loss 26.685589 loss_att 25.093658 loss_ctc 30.400091 lr 0.00225904 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:42:01,849 DEBUG TRAIN Batch 2/2400 loss 34.071178 loss_att 29.108349 loss_ctc 45.651115 lr 0.00225888 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:42:01,852 DEBUG TRAIN Batch 2/2400 loss 47.901207 loss_att 41.780640 loss_ctc 62.182537 lr 0.00225888 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:42:01,854 DEBUG TRAIN Batch 2/2400 loss 29.658669 loss_att 26.368093 loss_ctc 37.336678 lr 0.00225872 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:42:01,861 DEBUG TRAIN Batch 2/2400 loss 33.067001 loss_att 28.722685 loss_ctc 43.203747 lr 0.00225888 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:42:01,867 DEBUG TRAIN Batch 2/2400 loss 32.122009 loss_att 28.947731 loss_ctc 39.528656 lr 0.00225904 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:42:01,878 DEBUG TRAIN Batch 2/2400 loss 32.957150 loss_att 28.657990 loss_ctc 42.988525 lr 0.00225904 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:44:52,194 DEBUG TRAIN Batch 2/2500 loss 15.863985 loss_att 14.301888 loss_ctc 19.508881 lr 0.00227488 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:44:52,194 DEBUG TRAIN Batch 2/2500 loss 9.927906 loss_att 8.919061 loss_ctc 12.281878 lr 0.00227488 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:44:52,198 DEBUG TRAIN Batch 2/2500 loss 11.660830 loss_att 10.043914 loss_ctc 15.433635 lr 0.00227472 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:44:52,198 DEBUG TRAIN Batch 2/2500 loss 13.275834 loss_att 11.702896 loss_ctc 16.946022 lr 0.00227504 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:44:52,205 DEBUG TRAIN Batch 2/2500 loss 15.620794 loss_att 13.246531 loss_ctc 21.160744 lr 0.00227488 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:44:52,207 DEBUG TRAIN Batch 2/2500 loss 13.146374 loss_att 11.317195 loss_ctc 17.414459 lr 0.00227504 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:44:52,217 DEBUG TRAIN Batch 2/2500 loss 10.733221 loss_att 9.488601 loss_ctc 13.637335 lr 0.00227504 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:44:52,273 DEBUG TRAIN Batch 2/2500 loss 14.715144 loss_att 13.283278 loss_ctc 18.056166 lr 0.00227488 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:45:24,853 DEBUG TRAIN Batch 2/2600 loss 38.037144 loss_att 32.432636 loss_ctc 51.114330 lr 0.00229104 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:45:24,855 DEBUG TRAIN Batch 2/2600 loss 23.737930 loss_att 20.350746 loss_ctc 31.641354 lr 0.00229072 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:45:24,856 DEBUG TRAIN Batch 2/2600 loss 31.338097 loss_att 27.112759 loss_ctc 41.197212 lr 0.00229088 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:45:24,856 DEBUG TRAIN Batch 2/2600 loss 29.041916 loss_att 25.493742 loss_ctc 37.320988 lr 0.00229088 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:45:24,857 DEBUG TRAIN Batch 2/2600 loss 28.898987 loss_att 25.282940 loss_ctc 37.336433 lr 0.00229088 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:45:24,863 DEBUG TRAIN Batch 2/2600 loss 29.992088 loss_att 26.310329 loss_ctc 38.582855 lr 0.00229088 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:45:24,866 DEBUG TRAIN Batch 2/2600 loss 41.999374 loss_att 37.312683 loss_ctc 52.934982 lr 0.00229104 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:45:24,869 DEBUG TRAIN Batch 2/2600 loss 39.307884 loss_att 35.349213 loss_ctc 48.544781 lr 0.00229104 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:48:15,668 DEBUG TRAIN Batch 2/2700 loss 30.278057 loss_att 27.040565 loss_ctc 37.832199 lr 0.00230688 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:48:15,669 DEBUG TRAIN Batch 2/2700 loss 31.766823 loss_att 27.339178 loss_ctc 42.097988 lr 0.00230688 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:48:15,674 DEBUG TRAIN Batch 2/2700 loss 27.946404 loss_att 25.293827 loss_ctc 34.135746 lr 0.00230688 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:48:15,676 DEBUG TRAIN Batch 2/2700 loss 26.911072 loss_att 23.315460 loss_ctc 35.300831 lr 0.00230704 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:48:15,677 DEBUG TRAIN Batch 2/2700 loss 26.340384 loss_att 23.907040 loss_ctc 32.018185 lr 0.00230672 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:48:15,678 DEBUG TRAIN Batch 2/2700 loss 33.429146 loss_att 29.188705 loss_ctc 43.323509 lr 0.00230688 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:48:15,685 DEBUG TRAIN Batch 2/2700 loss 27.504044 loss_att 24.031347 loss_ctc 35.607002 lr 0.00230704 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:48:15,687 DEBUG TRAIN Batch 2/2700 loss 28.749012 loss_att 25.118416 loss_ctc 37.220398 lr 0.00230704 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:51:03,523 DEBUG TRAIN Batch 2/2800 loss 22.496323 loss_att 20.085974 loss_ctc 28.120464 lr 0.00232288 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:51:03,524 DEBUG TRAIN Batch 2/2800 loss 28.441992 loss_att 24.370903 loss_ctc 37.941200 lr 0.00232304 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:51:03,525 DEBUG TRAIN Batch 2/2800 loss 27.606894 loss_att 24.409805 loss_ctc 35.066769 lr 0.00232272 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:51:03,528 DEBUG TRAIN Batch 2/2800 loss 23.993914 loss_att 20.946854 loss_ctc 31.103720 lr 0.00232288 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:51:03,530 DEBUG TRAIN Batch 2/2800 loss 26.264219 loss_att 23.479185 loss_ctc 32.762634 lr 0.00232288 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:51:03,536 DEBUG TRAIN Batch 2/2800 loss 29.520441 loss_att 26.247763 loss_ctc 37.156689 lr 0.00232288 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:51:03,543 DEBUG TRAIN Batch 2/2800 loss 30.888012 loss_att 26.012924 loss_ctc 42.263214 lr 0.00232304 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:51:03,570 DEBUG TRAIN Batch 2/2800 loss 26.416782 loss_att 22.698004 loss_ctc 35.093933 lr 0.00232304 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:53:51,090 DEBUG TRAIN Batch 2/2900 loss 21.990940 loss_att 20.170767 loss_ctc 26.238012 lr 0.00233872 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:53:51,091 DEBUG TRAIN Batch 2/2900 loss 33.153507 loss_att 29.163458 loss_ctc 42.463619 lr 0.00233888 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:53:51,094 DEBUG TRAIN Batch 2/2900 loss 28.934147 loss_att 25.754230 loss_ctc 36.353951 lr 0.00233888 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:53:51,094 DEBUG TRAIN Batch 2/2900 loss 33.385456 loss_att 29.154308 loss_ctc 43.258129 lr 0.00233904 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:53:51,097 DEBUG TRAIN Batch 2/2900 loss 30.650852 loss_att 26.861828 loss_ctc 39.491905 lr 0.00233888 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:53:51,098 DEBUG TRAIN Batch 2/2900 loss 31.457169 loss_att 28.040243 loss_ctc 39.429996 lr 0.00233888 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:53:51,108 DEBUG TRAIN Batch 2/2900 loss 37.560776 loss_att 33.591644 loss_ctc 46.822079 lr 0.00233904 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:53:51,116 DEBUG TRAIN Batch 2/2900 loss 27.642580 loss_att 23.084381 loss_ctc 38.278374 lr 0.00233904 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:56:38,464 DEBUG TRAIN Batch 2/3000 loss 9.836178 loss_att 8.294274 loss_ctc 13.433952 lr 0.00235488 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:56:38,466 DEBUG TRAIN Batch 2/3000 loss 14.055101 loss_att 11.880770 loss_ctc 19.128542 lr 0.00235472 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:56:38,468 DEBUG TRAIN Batch 2/3000 loss 11.364155 loss_att 10.395247 loss_ctc 13.624941 lr 0.00235488 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:56:38,473 DEBUG TRAIN Batch 2/3000 loss 15.204619 loss_att 13.388835 loss_ctc 19.441448 lr 0.00235488 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:56:38,475 DEBUG TRAIN Batch 2/3000 loss 17.029350 loss_att 14.442607 loss_ctc 23.065084 lr 0.00235504 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:56:38,482 DEBUG TRAIN Batch 2/3000 loss 18.572897 loss_att 16.193958 loss_ctc 24.123755 lr 0.00235488 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:56:38,489 DEBUG TRAIN Batch 2/3000 loss 12.047761 loss_att 10.528763 loss_ctc 15.592090 lr 0.00235504 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:56:38,558 DEBUG TRAIN Batch 2/3000 loss 9.689953 loss_att 8.559291 loss_ctc 12.328165 lr 0.00235504 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:57:11,577 DEBUG TRAIN Batch 2/3100 loss 38.925568 loss_att 34.346260 loss_ctc 49.610619 lr 0.00237088 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:57:11,580 DEBUG TRAIN Batch 2/3100 loss 26.738111 loss_att 25.056828 loss_ctc 30.661106 lr 0.00237072 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:57:11,582 DEBUG TRAIN Batch 2/3100 loss 29.207689 loss_att 25.230221 loss_ctc 38.488449 lr 0.00237104 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:57:11,586 DEBUG TRAIN Batch 2/3100 loss 36.006248 loss_att 31.344618 loss_ctc 46.883385 lr 0.00237088 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:57:11,587 DEBUG TRAIN Batch 2/3100 loss 33.746826 loss_att 30.675573 loss_ctc 40.913078 lr 0.00237088 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:57:11,588 DEBUG TRAIN Batch 2/3100 loss 28.444595 loss_att 25.142775 loss_ctc 36.148849 lr 0.00237088 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:57:11,599 DEBUG TRAIN Batch 2/3100 loss 25.013742 loss_att 21.547176 loss_ctc 33.102398 lr 0.00237104 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 09:57:11,598 DEBUG TRAIN Batch 2/3100 loss 35.560089 loss_att 31.610275 loss_ctc 44.776321 lr 0.00237104 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:59:55,735 DEBUG TRAIN Batch 2/3200 loss 30.113375 loss_att 26.819115 loss_ctc 37.799980 lr 0.00238704 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 09:59:55,739 DEBUG TRAIN Batch 2/3200 loss 26.566563 loss_att 23.075623 loss_ctc 34.712090 lr 0.00238672 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 09:59:55,740 DEBUG TRAIN Batch 2/3200 loss 30.077593 loss_att 27.032200 loss_ctc 37.183514 lr 0.00238688 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 09:59:55,743 DEBUG TRAIN Batch 2/3200 loss 30.581106 loss_att 26.837914 loss_ctc 39.315224 lr 0.00238688 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 09:59:55,746 DEBUG TRAIN Batch 2/3200 loss 26.144199 loss_att 22.530117 loss_ctc 34.577057 lr 0.00238704 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 09:59:55,747 DEBUG TRAIN Batch 2/3200 loss 32.091755 loss_att 28.813049 loss_ctc 39.742065 lr 0.00238688 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 09:59:55,749 DEBUG TRAIN Batch 2/3200 loss 28.143845 loss_att 25.025112 loss_ctc 35.420891 lr 0.00238688 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 09:59:55,757 DEBUG TRAIN Batch 2/3200 loss 30.366732 loss_att 25.272713 loss_ctc 42.252773 lr 0.00238704 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:02:48,515 DEBUG TRAIN Batch 2/3300 loss 22.910233 loss_att 21.043589 loss_ctc 27.265736 lr 0.00240272 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:02:48,523 DEBUG TRAIN Batch 2/3300 loss 22.150539 loss_att 19.890594 loss_ctc 27.423742 lr 0.00240304 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:02:48,525 DEBUG TRAIN Batch 2/3300 loss 23.513544 loss_att 20.119650 loss_ctc 31.432631 lr 0.00240288 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:02:48,525 DEBUG TRAIN Batch 2/3300 loss 30.180443 loss_att 26.480621 loss_ctc 38.813362 lr 0.00240288 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:02:48,527 DEBUG TRAIN Batch 2/3300 loss 21.995373 loss_att 19.801325 loss_ctc 27.114813 lr 0.00240304 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:02:48,529 DEBUG TRAIN Batch 2/3300 loss 30.462814 loss_att 26.511669 loss_ctc 39.682156 lr 0.00240288 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:02:48,532 DEBUG TRAIN Batch 2/3300 loss 22.850306 loss_att 20.224121 loss_ctc 28.978069 lr 0.00240288 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:02:48,537 DEBUG TRAIN Batch 2/3300 loss 18.763870 loss_att 17.019962 loss_ctc 22.832991 lr 0.00240304 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:05:42,418 DEBUG TRAIN Batch 2/3400 loss 36.689327 loss_att 31.552578 loss_ctc 48.675079 lr 0.00241888 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:05:42,420 DEBUG TRAIN Batch 2/3400 loss 32.707760 loss_att 29.673477 loss_ctc 39.787758 lr 0.00241888 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:05:42,421 DEBUG TRAIN Batch 2/3400 loss 29.183178 loss_att 25.164875 loss_ctc 38.559219 lr 0.00241904 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:05:42,426 DEBUG TRAIN Batch 2/3400 loss 36.979927 loss_att 31.223824 loss_ctc 50.410835 lr 0.00241888 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:05:42,430 DEBUG TRAIN Batch 2/3400 loss 28.809536 loss_att 24.803631 loss_ctc 38.156647 lr 0.00241872 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:05:42,438 DEBUG TRAIN Batch 2/3400 loss 30.273327 loss_att 27.006798 loss_ctc 37.895226 lr 0.00241904 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:05:42,444 DEBUG TRAIN Batch 2/3400 loss 26.920931 loss_att 24.141602 loss_ctc 33.406033 lr 0.00241888 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:05:42,445 DEBUG TRAIN Batch 2/3400 loss 28.046413 loss_att 25.647612 loss_ctc 33.643616 lr 0.00241904 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:08:36,811 DEBUG TRAIN Batch 2/3500 loss 16.562756 loss_att 15.361468 loss_ctc 19.365755 lr 0.00243488 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:08:36,813 DEBUG TRAIN Batch 2/3500 loss 12.598099 loss_att 10.368326 loss_ctc 17.800901 lr 0.00243504 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:08:36,814 DEBUG TRAIN Batch 2/3500 loss 8.478780 loss_att 7.804044 loss_ctc 10.053163 lr 0.00243488 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:08:36,814 DEBUG TRAIN Batch 2/3500 loss 11.272122 loss_att 9.951099 loss_ctc 14.354509 lr 0.00243488 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:08:36,817 DEBUG TRAIN Batch 2/3500 loss 11.642601 loss_att 10.037192 loss_ctc 15.388553 lr 0.00243488 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:08:36,826 DEBUG TRAIN Batch 2/3500 loss 13.772169 loss_att 11.878256 loss_ctc 18.191298 lr 0.00243472 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:08:36,839 DEBUG TRAIN Batch 2/3500 loss 14.026056 loss_att 11.840561 loss_ctc 19.125546 lr 0.00243504 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:08:36,891 DEBUG TRAIN Batch 2/3500 loss 4.917011 loss_att 4.267394 loss_ctc 6.432785 lr 0.00243504 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:09:10,089 DEBUG TRAIN Batch 2/3600 loss 34.722115 loss_att 30.432430 loss_ctc 44.731380 lr 0.00245104 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:09:10,093 DEBUG TRAIN Batch 2/3600 loss 29.322620 loss_att 25.744162 loss_ctc 37.672356 lr 0.00245088 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:09:10,095 DEBUG TRAIN Batch 2/3600 loss 30.152195 loss_att 26.416626 loss_ctc 38.868523 lr 0.00245072 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:09:10,099 DEBUG TRAIN Batch 2/3600 loss 42.358505 loss_att 36.988045 loss_ctc 54.889580 lr 0.00245088 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:09:10,121 DEBUG TRAIN Batch 2/3600 loss 30.023609 loss_att 25.166046 loss_ctc 41.357918 lr 0.00245088 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:09:10,120 DEBUG TRAIN Batch 2/3600 loss 30.170412 loss_att 26.221989 loss_ctc 39.383396 lr 0.00245088 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:09:10,133 DEBUG TRAIN Batch 2/3600 loss 25.846498 loss_att 22.402782 loss_ctc 33.881836 lr 0.00245104 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:09:10,139 DEBUG TRAIN Batch 2/3600 loss 27.841869 loss_att 23.944479 loss_ctc 36.935776 lr 0.00245104 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:12:02,168 DEBUG TRAIN Batch 2/3700 loss 33.881863 loss_att 30.471640 loss_ctc 41.839046 lr 0.00246688 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:12:02,169 DEBUG TRAIN Batch 2/3700 loss 29.267437 loss_att 26.869778 loss_ctc 34.861980 lr 0.00246704 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:12:02,171 DEBUG TRAIN Batch 2/3700 loss 38.401104 loss_att 33.082157 loss_ctc 50.811977 lr 0.00246688 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:12:02,176 DEBUG TRAIN Batch 2/3700 loss 33.007694 loss_att 28.711721 loss_ctc 43.031624 lr 0.00246672 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:12:02,177 DEBUG TRAIN Batch 2/3700 loss 31.132828 loss_att 27.340467 loss_ctc 39.981670 lr 0.00246688 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:12:02,178 DEBUG TRAIN Batch 2/3700 loss 36.108898 loss_att 31.827421 loss_ctc 46.099014 lr 0.00246704 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:12:02,187 DEBUG TRAIN Batch 2/3700 loss 33.748203 loss_att 29.537415 loss_ctc 43.573376 lr 0.00246704 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:12:02,197 DEBUG TRAIN Batch 2/3700 loss 29.378603 loss_att 25.308231 loss_ctc 38.876137 lr 0.00246688 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:14:54,749 DEBUG TRAIN Batch 2/3800 loss 27.120739 loss_att 23.874493 loss_ctc 34.695309 lr 0.00248288 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:14:54,755 DEBUG TRAIN Batch 2/3800 loss 27.746639 loss_att 24.496191 loss_ctc 35.331017 lr 0.00248288 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:14:54,755 DEBUG TRAIN Batch 2/3800 loss 31.319754 loss_att 29.055710 loss_ctc 36.602520 lr 0.00248304 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:14:54,755 DEBUG TRAIN Batch 2/3800 loss 29.204226 loss_att 25.499151 loss_ctc 37.849400 lr 0.00248288 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:14:54,763 DEBUG TRAIN Batch 2/3800 loss 25.065014 loss_att 22.795044 loss_ctc 30.361609 lr 0.00248272 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:14:54,764 DEBUG TRAIN Batch 2/3800 loss 33.237595 loss_att 28.756725 loss_ctc 43.692955 lr 0.00248304 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:14:54,765 DEBUG TRAIN Batch 2/3800 loss 27.519499 loss_att 24.022141 loss_ctc 35.680000 lr 0.00248288 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:14:54,779 DEBUG TRAIN Batch 2/3800 loss 22.943169 loss_att 19.171494 loss_ctc 31.743742 lr 0.00248304 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:17:48,658 DEBUG TRAIN Batch 2/3900 loss 26.324711 loss_att 22.310101 loss_ctc 35.692135 lr 0.00249888 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:17:48,660 DEBUG TRAIN Batch 2/3900 loss 38.987965 loss_att 32.470119 loss_ctc 54.196266 lr 0.00249888 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:17:48,661 DEBUG TRAIN Batch 2/3900 loss 30.301178 loss_att 27.132833 loss_ctc 37.693985 lr 0.00249904 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:17:48,665 DEBUG TRAIN Batch 2/3900 loss 27.623440 loss_att 24.277023 loss_ctc 35.431747 lr 0.00249888 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:17:48,670 DEBUG TRAIN Batch 2/3900 loss 37.941536 loss_att 32.920700 loss_ctc 49.656822 lr 0.00249904 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:17:48,670 DEBUG TRAIN Batch 2/3900 loss 32.118576 loss_att 29.525612 loss_ctc 38.168819 lr 0.00249888 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:17:48,680 DEBUG TRAIN Batch 2/3900 loss 22.760469 loss_att 19.672384 loss_ctc 29.966002 lr 0.00249872 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:17:48,683 DEBUG TRAIN Batch 2/3900 loss 33.225742 loss_att 28.643738 loss_ctc 43.917084 lr 0.00249904 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:20:41,436 DEBUG TRAIN Batch 2/4000 loss 14.492680 loss_att 12.757963 loss_ctc 18.540350 lr 0.00251488 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:20:41,440 DEBUG TRAIN Batch 2/4000 loss 15.297830 loss_att 13.609875 loss_ctc 19.236393 lr 0.00251488 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:20:41,441 DEBUG TRAIN Batch 2/4000 loss 18.433134 loss_att 16.270119 loss_ctc 23.480167 lr 0.00251504 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:20:41,443 DEBUG TRAIN Batch 2/4000 loss 13.170143 loss_att 11.396083 loss_ctc 17.309616 lr 0.00251472 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:20:41,446 DEBUG TRAIN Batch 2/4000 loss 9.067950 loss_att 7.889815 loss_ctc 11.816933 lr 0.00251488 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:20:41,463 DEBUG TRAIN Batch 2/4000 loss 13.158388 loss_att 11.422827 loss_ctc 17.208033 lr 0.00251504 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:20:41,541 DEBUG TRAIN Batch 2/4000 loss 9.624870 loss_att 8.950449 loss_ctc 11.198521 lr 0.00251504 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:20:41,553 DEBUG TRAIN Batch 2/4000 loss 13.850112 loss_att 12.128613 loss_ctc 17.866945 lr 0.00251488 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:21:14,563 DEBUG TRAIN Batch 2/4100 loss 29.229248 loss_att 25.340712 loss_ctc 38.302498 lr 0.00253104 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:21:14,563 DEBUG TRAIN Batch 2/4100 loss 27.107361 loss_att 23.317190 loss_ctc 35.951088 lr 0.00253088 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:21:14,564 DEBUG TRAIN Batch 2/4100 loss 28.620333 loss_att 24.469090 loss_ctc 38.306564 lr 0.00253088 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:21:14,567 DEBUG TRAIN Batch 2/4100 loss 27.805008 loss_att 24.447241 loss_ctc 35.639797 lr 0.00253088 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:21:14,570 DEBUG TRAIN Batch 2/4100 loss 30.059517 loss_att 26.856850 loss_ctc 37.532402 lr 0.00253088 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:21:14,572 DEBUG TRAIN Batch 2/4100 loss 33.599468 loss_att 29.292860 loss_ctc 43.648224 lr 0.00253072 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:21:14,579 DEBUG TRAIN Batch 2/4100 loss 27.523499 loss_att 25.526573 loss_ctc 32.182991 lr 0.00253104 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:21:14,580 DEBUG TRAIN Batch 2/4100 loss 24.720312 loss_att 22.796188 loss_ctc 29.209934 lr 0.00253104 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:24:07,738 DEBUG TRAIN Batch 2/4200 loss 25.258152 loss_att 21.716459 loss_ctc 33.522106 lr 0.00254704 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:24:07,741 DEBUG TRAIN Batch 2/4200 loss 35.422836 loss_att 30.325153 loss_ctc 47.317421 lr 0.00254688 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:24:07,742 DEBUG TRAIN Batch 2/4200 loss 23.040813 loss_att 20.274324 loss_ctc 29.495953 lr 0.00254688 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:24:07,744 DEBUG TRAIN Batch 2/4200 loss 32.593155 loss_att 28.279844 loss_ctc 42.657551 lr 0.00254688 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:24:07,745 DEBUG TRAIN Batch 2/4200 loss 30.785259 loss_att 26.419157 loss_ctc 40.972824 lr 0.00254704 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:24:07,747 DEBUG TRAIN Batch 2/4200 loss 29.301924 loss_att 25.869232 loss_ctc 37.311539 lr 0.00254672 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:24:07,751 DEBUG TRAIN Batch 2/4200 loss 33.513908 loss_att 29.271759 loss_ctc 43.412262 lr 0.00254688 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:24:07,758 DEBUG TRAIN Batch 2/4200 loss 37.937653 loss_att 33.204994 loss_ctc 48.980522 lr 0.00254704 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:26:57,847 DEBUG TRAIN Batch 2/4300 loss 22.284126 loss_att 19.339897 loss_ctc 29.153997 lr 0.00256288 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:26:57,849 DEBUG TRAIN Batch 2/4300 loss 28.214256 loss_att 24.504019 loss_ctc 36.871479 lr 0.00256304 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:26:57,852 DEBUG TRAIN Batch 2/4300 loss 24.899107 loss_att 22.119671 loss_ctc 31.384459 lr 0.00256288 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:26:57,852 DEBUG TRAIN Batch 2/4300 loss 25.047745 loss_att 21.788731 loss_ctc 32.652107 lr 0.00256288 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:26:57,854 DEBUG TRAIN Batch 2/4300 loss 26.030205 loss_att 22.627441 loss_ctc 33.969986 lr 0.00256272 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:26:57,858 DEBUG TRAIN Batch 2/4300 loss 25.156538 loss_att 22.324543 loss_ctc 31.764524 lr 0.00256288 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:26:57,865 DEBUG TRAIN Batch 2/4300 loss 20.524124 loss_att 18.355217 loss_ctc 25.584906 lr 0.00256304 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:26:57,872 DEBUG TRAIN Batch 2/4300 loss 31.025616 loss_att 26.564972 loss_ctc 41.433788 lr 0.00256304 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:29:48,017 DEBUG TRAIN Batch 2/4400 loss 29.251677 loss_att 25.497335 loss_ctc 38.011803 lr 0.00257888 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:29:48,019 DEBUG TRAIN Batch 2/4400 loss 37.966110 loss_att 33.367298 loss_ctc 48.696667 lr 0.00257888 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:29:48,023 DEBUG TRAIN Batch 2/4400 loss 27.101185 loss_att 24.894115 loss_ctc 32.251011 lr 0.00257888 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:29:48,027 DEBUG TRAIN Batch 2/4400 loss 27.579855 loss_att 24.167141 loss_ctc 35.542854 lr 0.00257888 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:29:48,028 DEBUG TRAIN Batch 2/4400 loss 34.662884 loss_att 30.315456 loss_ctc 44.806881 lr 0.00257872 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:29:48,032 DEBUG TRAIN Batch 2/4400 loss 37.629524 loss_att 33.364090 loss_ctc 47.582211 lr 0.00257904 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:29:48,035 DEBUG TRAIN Batch 2/4400 loss 46.550545 loss_att 40.571396 loss_ctc 60.501892 lr 0.00257904 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:29:48,036 DEBUG TRAIN Batch 2/4400 loss 28.841541 loss_att 25.989309 loss_ctc 35.496750 lr 0.00257904 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:32:41,997 DEBUG TRAIN Batch 2/4500 loss 10.126101 loss_att 8.993236 loss_ctc 12.769453 lr 0.00259488 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:32:41,998 DEBUG TRAIN Batch 2/4500 loss 13.150587 loss_att 11.604247 loss_ctc 16.758713 lr 0.00259472 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:32:42,000 DEBUG TRAIN Batch 2/4500 loss 17.249697 loss_att 15.415461 loss_ctc 21.529583 lr 0.00259488 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:32:42,000 DEBUG TRAIN Batch 2/4500 loss 11.240960 loss_att 10.161914 loss_ctc 13.758736 lr 0.00259504 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:32:42,001 DEBUG TRAIN Batch 2/4500 loss 10.206917 loss_att 8.973765 loss_ctc 13.084270 lr 0.00259488 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:32:42,006 DEBUG TRAIN Batch 2/4500 loss 13.232623 loss_att 10.936142 loss_ctc 18.591080 lr 0.00259504 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:32:42,024 DEBUG TRAIN Batch 2/4500 loss 14.336387 loss_att 12.940575 loss_ctc 17.593281 lr 0.00259504 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:32:42,058 DEBUG TRAIN Batch 2/4500 loss 13.605438 loss_att 11.649635 loss_ctc 18.168980 lr 0.00259488 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:33:15,279 DEBUG TRAIN Batch 2/4600 loss 20.353828 loss_att 18.689213 loss_ctc 24.237928 lr 0.00261104 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:33:15,279 DEBUG TRAIN Batch 2/4600 loss 38.049812 loss_att 32.365143 loss_ctc 51.314037 lr 0.00261088 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:33:15,280 DEBUG TRAIN Batch 2/4600 loss 32.349583 loss_att 28.512047 loss_ctc 41.303833 lr 0.00261072 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:33:15,284 DEBUG TRAIN Batch 2/4600 loss 23.385941 loss_att 21.019516 loss_ctc 28.907598 lr 0.00261088 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:33:15,285 DEBUG TRAIN Batch 2/4600 loss 27.799168 loss_att 24.650831 loss_ctc 35.145290 lr 0.00261088 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:33:15,286 DEBUG TRAIN Batch 2/4600 loss 29.339729 loss_att 26.070885 loss_ctc 36.967033 lr 0.00261088 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:33:15,297 DEBUG TRAIN Batch 2/4600 loss 34.562271 loss_att 30.083912 loss_ctc 45.011780 lr 0.00261104 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:33:15,301 DEBUG TRAIN Batch 2/4600 loss 25.804836 loss_att 22.508858 loss_ctc 33.495457 lr 0.00261104 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:36:06,398 DEBUG TRAIN Batch 2/4700 loss 27.771255 loss_att 24.817780 loss_ctc 34.662701 lr 0.00262688 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:36:06,400 DEBUG TRAIN Batch 2/4700 loss 35.745964 loss_att 31.420879 loss_ctc 45.837830 lr 0.00262688 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:36:06,400 DEBUG TRAIN Batch 2/4700 loss 29.036249 loss_att 24.404114 loss_ctc 39.844563 lr 0.00262704 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:36:06,401 DEBUG TRAIN Batch 2/4700 loss 26.761093 loss_att 24.198387 loss_ctc 32.740738 lr 0.00262672 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:36:06,406 DEBUG TRAIN Batch 2/4700 loss 37.772396 loss_att 33.053368 loss_ctc 48.783463 lr 0.00262688 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:36:06,414 DEBUG TRAIN Batch 2/4700 loss 27.742102 loss_att 24.410049 loss_ctc 35.516891 lr 0.00262704 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:36:06,415 DEBUG TRAIN Batch 2/4700 loss 31.314411 loss_att 26.631582 loss_ctc 42.241013 lr 0.00262704 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:36:06,417 DEBUG TRAIN Batch 2/4700 loss 25.648331 loss_att 23.000082 loss_ctc 31.827576 lr 0.00262688 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:38:57,335 DEBUG TRAIN Batch 2/4800 loss 24.166475 loss_att 21.677227 loss_ctc 29.974720 lr 0.00264288 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:38:57,337 DEBUG TRAIN Batch 2/4800 loss 29.990562 loss_att 25.486736 loss_ctc 40.499493 lr 0.00264288 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:38:57,341 DEBUG TRAIN Batch 2/4800 loss 18.704659 loss_att 16.854061 loss_ctc 23.022720 lr 0.00264304 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:38:57,342 DEBUG TRAIN Batch 2/4800 loss 23.928762 loss_att 20.848694 loss_ctc 31.115589 lr 0.00264288 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:38:57,345 DEBUG TRAIN Batch 2/4800 loss 31.239868 loss_att 27.856382 loss_ctc 39.134666 lr 0.00264288 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:38:57,345 DEBUG TRAIN Batch 2/4800 loss 19.024199 loss_att 17.341499 loss_ctc 22.950497 lr 0.00264304 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:38:57,350 DEBUG TRAIN Batch 2/4800 loss 22.413250 loss_att 19.898796 loss_ctc 28.280312 lr 0.00264272 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:38:57,367 DEBUG TRAIN Batch 2/4800 loss 25.200403 loss_att 22.983276 loss_ctc 30.373699 lr 0.00264304 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:41:46,421 DEBUG TRAIN Batch 2/4900 loss 37.603523 loss_att 32.883907 loss_ctc 48.615959 lr 0.00265888 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:41:46,422 DEBUG TRAIN Batch 2/4900 loss 34.590015 loss_att 31.303595 loss_ctc 42.258335 lr 0.00265888 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:41:46,424 DEBUG TRAIN Batch 2/4900 loss 40.568386 loss_att 35.662228 loss_ctc 52.016094 lr 0.00265904 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:41:46,425 DEBUG TRAIN Batch 2/4900 loss 37.757339 loss_att 33.013226 loss_ctc 48.826942 lr 0.00265872 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:41:46,433 DEBUG TRAIN Batch 2/4900 loss 30.329906 loss_att 26.816788 loss_ctc 38.527184 lr 0.00265888 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:41:46,435 DEBUG TRAIN Batch 2/4900 loss 33.470337 loss_att 29.878983 loss_ctc 41.850170 lr 0.00265888 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:41:46,441 DEBUG TRAIN Batch 2/4900 loss 21.459267 loss_att 18.821854 loss_ctc 27.613226 lr 0.00265904 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:41:46,445 DEBUG TRAIN Batch 2/4900 loss 31.051647 loss_att 25.949490 loss_ctc 42.956676 lr 0.00265904 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:44:34,103 DEBUG TRAIN Batch 2/5000 loss 11.241922 loss_att 10.396223 loss_ctc 13.215221 lr 0.00267504 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:44:34,104 DEBUG TRAIN Batch 2/5000 loss 15.096014 loss_att 13.429277 loss_ctc 18.985065 lr 0.00267472 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:44:34,118 DEBUG TRAIN Batch 2/5000 loss 13.502874 loss_att 12.113539 loss_ctc 16.744658 lr 0.00267504 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:44:34,120 DEBUG TRAIN Batch 2/5000 loss 16.686197 loss_att 14.978109 loss_ctc 20.671738 lr 0.00267488 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:44:34,134 DEBUG TRAIN Batch 2/5000 loss 12.640536 loss_att 11.716402 loss_ctc 14.796850 lr 0.00267488 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:44:34,142 DEBUG TRAIN Batch 2/5000 loss 17.561047 loss_att 15.659423 loss_ctc 21.998173 lr 0.00267488 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:44:34,173 DEBUG TRAIN Batch 2/5000 loss 14.337219 loss_att 12.857615 loss_ctc 17.789625 lr 0.00267504 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:44:34,188 DEBUG TRAIN Batch 2/5000 loss 17.749159 loss_att 15.276989 loss_ctc 23.517555 lr 0.00267488 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:45:07,129 DEBUG TRAIN Batch 2/5100 loss 31.152487 loss_att 26.482382 loss_ctc 42.049400 lr 0.00269072 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:45:07,135 DEBUG TRAIN Batch 2/5100 loss 30.685505 loss_att 26.461853 loss_ctc 40.540691 lr 0.00269088 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:45:07,136 DEBUG TRAIN Batch 2/5100 loss 29.399036 loss_att 25.962856 loss_ctc 37.416786 lr 0.00269104 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:45:07,138 DEBUG TRAIN Batch 2/5100 loss 31.340256 loss_att 26.972555 loss_ctc 41.531559 lr 0.00269088 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:45:07,139 DEBUG TRAIN Batch 2/5100 loss 26.128342 loss_att 23.611311 loss_ctc 32.001411 lr 0.00269088 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:45:07,147 DEBUG TRAIN Batch 2/5100 loss 25.941029 loss_att 21.795277 loss_ctc 35.614452 lr 0.00269104 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:45:07,151 DEBUG TRAIN Batch 2/5100 loss 33.342125 loss_att 28.203476 loss_ctc 45.332298 lr 0.00269104 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:45:07,154 DEBUG TRAIN Batch 2/5100 loss 21.786928 loss_att 19.346111 loss_ctc 27.482168 lr 0.00269088 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:47:56,098 DEBUG TRAIN Batch 2/5200 loss 25.676735 loss_att 22.064703 loss_ctc 34.104809 lr 0.00270688 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:47:56,099 DEBUG TRAIN Batch 2/5200 loss 38.168526 loss_att 33.646503 loss_ctc 48.719910 lr 0.00270688 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:47:56,099 DEBUG TRAIN Batch 2/5200 loss 33.770000 loss_att 28.569603 loss_ctc 45.904263 lr 0.00270688 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:47:56,106 DEBUG TRAIN Batch 2/5200 loss 32.271770 loss_att 28.354691 loss_ctc 41.411625 lr 0.00270704 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:47:56,108 DEBUG TRAIN Batch 2/5200 loss 35.748447 loss_att 31.748541 loss_ctc 45.081566 lr 0.00270672 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:47:56,112 DEBUG TRAIN Batch 2/5200 loss 24.737389 loss_att 21.423927 loss_ctc 32.468796 lr 0.00270688 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:47:56,113 DEBUG TRAIN Batch 2/5200 loss 29.489914 loss_att 25.218899 loss_ctc 39.455612 lr 0.00270704 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:47:56,117 DEBUG TRAIN Batch 2/5200 loss 24.709759 loss_att 22.320227 loss_ctc 30.285336 lr 0.00270704 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:50:43,957 DEBUG TRAIN Batch 2/5300 loss 32.390320 loss_att 28.635078 loss_ctc 41.152550 lr 0.00272288 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:50:43,963 DEBUG TRAIN Batch 2/5300 loss 34.930866 loss_att 30.415884 loss_ctc 45.465828 lr 0.00272288 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:50:43,968 DEBUG TRAIN Batch 2/5300 loss 24.719521 loss_att 21.270414 loss_ctc 32.767437 lr 0.00272272 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:50:43,970 DEBUG TRAIN Batch 2/5300 loss 43.594658 loss_att 38.363388 loss_ctc 55.800941 lr 0.00272288 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:50:43,974 DEBUG TRAIN Batch 2/5300 loss 21.376102 loss_att 18.609556 loss_ctc 27.831375 lr 0.00272304 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:50:43,974 DEBUG TRAIN Batch 2/5300 loss 27.653496 loss_att 22.673183 loss_ctc 39.274227 lr 0.00272304 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:50:43,983 DEBUG TRAIN Batch 2/5300 loss 32.821182 loss_att 29.279186 loss_ctc 41.085835 lr 0.00272304 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:50:44,017 DEBUG TRAIN Batch 2/5300 loss 26.851482 loss_att 23.442181 loss_ctc 34.806519 lr 0.00272288 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:53:30,245 DEBUG TRAIN Batch 2/5400 loss 30.288916 loss_att 26.779205 loss_ctc 38.478241 lr 0.00273888 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:53:30,247 DEBUG TRAIN Batch 2/5400 loss 26.080032 loss_att 23.193996 loss_ctc 32.814110 lr 0.00273872 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:53:30,248 DEBUG TRAIN Batch 2/5400 loss 32.058170 loss_att 27.724987 loss_ctc 42.168938 lr 0.00273904 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:53:30,250 DEBUG TRAIN Batch 2/5400 loss 28.578838 loss_att 23.482056 loss_ctc 40.471325 lr 0.00273888 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:53:30,253 DEBUG TRAIN Batch 2/5400 loss 31.805717 loss_att 27.239273 loss_ctc 42.460754 lr 0.00273888 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:53:30,253 DEBUG TRAIN Batch 2/5400 loss 31.078682 loss_att 27.579247 loss_ctc 39.244030 lr 0.00273888 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:53:30,265 DEBUG TRAIN Batch 2/5400 loss 31.406410 loss_att 28.184189 loss_ctc 38.924927 lr 0.00273904 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:53:30,279 DEBUG TRAIN Batch 2/5400 loss 21.344255 loss_att 18.686825 loss_ctc 27.544928 lr 0.00273904 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:14,879 DEBUG TRAIN Batch 2/5500 loss 10.836301 loss_att 9.247505 loss_ctc 14.543489 lr 0.00275488 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:14,879 DEBUG TRAIN Batch 2/5500 loss 8.634068 loss_att 7.324843 loss_ctc 11.688922 lr 0.00275488 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:14,889 DEBUG TRAIN Batch 2/5500 loss 12.145073 loss_att 10.414750 loss_ctc 16.182491 lr 0.00275504 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:14,891 DEBUG TRAIN Batch 2/5500 loss 13.539530 loss_att 11.953078 loss_ctc 17.241249 lr 0.00275488 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:14,907 DEBUG TRAIN Batch 2/5500 loss 10.617756 loss_att 9.350907 loss_ctc 13.573733 lr 0.00275488 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:14,911 DEBUG TRAIN Batch 2/5500 loss 11.309673 loss_att 9.761784 loss_ctc 14.921415 lr 0.00275504 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:14,925 DEBUG TRAIN Batch 2/5500 loss 14.051439 loss_att 11.798002 loss_ctc 19.309458 lr 0.00275472 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:14,952 DEBUG TRAIN Batch 2/5500 loss 11.158621 loss_att 9.773974 loss_ctc 14.389462 lr 0.00275504 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:47,724 DEBUG TRAIN Batch 2/5600 loss 27.104221 loss_att 23.730175 loss_ctc 34.976994 lr 0.00277088 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:47,726 DEBUG TRAIN Batch 2/5600 loss 34.747314 loss_att 29.068914 loss_ctc 47.996910 lr 0.00277088 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:47,728 DEBUG TRAIN Batch 2/5600 loss 32.718288 loss_att 29.251524 loss_ctc 40.807400 lr 0.00277104 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:47,730 DEBUG TRAIN Batch 2/5600 loss 23.835619 loss_att 21.112869 loss_ctc 30.188702 lr 0.00277088 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:47,739 DEBUG TRAIN Batch 2/5600 loss 27.765438 loss_att 24.323563 loss_ctc 35.796482 lr 0.00277072 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:47,740 DEBUG TRAIN Batch 2/5600 loss 49.521179 loss_att 44.475597 loss_ctc 61.294212 lr 0.00277088 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:47,742 DEBUG TRAIN Batch 2/5600 loss 25.695223 loss_att 23.986862 loss_ctc 29.681396 lr 0.00277104 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:56:47,748 DEBUG TRAIN Batch 2/5600 loss 37.622913 loss_att 32.979275 loss_ctc 48.458076 lr 0.00277104 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:59:31,475 DEBUG TRAIN Batch 2/5700 loss 31.265730 loss_att 28.523849 loss_ctc 37.663452 lr 0.00278688 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 10:59:31,482 DEBUG TRAIN Batch 2/5700 loss 28.741470 loss_att 24.192348 loss_ctc 39.356087 lr 0.00278688 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 10:59:31,483 DEBUG TRAIN Batch 2/5700 loss 33.453945 loss_att 28.534452 loss_ctc 44.932755 lr 0.00278704 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 10:59:31,485 DEBUG TRAIN Batch 2/5700 loss 34.387859 loss_att 30.003675 loss_ctc 44.617615 lr 0.00278672 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 10:59:31,485 DEBUG TRAIN Batch 2/5700 loss 25.569782 loss_att 23.262783 loss_ctc 30.952787 lr 0.00278688 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 10:59:31,488 DEBUG TRAIN Batch 2/5700 loss 31.970287 loss_att 27.127539 loss_ctc 43.270031 lr 0.00278704 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 10:59:31,498 DEBUG TRAIN Batch 2/5700 loss 32.675491 loss_att 27.991171 loss_ctc 43.605568 lr 0.00278704 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 10:59:31,574 DEBUG TRAIN Batch 2/5700 loss 36.071491 loss_att 31.016722 loss_ctc 47.865959 lr 0.00278688 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:01:55,643 DEBUG TRAIN Batch 2/5800 loss 29.007332 loss_att 24.740463 loss_ctc 38.963364 lr 0.00280288 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:01:55,646 DEBUG TRAIN Batch 2/5800 loss 23.321774 loss_att 20.665958 loss_ctc 29.518673 lr 0.00280288 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:01:55,648 DEBUG TRAIN Batch 2/5800 loss 21.186136 loss_att 18.325268 loss_ctc 27.861496 lr 0.00280288 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:01:55,650 DEBUG TRAIN Batch 2/5800 loss 24.452091 loss_att 22.725798 loss_ctc 28.480106 lr 0.00280304 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:01:55,651 DEBUG TRAIN Batch 2/5800 loss 24.104296 loss_att 21.576872 loss_ctc 30.001616 lr 0.00280272 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:01:55,652 DEBUG TRAIN Batch 2/5800 loss 24.566078 loss_att 22.490726 loss_ctc 29.408566 lr 0.00280304 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:01:55,653 DEBUG TRAIN Batch 2/5800 loss 22.885592 loss_att 19.078856 loss_ctc 31.767973 lr 0.00280288 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:01:55,666 DEBUG TRAIN Batch 2/5800 loss 23.758835 loss_att 21.195118 loss_ctc 29.740841 lr 0.00280304 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:02:51,154 DEBUG CV Batch 2/0 loss 4.594197 loss_att 3.636496 loss_ctc 6.828831 history loss 4.240797 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:02:51,158 DEBUG CV Batch 2/0 loss 4.594197 loss_att 3.636496 loss_ctc 6.828831 history loss 4.240797 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:02:51,159 DEBUG CV Batch 2/0 loss 4.594197 loss_att 3.636496 loss_ctc 6.828831 history loss 4.240797 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:02:51,160 DEBUG CV Batch 2/0 loss 4.594197 loss_att 3.636496 loss_ctc 6.828831 history loss 4.240797 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:02:51,169 DEBUG CV Batch 2/0 loss 4.594197 loss_att 3.636496 loss_ctc 6.828831 history loss 4.240797 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:02:51,173 DEBUG CV Batch 2/0 loss 4.594197 loss_att 3.636496 loss_ctc 6.828831 history loss 4.240797 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:02:51,197 DEBUG CV Batch 2/0 loss 4.594197 loss_att 3.636496 loss_ctc 6.828831 history loss 4.240797 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:02:51,197 DEBUG CV Batch 2/0 loss 4.594197 loss_att 3.636496 loss_ctc 6.828831 history loss 4.240797 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:04:06,335 DEBUG CV Batch 2/100 loss 6.320330 loss_att 5.555094 loss_ctc 8.105882 history loss 8.093067 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:04:06,367 DEBUG CV Batch 2/100 loss 6.320330 loss_att 5.555094 loss_ctc 8.105882 history loss 8.093067 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:04:06,373 DEBUG CV Batch 2/100 loss 6.320330 loss_att 5.555094 loss_ctc 8.105882 history loss 8.093067 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:04:06,374 DEBUG CV Batch 2/100 loss 6.320330 loss_att 5.555094 loss_ctc 8.105882 history loss 8.093067 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:04:06,384 DEBUG CV Batch 2/100 loss 6.320330 loss_att 5.555094 loss_ctc 8.105882 history loss 8.093067 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:04:06,394 DEBUG CV Batch 2/100 loss 6.320330 loss_att 5.555094 loss_ctc 8.105882 history loss 8.093067 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:04:06,395 DEBUG CV Batch 2/100 loss 6.320330 loss_att 5.555094 loss_ctc 8.105882 history loss 8.093067 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:04:06,413 DEBUG CV Batch 2/100 loss 6.320330 loss_att 5.555094 loss_ctc 8.105882 history loss 8.093067 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:05:19,724 DEBUG CV Batch 2/200 loss 12.468458 loss_att 10.928640 loss_ctc 16.061367 history loss 8.746800 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:05:19,730 DEBUG CV Batch 2/200 loss 12.468458 loss_att 10.928640 loss_ctc 16.061367 history loss 8.746800 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:05:19,739 DEBUG CV Batch 2/200 loss 12.468458 loss_att 10.928640 loss_ctc 16.061367 history loss 8.746800 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:05:19,776 DEBUG CV Batch 2/200 loss 12.468458 loss_att 10.928640 loss_ctc 16.061367 history loss 8.746800 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:05:19,782 DEBUG CV Batch 2/200 loss 12.468458 loss_att 10.928640 loss_ctc 16.061367 history loss 8.746800 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:05:19,821 DEBUG CV Batch 2/200 loss 12.468458 loss_att 10.928640 loss_ctc 16.061367 history loss 8.746800 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:05:19,835 DEBUG CV Batch 2/200 loss 12.468458 loss_att 10.928640 loss_ctc 16.061367 history loss 8.746800 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:05:19,856 DEBUG CV Batch 2/200 loss 12.468458 loss_att 10.928640 loss_ctc 16.061367 history loss 8.746800 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:07:04,292 DEBUG CV Batch 2/300 loss 7.230114 loss_att 6.190005 loss_ctc 9.657033 history loss 11.051735 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:07:04,308 DEBUG CV Batch 2/300 loss 7.230114 loss_att 6.190005 loss_ctc 9.657033 history loss 11.051735 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:07:04,307 DEBUG CV Batch 2/300 loss 7.230114 loss_att 6.190005 loss_ctc 9.657033 history loss 11.051735 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:07:04,309 DEBUG CV Batch 2/300 loss 7.230114 loss_att 6.190005 loss_ctc 9.657033 history loss 11.051735 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:07:04,310 DEBUG CV Batch 2/300 loss 7.230114 loss_att 6.190005 loss_ctc 9.657033 history loss 11.051735 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:07:04,312 DEBUG CV Batch 2/300 loss 7.230114 loss_att 6.190005 loss_ctc 9.657033 history loss 11.051735 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:07:04,328 DEBUG CV Batch 2/300 loss 7.230114 loss_att 6.190005 loss_ctc 9.657033 history loss 11.051735 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:07:04,330 DEBUG CV Batch 2/300 loss 7.230114 loss_att 6.190005 loss_ctc 9.657033 history loss 11.051735 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:08:13,930 DEBUG CV Batch 2/400 loss 14.904158 loss_att 11.641708 loss_ctc 22.516537 history loss 12.189173 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:08:13,952 DEBUG CV Batch 2/400 loss 14.904158 loss_att 11.641708 loss_ctc 22.516537 history loss 12.189173 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:08:13,957 DEBUG CV Batch 2/400 loss 14.904158 loss_att 11.641708 loss_ctc 22.516537 history loss 12.189173 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:08:13,970 DEBUG CV Batch 2/400 loss 14.904158 loss_att 11.641708 loss_ctc 22.516537 history loss 12.189173 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:08:13,979 DEBUG CV Batch 2/400 loss 14.904158 loss_att 11.641708 loss_ctc 22.516537 history loss 12.189173 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:08:13,985 DEBUG CV Batch 2/400 loss 14.904158 loss_att 11.641708 loss_ctc 22.516537 history loss 12.189173 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:08:14,007 DEBUG CV Batch 2/400 loss 14.904158 loss_att 11.641708 loss_ctc 22.516537 history loss 12.189173 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:08:14,022 DEBUG CV Batch 2/400 loss 14.904158 loss_att 11.641708 loss_ctc 22.516537 history loss 12.189173 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:09:55,629 DEBUG CV Batch 2/500 loss 2.772330 loss_att 2.000610 loss_ctc 4.573009 history loss 12.545665 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:09:55,631 DEBUG CV Batch 2/500 loss 2.772330 loss_att 2.000610 loss_ctc 4.573009 history loss 12.545665 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:09:55,634 DEBUG CV Batch 2/500 loss 2.772330 loss_att 2.000610 loss_ctc 4.573009 history loss 12.545665 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:09:55,634 DEBUG CV Batch 2/500 loss 2.772330 loss_att 2.000610 loss_ctc 4.573009 history loss 12.545665 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:09:55,643 DEBUG CV Batch 2/500 loss 2.772330 loss_att 2.000610 loss_ctc 4.573009 history loss 12.545665 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:09:55,645 DEBUG CV Batch 2/500 loss 2.772330 loss_att 2.000610 loss_ctc 4.573009 history loss 12.545665 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:09:55,648 DEBUG CV Batch 2/500 loss 2.772330 loss_att 2.000610 loss_ctc 4.573009 history loss 12.545665 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:09:55,658 DEBUG CV Batch 2/500 loss 2.772330 loss_att 2.000610 loss_ctc 4.573009 history loss 12.545665 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:11:04,809 DEBUG CV Batch 2/600 loss 9.406208 loss_att 8.029610 loss_ctc 12.618271 history loss 11.810821 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:11:04,828 DEBUG CV Batch 2/600 loss 9.406208 loss_att 8.029610 loss_ctc 12.618271 history loss 11.810821 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:11:04,844 DEBUG CV Batch 2/600 loss 9.406208 loss_att 8.029610 loss_ctc 12.618271 history loss 11.810821 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:11:04,846 DEBUG CV Batch 2/600 loss 9.406208 loss_att 8.029610 loss_ctc 12.618271 history loss 11.810821 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:11:04,851 DEBUG CV Batch 2/600 loss 9.406208 loss_att 8.029610 loss_ctc 12.618271 history loss 11.810821 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:11:04,869 DEBUG CV Batch 2/600 loss 9.406208 loss_att 8.029610 loss_ctc 12.618271 history loss 11.810821 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:11:04,898 DEBUG CV Batch 2/600 loss 9.406208 loss_att 8.029610 loss_ctc 12.618271 history loss 11.810821 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:11:04,945 DEBUG CV Batch 2/600 loss 9.406208 loss_att 8.029610 loss_ctc 12.618271 history loss 11.810821 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:12:12,941 DEBUG CV Batch 2/700 loss 18.332741 loss_att 15.554866 loss_ctc 24.814451 history loss 11.701830 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:12:12,963 DEBUG CV Batch 2/700 loss 18.332741 loss_att 15.554866 loss_ctc 24.814451 history loss 11.701830 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:12:12,986 DEBUG CV Batch 2/700 loss 18.332741 loss_att 15.554866 loss_ctc 24.814451 history loss 11.701830 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:12:13,002 DEBUG CV Batch 2/700 loss 18.332741 loss_att 15.554866 loss_ctc 24.814451 history loss 11.701830 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:12:13,004 DEBUG CV Batch 2/700 loss 18.332741 loss_att 15.554866 loss_ctc 24.814451 history loss 11.701830 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:12:13,032 DEBUG CV Batch 2/700 loss 18.332741 loss_att 15.554866 loss_ctc 24.814451 history loss 11.701830 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:12:13,044 DEBUG CV Batch 2/700 loss 18.332741 loss_att 15.554866 loss_ctc 24.814451 history loss 11.701830 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:12:13,081 DEBUG CV Batch 2/700 loss 18.332741 loss_att 15.554866 loss_ctc 24.814451 history loss 11.701830 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:13:51,569 DEBUG CV Batch 2/800 loss 7.811614 loss_att 6.688226 loss_ctc 10.432852 history loss 12.267744 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:13:51,570 DEBUG CV Batch 2/800 loss 7.811614 loss_att 6.688226 loss_ctc 10.432852 history loss 12.267744 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:13:51,579 DEBUG CV Batch 2/800 loss 7.811614 loss_att 6.688226 loss_ctc 10.432852 history loss 12.267744 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:13:51,579 DEBUG CV Batch 2/800 loss 7.811614 loss_att 6.688226 loss_ctc 10.432852 history loss 12.267744 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:13:51,580 DEBUG CV Batch 2/800 loss 7.811614 loss_att 6.688226 loss_ctc 10.432852 history loss 12.267744 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:13:51,583 DEBUG CV Batch 2/800 loss 7.811614 loss_att 6.688226 loss_ctc 10.432852 history loss 12.267744 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:13:51,596 DEBUG CV Batch 2/800 loss 7.811614 loss_att 6.688226 loss_ctc 10.432852 history loss 12.267744 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:13:51,618 DEBUG CV Batch 2/800 loss 7.811614 loss_att 6.688226 loss_ctc 10.432852 history loss 12.267744 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:49,668 DEBUG CV Batch 2/900 loss 19.768431 loss_att 16.066437 loss_ctc 28.406416 history loss 12.717126 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:49,682 DEBUG CV Batch 2/900 loss 19.768431 loss_att 16.066437 loss_ctc 28.406416 history loss 12.717126 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:49,683 DEBUG CV Batch 2/900 loss 19.768431 loss_att 16.066437 loss_ctc 28.406416 history loss 12.717126 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:49,691 DEBUG CV Batch 2/900 loss 19.768431 loss_att 16.066437 loss_ctc 28.406416 history loss 12.717126 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:49,694 DEBUG CV Batch 2/900 loss 19.768431 loss_att 16.066437 loss_ctc 28.406416 history loss 12.717126 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:49,736 DEBUG CV Batch 2/900 loss 19.768431 loss_att 16.066437 loss_ctc 28.406416 history loss 12.717126 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:49,758 DEBUG CV Batch 2/900 loss 19.768431 loss_att 16.066437 loss_ctc 28.406416 history loss 12.717126 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:49,759 DEBUG CV Batch 2/900 loss 19.768431 loss_att 16.066437 loss_ctc 28.406416 history loss 12.717126 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,346 INFO Epoch 2 CV info cv_loss 12.86645962783898\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,347 INFO Epoch 3 TRAIN info lr 0.00281216\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,349 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,370 INFO Epoch 2 CV info cv_loss 12.86645962783898\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,371 INFO Epoch 3 TRAIN info lr 0.002812\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,373 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,373 INFO Epoch 2 CV info cv_loss 12.86645962783898\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,374 INFO Checkpoint: save to checkpoint /opt/ml/model/2.pt\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,375 INFO Epoch 2 CV info cv_loss 12.86645962783898\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,376 INFO Epoch 3 TRAIN info lr 0.002812\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,378 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,379 INFO Epoch 2 CV info cv_loss 12.86645962783898\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,380 INFO Epoch 3 TRAIN info lr 0.002812\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,382 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,410 INFO Epoch 2 CV info cv_loss 12.86645962783898\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,411 INFO Epoch 3 TRAIN info lr 0.0028123199999999997\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,413 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,426 INFO Epoch 2 CV info cv_loss 12.86645962783898\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,427 INFO Epoch 3 TRAIN info lr 0.0028123199999999997\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,427 INFO Epoch 2 CV info cv_loss 12.86645962783898\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,428 INFO Epoch 3 TRAIN info lr 0.002812\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,430 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,431 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,715 INFO Epoch 3 TRAIN info lr 0.00281216\u001b[0m\n",
      "\u001b[34m2022-11-28 11:14:50,718 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2022-11-28 11:17:40,054 DEBUG TRAIN Batch 3/0 loss 12.812565 loss_att 10.942966 loss_ctc 17.174961 lr 0.00281232 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:17:40,054 DEBUG TRAIN Batch 3/0 loss 13.246710 loss_att 11.246673 loss_ctc 17.913465 lr 0.00281216 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:17:40,056 DEBUG TRAIN Batch 3/0 loss 10.945974 loss_att 9.568502 loss_ctc 14.160076 lr 0.00281216 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:17:40,056 DEBUG TRAIN Batch 3/0 loss 12.485344 loss_att 10.927876 loss_ctc 16.119436 lr 0.00281248 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:17:40,060 DEBUG TRAIN Batch 3/0 loss 10.573410 loss_att 9.055335 loss_ctc 14.115583 lr 0.00281216 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:17:40,061 DEBUG TRAIN Batch 3/0 loss 14.260292 loss_att 12.860026 loss_ctc 17.527580 lr 0.00281216 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:17:40,123 DEBUG TRAIN Batch 3/0 loss 8.598501 loss_att 7.585896 loss_ctc 10.961246 lr 0.00281232 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:17:40,137 DEBUG TRAIN Batch 3/0 loss 12.327406 loss_att 10.809069 loss_ctc 15.870193 lr 0.00281248 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:18:12,983 DEBUG TRAIN Batch 3/100 loss 21.343626 loss_att 19.048637 loss_ctc 26.698599 lr 0.00282816 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:18:12,984 DEBUG TRAIN Batch 3/100 loss 35.762878 loss_att 30.982086 loss_ctc 46.918060 lr 0.00282816 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:18:12,992 DEBUG TRAIN Batch 3/100 loss 27.519451 loss_att 24.422482 loss_ctc 34.745712 lr 0.00282832 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:18:12,992 DEBUG TRAIN Batch 3/100 loss 22.393719 loss_att 19.553406 loss_ctc 29.021118 lr 0.00282816 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:18:12,993 DEBUG TRAIN Batch 3/100 loss 31.579338 loss_att 27.794552 loss_ctc 40.410503 lr 0.00282848 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:18:13,002 DEBUG TRAIN Batch 3/100 loss 36.098270 loss_att 31.528524 loss_ctc 46.761009 lr 0.00282848 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:18:13,004 DEBUG TRAIN Batch 3/100 loss 31.209229 loss_att 26.056221 loss_ctc 43.232910 lr 0.00282816 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:18:13,010 DEBUG TRAIN Batch 3/100 loss 27.268642 loss_att 23.705132 loss_ctc 35.583504 lr 0.00282832 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:21:01,781 DEBUG TRAIN Batch 3/200 loss 32.755882 loss_att 30.257141 loss_ctc 38.586285 lr 0.00284432 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:21:01,785 DEBUG TRAIN Batch 3/200 loss 27.042946 loss_att 23.940994 loss_ctc 34.280830 lr 0.00284416 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:21:01,787 DEBUG TRAIN Batch 3/200 loss 26.784172 loss_att 22.968054 loss_ctc 35.688454 lr 0.00284448 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:21:01,788 DEBUG TRAIN Batch 3/200 loss 34.013607 loss_att 28.149826 loss_ctc 47.695766 lr 0.00284416 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:21:01,799 DEBUG TRAIN Batch 3/200 loss 33.147789 loss_att 27.827185 loss_ctc 45.562538 lr 0.00284416 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:21:01,801 DEBUG TRAIN Batch 3/200 loss 29.475613 loss_att 25.746777 loss_ctc 38.176228 lr 0.00284448 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:21:01,801 DEBUG TRAIN Batch 3/200 loss 34.609268 loss_att 29.997360 loss_ctc 45.370392 lr 0.00284416 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:21:01,804 DEBUG TRAIN Batch 3/200 loss 32.015736 loss_att 28.959538 loss_ctc 39.146866 lr 0.00284432 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:23:50,103 DEBUG TRAIN Batch 3/300 loss 23.430618 loss_att 20.365368 loss_ctc 30.582870 lr 0.00286016 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:23:50,105 DEBUG TRAIN Batch 3/300 loss 33.010521 loss_att 29.625439 loss_ctc 40.909050 lr 0.00286032 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:23:50,109 DEBUG TRAIN Batch 3/300 loss 29.123163 loss_att 25.488403 loss_ctc 37.604267 lr 0.00286016 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:23:50,112 DEBUG TRAIN Batch 3/300 loss 16.565847 loss_att 14.544739 loss_ctc 21.281765 lr 0.00286048 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:23:50,113 DEBUG TRAIN Batch 3/300 loss 24.524384 loss_att 22.308998 loss_ctc 29.693619 lr 0.00286048 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:23:50,120 DEBUG TRAIN Batch 3/300 loss 22.840340 loss_att 20.237606 loss_ctc 28.913387 lr 0.00286016 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:23:50,122 DEBUG TRAIN Batch 3/300 loss 22.152233 loss_att 19.775738 loss_ctc 27.697386 lr 0.00286016 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:23:50,130 DEBUG TRAIN Batch 3/300 loss 27.555485 loss_att 24.508881 loss_ctc 34.664230 lr 0.00286032 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:26:39,436 DEBUG TRAIN Batch 3/400 loss 22.554981 loss_att 19.474958 loss_ctc 29.741703 lr 0.00287616 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:26:39,441 DEBUG TRAIN Batch 3/400 loss 31.501282 loss_att 27.091518 loss_ctc 41.790733 lr 0.00287632 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:26:39,441 DEBUG TRAIN Batch 3/400 loss 30.094650 loss_att 27.309072 loss_ctc 36.594330 lr 0.00287616 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:26:39,442 DEBUG TRAIN Batch 3/400 loss 35.280472 loss_att 32.035305 loss_ctc 42.852520 lr 0.00287648 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:26:39,448 DEBUG TRAIN Batch 3/400 loss 29.130516 loss_att 25.922995 loss_ctc 36.614731 lr 0.00287616 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:26:39,450 DEBUG TRAIN Batch 3/400 loss 35.695911 loss_att 30.923748 loss_ctc 46.830959 lr 0.00287616 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:26:39,457 DEBUG TRAIN Batch 3/400 loss 26.541872 loss_att 24.991079 loss_ctc 30.160389 lr 0.00287648 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:26:39,463 DEBUG TRAIN Batch 3/400 loss 31.916306 loss_att 27.271589 loss_ctc 42.753979 lr 0.00287632 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:29:26,802 DEBUG TRAIN Batch 3/500 loss 13.907895 loss_att 12.099825 loss_ctc 18.126726 lr 0.00289216 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:29:26,803 DEBUG TRAIN Batch 3/500 loss 11.167342 loss_att 9.586887 loss_ctc 14.855070 lr 0.00289216 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:29:26,806 DEBUG TRAIN Batch 3/500 loss 11.113966 loss_att 9.760730 loss_ctc 14.271515 lr 0.00289216 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:29:26,810 DEBUG TRAIN Batch 3/500 loss 11.858707 loss_att 10.448432 loss_ctc 15.149351 lr 0.00289248 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:29:26,815 DEBUG TRAIN Batch 3/500 loss 16.369129 loss_att 13.749086 loss_ctc 22.482559 lr 0.00289248 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:29:26,829 DEBUG TRAIN Batch 3/500 loss 13.187624 loss_att 11.206897 loss_ctc 17.809322 lr 0.00289232 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:29:26,834 DEBUG TRAIN Batch 3/500 loss 9.527218 loss_att 8.253863 loss_ctc 12.498378 lr 0.00289232 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:29:26,885 DEBUG TRAIN Batch 3/500 loss 17.582047 loss_att 15.282531 loss_ctc 22.947586 lr 0.00289216 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:30:00,204 DEBUG TRAIN Batch 3/600 loss 32.374889 loss_att 28.957991 loss_ctc 40.347649 lr 0.00290816 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:30:00,205 DEBUG TRAIN Batch 3/600 loss 33.454929 loss_att 27.231251 loss_ctc 47.976849 lr 0.00290816 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:30:00,206 DEBUG TRAIN Batch 3/600 loss 31.218277 loss_att 26.088825 loss_ctc 43.187004 lr 0.00290848 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:30:00,207 DEBUG TRAIN Batch 3/600 loss 30.796421 loss_att 26.832342 loss_ctc 40.045944 lr 0.00290832 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:30:00,211 DEBUG TRAIN Batch 3/600 loss 24.496105 loss_att 21.568050 loss_ctc 31.328228 lr 0.00290816 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:30:00,211 DEBUG TRAIN Batch 3/600 loss 30.506950 loss_att 26.271935 loss_ctc 40.388649 lr 0.00290816 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:30:00,217 DEBUG TRAIN Batch 3/600 loss 25.094423 loss_att 21.755085 loss_ctc 32.886211 lr 0.00290848 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:30:00,220 DEBUG TRAIN Batch 3/600 loss 29.045929 loss_att 24.485464 loss_ctc 39.687019 lr 0.00290832 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:32:48,989 DEBUG TRAIN Batch 3/700 loss 28.005245 loss_att 24.692989 loss_ctc 35.733845 lr 0.00292416 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:32:48,989 DEBUG TRAIN Batch 3/700 loss 26.489635 loss_att 23.489471 loss_ctc 33.490021 lr 0.00292416 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:32:48,995 DEBUG TRAIN Batch 3/700 loss 21.637510 loss_att 19.442759 loss_ctc 26.758596 lr 0.00292432 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:32:48,996 DEBUG TRAIN Batch 3/700 loss 30.562500 loss_att 26.094959 loss_ctc 40.986763 lr 0.00292448 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:32:48,997 DEBUG TRAIN Batch 3/700 loss 29.935080 loss_att 25.849951 loss_ctc 39.467049 lr 0.00292416 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:32:49,000 DEBUG TRAIN Batch 3/700 loss 35.003616 loss_att 30.372509 loss_ctc 45.809540 lr 0.00292448 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:32:49,000 DEBUG TRAIN Batch 3/700 loss 25.123924 loss_att 21.861618 loss_ctc 32.735977 lr 0.00292416 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:32:49,006 DEBUG TRAIN Batch 3/700 loss 20.904488 loss_att 18.181231 loss_ctc 27.258757 lr 0.00292432 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:35:36,170 DEBUG TRAIN Batch 3/800 loss 22.081728 loss_att 19.469681 loss_ctc 28.176504 lr 0.00294048 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:35:36,170 DEBUG TRAIN Batch 3/800 loss 19.147419 loss_att 16.562553 loss_ctc 25.178768 lr 0.00294032 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:35:36,172 DEBUG TRAIN Batch 3/800 loss 20.671564 loss_att 17.292006 loss_ctc 28.557201 lr 0.00294016 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:35:36,175 DEBUG TRAIN Batch 3/800 loss 28.454199 loss_att 24.449764 loss_ctc 37.797882 lr 0.00294016 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:35:36,176 DEBUG TRAIN Batch 3/800 loss 23.218300 loss_att 19.568659 loss_ctc 31.734131 lr 0.00294016 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:35:36,185 DEBUG TRAIN Batch 3/800 loss 28.383690 loss_att 23.827702 loss_ctc 39.014328 lr 0.00294048 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:35:36,189 DEBUG TRAIN Batch 3/800 loss 22.068045 loss_att 19.934555 loss_ctc 27.046186 lr 0.00294016 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:35:36,200 DEBUG TRAIN Batch 3/800 loss 23.068314 loss_att 20.587385 loss_ctc 28.857147 lr 0.00294032 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:38:22,929 DEBUG TRAIN Batch 3/900 loss 29.803574 loss_att 25.134411 loss_ctc 40.698280 lr 0.00295616 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:38:22,930 DEBUG TRAIN Batch 3/900 loss 28.760098 loss_att 25.870438 loss_ctc 35.502640 lr 0.00295616 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:38:22,932 DEBUG TRAIN Batch 3/900 loss 27.387085 loss_att 24.770649 loss_ctc 33.492104 lr 0.00295648 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:38:22,938 DEBUG TRAIN Batch 3/900 loss 31.108170 loss_att 27.086945 loss_ctc 40.491028 lr 0.00295616 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:38:22,940 DEBUG TRAIN Batch 3/900 loss 33.141411 loss_att 29.076443 loss_ctc 42.626328 lr 0.00295616 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:38:22,942 DEBUG TRAIN Batch 3/900 loss 28.816849 loss_att 26.028931 loss_ctc 35.321991 lr 0.00295632 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:38:22,947 DEBUG TRAIN Batch 3/900 loss 31.226181 loss_att 27.104446 loss_ctc 40.843563 lr 0.00295648 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:38:22,948 DEBUG TRAIN Batch 3/900 loss 25.380604 loss_att 20.412003 loss_ctc 36.974010 lr 0.00295632 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:10,394 DEBUG TRAIN Batch 3/1000 loss 12.492323 loss_att 11.017913 loss_ctc 15.932611 lr 0.00297232 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:10,396 DEBUG TRAIN Batch 3/1000 loss 13.052359 loss_att 11.604948 loss_ctc 16.429651 lr 0.00297216 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:10,398 DEBUG TRAIN Batch 3/1000 loss 12.079631 loss_att 10.144890 loss_ctc 16.594027 lr 0.00297216 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:10,399 DEBUG TRAIN Batch 3/1000 loss 12.028563 loss_att 10.315310 loss_ctc 16.026152 lr 0.00297248 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:10,400 DEBUG TRAIN Batch 3/1000 loss 9.630722 loss_att 9.030146 loss_ctc 11.032068 lr 0.00297216 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:10,406 DEBUG TRAIN Batch 3/1000 loss 11.028091 loss_att 9.507138 loss_ctc 14.576982 lr 0.00297216 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:10,414 DEBUG TRAIN Batch 3/1000 loss 13.348272 loss_att 11.442579 loss_ctc 17.794888 lr 0.00297248 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:10,462 DEBUG TRAIN Batch 3/1000 loss 7.960813 loss_att 6.434464 loss_ctc 11.522295 lr 0.00297232 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:43,399 DEBUG TRAIN Batch 3/1100 loss 25.987144 loss_att 22.122091 loss_ctc 35.005604 lr 0.00298816 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:43,399 DEBUG TRAIN Batch 3/1100 loss 27.075802 loss_att 23.838654 loss_ctc 34.629147 lr 0.00298816 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:43,400 DEBUG TRAIN Batch 3/1100 loss 28.145069 loss_att 24.492300 loss_ctc 36.668201 lr 0.00298816 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:43,403 DEBUG TRAIN Batch 3/1100 loss 23.229671 loss_att 19.669401 loss_ctc 31.536972 lr 0.00298832 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:43,405 DEBUG TRAIN Batch 3/1100 loss 34.974419 loss_att 30.180096 loss_ctc 46.161171 lr 0.00298848 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:43,414 DEBUG TRAIN Batch 3/1100 loss 31.582741 loss_att 26.584934 loss_ctc 43.244286 lr 0.00298848 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:43,421 DEBUG TRAIN Batch 3/1100 loss 29.623060 loss_att 25.652950 loss_ctc 38.886646 lr 0.00298816 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:41:43,422 DEBUG TRAIN Batch 3/1100 loss 27.031420 loss_att 23.076376 loss_ctc 36.259853 lr 0.00298832 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:44:28,058 DEBUG TRAIN Batch 3/1200 loss 27.430820 loss_att 25.699965 loss_ctc 31.469488 lr 0.00300416 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:44:28,060 DEBUG TRAIN Batch 3/1200 loss 29.560867 loss_att 26.286823 loss_ctc 37.200306 lr 0.00300448 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:44:28,063 DEBUG TRAIN Batch 3/1200 loss 35.484615 loss_att 30.778320 loss_ctc 46.465965 lr 0.00300416 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:44:28,066 DEBUG TRAIN Batch 3/1200 loss 27.589165 loss_att 24.688293 loss_ctc 34.357864 lr 0.00300432 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:44:28,068 DEBUG TRAIN Batch 3/1200 loss 21.943676 loss_att 19.775620 loss_ctc 27.002478 lr 0.00300416 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:44:28,072 DEBUG TRAIN Batch 3/1200 loss 27.832998 loss_att 24.520254 loss_ctc 35.562737 lr 0.00300416 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:44:28,073 DEBUG TRAIN Batch 3/1200 loss 27.508713 loss_att 25.232397 loss_ctc 32.820114 lr 0.00300448 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:44:28,085 DEBUG TRAIN Batch 3/1200 loss 20.675068 loss_att 17.741301 loss_ctc 27.520523 lr 0.00300432 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:47:12,283 DEBUG TRAIN Batch 3/1300 loss 22.776432 loss_att 18.759905 loss_ctc 32.148331 lr 0.00302016 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:47:12,283 DEBUG TRAIN Batch 3/1300 loss 30.756985 loss_att 26.647329 loss_ctc 40.346184 lr 0.00302048 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:47:12,285 DEBUG TRAIN Batch 3/1300 loss 26.343220 loss_att 23.514309 loss_ctc 32.944012 lr 0.00302016 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:47:12,292 DEBUG TRAIN Batch 3/1300 loss 29.903845 loss_att 26.934204 loss_ctc 36.833004 lr 0.00302016 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:47:12,294 DEBUG TRAIN Batch 3/1300 loss 25.763062 loss_att 22.552078 loss_ctc 33.255356 lr 0.00302032 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:47:12,296 DEBUG TRAIN Batch 3/1300 loss 22.278976 loss_att 18.913166 loss_ctc 30.132534 lr 0.00302048 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:47:12,296 DEBUG TRAIN Batch 3/1300 loss 23.557962 loss_att 19.965996 loss_ctc 31.939217 lr 0.00302016 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:47:12,312 DEBUG TRAIN Batch 3/1300 loss 22.584660 loss_att 18.954517 loss_ctc 31.054989 lr 0.00302032 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:49:56,328 DEBUG TRAIN Batch 3/1400 loss 20.058922 loss_att 17.317379 loss_ctc 26.455854 lr 0.00303616 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:49:56,329 DEBUG TRAIN Batch 3/1400 loss 16.514107 loss_att 14.339117 loss_ctc 21.589085 lr 0.00303632 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:49:56,332 DEBUG TRAIN Batch 3/1400 loss 28.553907 loss_att 25.588623 loss_ctc 35.472908 lr 0.00303616 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:49:56,335 DEBUG TRAIN Batch 3/1400 loss 32.448196 loss_att 29.116947 loss_ctc 40.221115 lr 0.00303648 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:49:56,338 DEBUG TRAIN Batch 3/1400 loss 25.140003 loss_att 22.589458 loss_ctc 31.091274 lr 0.00303616 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:49:56,349 DEBUG TRAIN Batch 3/1400 loss 24.461887 loss_att 21.486675 loss_ctc 31.404051 lr 0.00303648 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:49:56,350 DEBUG TRAIN Batch 3/1400 loss 36.193310 loss_att 31.673605 loss_ctc 46.739281 lr 0.00303616 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:49:56,354 DEBUG TRAIN Batch 3/1400 loss 20.043785 loss_att 18.259136 loss_ctc 24.207966 lr 0.00303632 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:52:41,173 DEBUG TRAIN Batch 3/1500 loss 13.139141 loss_att 11.214401 loss_ctc 17.630201 lr 0.00305232 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:52:41,174 DEBUG TRAIN Batch 3/1500 loss 9.709012 loss_att 8.071016 loss_ctc 13.531002 lr 0.00305248 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:52:41,183 DEBUG TRAIN Batch 3/1500 loss 12.403898 loss_att 10.526459 loss_ctc 16.784592 lr 0.00305216 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:52:41,186 DEBUG TRAIN Batch 3/1500 loss 11.720320 loss_att 9.837977 loss_ctc 16.112452 lr 0.00305216 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:52:41,192 DEBUG TRAIN Batch 3/1500 loss 9.235292 loss_att 7.893178 loss_ctc 12.366892 lr 0.00305216 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:52:41,197 DEBUG TRAIN Batch 3/1500 loss 13.091212 loss_att 11.556590 loss_ctc 16.671997 lr 0.00305232 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:52:41,206 DEBUG TRAIN Batch 3/1500 loss 11.509487 loss_att 10.361805 loss_ctc 14.187411 lr 0.00305248 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:52:41,226 DEBUG TRAIN Batch 3/1500 loss 8.630560 loss_att 7.119605 loss_ctc 12.156120 lr 0.00305216 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:53:14,070 DEBUG TRAIN Batch 3/1600 loss 31.946884 loss_att 27.878668 loss_ctc 41.439388 lr 0.00306816 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:53:14,074 DEBUG TRAIN Batch 3/1600 loss 23.940716 loss_att 21.021343 loss_ctc 30.752583 lr 0.00306832 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:53:14,074 DEBUG TRAIN Batch 3/1600 loss 23.112843 loss_att 19.711721 loss_ctc 31.048794 lr 0.00306816 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:53:14,078 DEBUG TRAIN Batch 3/1600 loss 23.889811 loss_att 20.656155 loss_ctc 31.435009 lr 0.00306848 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:53:14,079 DEBUG TRAIN Batch 3/1600 loss 24.367884 loss_att 21.435467 loss_ctc 31.210188 lr 0.00306816 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:53:14,079 DEBUG TRAIN Batch 3/1600 loss 29.784681 loss_att 26.370192 loss_ctc 37.751827 lr 0.00306816 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:53:14,083 DEBUG TRAIN Batch 3/1600 loss 21.104366 loss_att 18.399702 loss_ctc 27.415253 lr 0.00306848 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:53:14,092 DEBUG TRAIN Batch 3/1600 loss 33.055267 loss_att 29.398809 loss_ctc 41.587002 lr 0.00306832 rank 0\u001b[0m\n",
      "\u001b[34m2022-11-28 11:55:55,354 DEBUG TRAIN Batch 3/1700 loss 27.405453 loss_att 23.035498 loss_ctc 37.602020 lr 0.00308448 rank 7\u001b[0m\n",
      "\u001b[34m2022-11-28 11:55:55,355 DEBUG TRAIN Batch 3/1700 loss 31.269470 loss_att 27.798561 loss_ctc 39.368263 lr 0.00308432 rank 4\u001b[0m\n",
      "\u001b[34m2022-11-28 11:55:55,356 DEBUG TRAIN Batch 3/1700 loss 27.766680 loss_att 23.889486 loss_ctc 36.813461 lr 0.00308416 rank 6\u001b[0m\n",
      "\u001b[34m2022-11-28 11:55:55,358 DEBUG TRAIN Batch 3/1700 loss 28.032490 loss_att 25.118849 loss_ctc 34.830986 lr 0.00308416 rank 5\u001b[0m\n",
      "\u001b[34m2022-11-28 11:55:55,358 DEBUG TRAIN Batch 3/1700 loss 26.841011 loss_att 23.579361 loss_ctc 34.451530 lr 0.00308416 rank 3\u001b[0m\n",
      "\u001b[34m2022-11-28 11:55:55,361 DEBUG TRAIN Batch 3/1700 loss 36.305359 loss_att 31.856375 loss_ctc 46.686325 lr 0.00308416 rank 2\u001b[0m\n",
      "\u001b[34m2022-11-28 11:55:55,366 DEBUG TRAIN Batch 3/1700 loss 21.396267 loss_att 19.767036 loss_ctc 25.197807 lr 0.00308448 rank 1\u001b[0m\n",
      "\u001b[34m2022-11-28 11:55:55,373 DEBUG TRAIN Batch 3/1700 loss 26.329979 loss_att 23.315140 loss_ctc 33.364609 lr 0.00308432 rank 0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "job_name = est.fit({\"training\":training})\n",
    "#job_name = est.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07b7c9-acde-4d0a-b179-3134d28f6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = est.model_data\n",
    "print(\"Model artifact saved at:\\n\", model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60519dfe-3653-4321-a6ab-b125cb1d8e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchaudio==0.10.0\n",
      "Pillow\n",
      "pyyaml>=5.1\n",
      "sentencepiece\n",
      "tensorboard\n",
      "tensorboardX\n",
      "typeguard\n",
      "textgrid\n",
      "pytest\n",
      "flake8==3.8.2\n",
      "flake8-bugbear\n",
      "flake8-comprehensions\n",
      "flake8-executable\n",
      "flake8-pyi==20.5.0\n",
      "mccabe\n",
      "pycodestyle==2.6.0\n",
      "pyflakes==2.2.0"
     ]
    }
   ],
   "source": [
    "!cat wenet/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55192e1-23f5-4c2f-8c0f-5d1c43655226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
