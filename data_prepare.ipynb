{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301e413-90bb-49d9-b0f2-29c4da6f3945",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "conda install pytorch=1.10.0 torchvision torchaudio=0.10.0 cudatoolkit=11.1 -c pytorch -c conda-forge\n",
    "cd /root/wenet\n",
    "git clone https://github.com/wenet-e2e/wenet.git\n",
    "cd wenet\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2288de5-09f0-4d08-a371-a6e3e323a6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/wenet/wenet/examples/librispeech/s0\n",
      "env: DATA_DIR=/root/wenet/data/training\n"
     ]
    }
   ],
   "source": [
    "%cd \"/root/wenet/wenet/examples/librispeech/s0/\"\n",
    "%env DATA_DIR=/root/wenet/data/training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826e810f-8492-4108-abb0-58cc1fbb09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "!chmod 777 *.sh\n",
    "!cp run.sh run-bak.sh\n",
    "#mkdir export\n",
    "!mkdir -p $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9571a-1e90-4420-bfe4-db01a971dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# install flac\n",
    "wget https://downloads.xiph.org/releases/flac/flac-1.3.2.tar.xz\n",
    "xz -d flac-1.3.2.tar.xz && tar -xvf flac-1.3.2.tar --no-same-owner && cd flac-1.3.2 && ./configure --prefix=/usr --disable-thorough-tests && make && make install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d21008-7bc8-48c0-b24f-ce6610d9e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Data download\n",
    "./run.sh --stage -1 --stop_stage -1 --datadir $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644509f-5a68-4df7-85fc-c73ac68ffcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Data preparep\n",
    "./run.sh --stage 0 --stop_stage 0 --datadir $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce8a48d-b250-44d9-8561-5342018cb45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage 1: Feature Generation\n",
      "using resample and new sample rate is 16000\n",
      "[2022-12-01 04:13:06.224 pytorch-1-12-cpu-py3-ml-c5-4xlarge-48f6c7eb29a07ab0f18a4946953d:4126 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-12-01 04:13:06.446 pytorch-1-12-cpu-py3-ml-c5-4xlarge-48f6c7eb29a07ab0f18a4946953d:4126 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "dictionary: data/lang_char/train_clean_100_unigram5000_units.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processed 1000 wavs, 1266562 frames\n",
      "processed 2000 wavs, 2521275 frames\n",
      "processed 3000 wavs, 3774702 frames\n",
      "processed 4000 wavs, 5044147 frames\n",
      "processed 5000 wavs, 6313428 frames\n",
      "processed 6000 wavs, 7586115 frames\n",
      "processed 7000 wavs, 8860718 frames\n",
      "processed 8000 wavs, 10142828 frames\n",
      "processed 9000 wavs, 11413681 frames\n",
      "processed 10000 wavs, 12684987 frames\n",
      "processed 11000 wavs, 13930836 frames\n",
      "processed 12000 wavs, 15197649 frames\n",
      "processed 13000 wavs, 16483920 frames\n",
      "processed 14000 wavs, 17756963 frames\n",
      "processed 15000 wavs, 19013510 frames\n",
      "processed 16000 wavs, 20269998 frames\n",
      "processed 17000 wavs, 21535826 frames\n",
      "processed 18000 wavs, 22812304 frames\n",
      "processed 19000 wavs, 24074440 frames\n",
      "processed 20000 wavs, 25333124 frames\n",
      "processed 21000 wavs, 26613578 frames\n",
      "processed 22000 wavs, 27891132 frames\n",
      "processed 23000 wavs, 29177650 frames\n",
      "processed 24000 wavs, 30439073 frames\n",
      "processed 25000 wavs, 31706880 frames\n",
      "processed 26000 wavs, 32996103 frames\n",
      "processed 27000 wavs, 34248068 frames\n",
      "processed 28000 wavs, 35495586 frames\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Feature Generation: \n",
    "./run.sh --stage 1 --stop_stage 1 --datadir $DATA_DIR\n",
    "\n",
    "#chmod -R 777 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3c5441f-2efb-4189-a7d9-a427ab08860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary: data/lang_char/train_clean_100_unigram5000_units.txt\n",
      "stage 2: Dictionary and Json Data Preparation\n",
      "5002 data/lang_char/train_clean_100_unigram5000_units.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=data/lang_char/input.txt --vocab_size=5000 --model_type=unigram --model_prefix=data/lang_char/train_clean_100_unigram5000 --input_sentence_size=100000000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data/lang_char/input.txt\n",
      "  input_format: \n",
      "  model_prefix: data/lang_char/train_clean_100_unigram5000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 100000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: data/lang_char/input.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 28539 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=5298357\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9536% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=27\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999536\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 28539 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 80522 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 28539\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 33798\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 33798 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=28243 obj=9.37547 num_tokens=59432 num_tokens/piece=2.10431\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=21795 obj=7.60127 num_tokens=59764 num_tokens/piece=2.7421\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=16342 obj=7.54919 num_tokens=64406 num_tokens/piece=3.94113\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=16337 obj=7.53067 num_tokens=64427 num_tokens/piece=3.94362\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=12252 obj=7.62772 num_tokens=72733 num_tokens/piece=5.93642\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=12252 obj=7.60385 num_tokens=72728 num_tokens/piece=5.93601\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=9189 obj=7.75535 num_tokens=81872 num_tokens/piece=8.90978\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=9189 obj=7.72237 num_tokens=81864 num_tokens/piece=8.90891\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=6891 obj=7.92348 num_tokens=90619 num_tokens/piece=13.1503\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=6891 obj=7.88293 num_tokens=90624 num_tokens/piece=13.1511\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=5500 obj=8.07046 num_tokens=97562 num_tokens/piece=17.7385\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=5500 obj=8.03585 num_tokens=97587 num_tokens/piece=17.7431\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: data/lang_char/train_clean_100_unigram5000.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: data/lang_char/train_clean_100_unigram5000.vocab\n",
      "processed 10000 lines\n",
      "processed 20000 lines\n",
      "skipped 0 empty lines\n",
      "filtered 0 lines\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./run.sh --stage 2 --stop_stage 2 --datadir $DATA_DIR\n",
    "    \n",
    "#chmod -R 777 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38339e5-6644-4237-b41c-154019e0cdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary: data/lang_char/train_clean_100_unigram5000_units.txt\n",
      "Prepare data, prepare required format\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./run.sh --stage 3 --stop_stage 3 --datadir $DATA_DIR\n",
    "    \n",
    "#chmod -R 777 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340d17e-540d-4c65-8127-4bdd33005612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sess.default_bucket() \n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dcecd8-d322-41fb-8ea2-7ca548816bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env BUCKET=sagemaker-us-east-1-348052051973\n",
    "%env PREFIX=wenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3c0815-38f5-41f9-8d05-d39169e48aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#upload dataset+etc to s3 bucket\n",
    "aws s3 cp --recursive  exp s3://$BUCKET/$PREFIX/exp\n",
    "aws s3 cp --recursive  data s3://$BUCKET/$PREFIX/data\n",
    "#make sure directory data/train_960 exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caacba29-62a6-4753-aeb2-7435e854a446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/dev-clean s3://sagemaker-us-east-1-348052051973/wenet/export/Librispeech/dev-clean --only-show-errors &\n",
       "\n",
       "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/dev-other s3://sagemaker-us-east-1-348052051973/wenet/export/Librispeech/dev-other --only-show-errors &\n",
       "\n",
       "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/test-clean s3://sagemaker-us-east-1-348052051973/wenet/export/Librispeech/test-clean --only-show-errors &\n",
       "\n",
       "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/test-other s3://sagemaker-us-east-1-348052051973/wenet/export/Librispeech/test-other --only-show-errors &\n",
       "\n",
       "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/train-clean-100 s3://sagemaker-us-east-1-348052051973/wenet/export/Librispeech/train-clean-100 --only-show-errors &\n",
       "\n",
       "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/train-clean-360 s3://sagemaker-us-east-1-348052051973/wenet/export/Librispeech/train-clean-360 --only-show-errors &\n",
       "\n",
       "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/train-other-500 s3://sagemaker-us-east-1-348052051973/wenet/export/Librispeech/train-other-500 --only-show-errors &\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "We could use terminal with following commands to speed up copying to s3. Replace sagemaker-us-east-1-348052051973 with your owne s3 bucket\n",
    "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/dev-clean s3://sagemaker-us-east-1-348052051973/wenet/export/LibriSpeech/dev-clean --only-show-errors &\n",
    "\n",
    "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/dev-other s3://sagemaker-us-east-1-348052051973/wenet/export/LibriSpeech/dev-other --only-show-errors &\n",
    "\n",
    "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/test-clean s3://sagemaker-us-east-1-348052051973/wenet/export/LibriSpeech/test-clean --only-show-errors &\n",
    "\n",
    "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/test-other s3://sagemaker-us-east-1-348052051973/wenet/export/LibriSpeech/test-other --only-show-errors &\n",
    "\n",
    "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/train-clean-100 s3://sagemaker-us-east-1-348052051973/wenet/export/LibriSpeech/train-clean-100 --only-show-errors &\n",
    "\n",
    "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/train-clean-360 s3://sagemaker-us-east-1-348052051973/wenet/export/LibriSpeech/train-clean-360 --only-show-errors &\n",
    "\n",
    "nohup aws s3 sync ~/wenet/data/training/LibriSpeech/train-other-500 s3://sagemaker-us-east-1-348052051973/wenet/export/LibriSpeech/train-other-500 --only-show-errors &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae261e8-1c68-4fad-8cf0-6315202d2783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.12-cpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
