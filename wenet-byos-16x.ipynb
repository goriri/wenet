{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fecbc5ab-852b-4157-916d-8feb9207b7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3.__version__:1.26.96\n",
      "sagemaker.__version__:2.140.1\n",
      "bucket:sagemaker-us-east-1-348052051973\n",
      "role:arn:aws:iam::348052051973:role/service-role/AmazonSageMakerServiceCatalogProductsExecutionRole\n",
      "CPU times: user 394 ms, sys: 59 ms, total: 453 ms\n",
      "Wall time: 871 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.utils import unique_name_from_base\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = get_execution_role()\n",
    "prefix = 'wenet220'\n",
    "output_path = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "print(\"boto3.__version__:{}\".format(boto3.__version__))\n",
    "print(\"sagemaker.__version__:{}\".format(sagemaker.__version__))\n",
    "print(\"bucket:{}\".format(bucket))\n",
    "print(\"role:{}\".format(role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aad794e-6139-4422-892b-936ccaaa3c39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Change the /root/wenet to /opt/ml/input in all data.list files (especially for train_960 and dev)\n",
       "The \"Librispeech\" in data.list file in Github has the wrong captalization because it's wrong when I upload it. Please change it yourself!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "Change the /root/wenet to /opt/ml/input in all data.list files (especially for train_960 and dev)\n",
    "The \"Librispeech\" in data.list file in Github has the wrong captalization because it's wrong when I upload it. Please change it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "991f505d-b0c4-4aa1-abae-fd3c3d4534d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "prefix_dataset = \"wenet220/export\"\n",
    "loc =f\"s3://{bucket}/{prefix_dataset}\"\n",
    "\n",
    "training = TrainingInput(\n",
    "    s3_data_type='S3Prefix', # Available Options: S3Prefix | ManifestFile | AugmentedManifestFile\n",
    "    s3_data=loc,\n",
    "    distribution='FullyReplicated', # Available Options: FullyReplicated | ShardedByS3Key \n",
    "    input_mode='FastFile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "922ce9af-f711-4702-b811-4379efbcc8fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "# instance_type='local'\n",
    "\n",
    "max_run = 432000\n",
    "checkpoint_s3_uri = f\"s3://{bucket}/{prefix}/checkpoints\"\n",
    "\n",
    "hyperparameters = {\n",
    "    'datadir':'/opt/ml/input/data/training',\n",
    "    'stage': '4',\n",
    "    'stop_stage': '4',\n",
    "    'train_config': 'conf/train_conformer.yaml',\n",
    "    'model_dir': '/opt/ml/model',\n",
    "    'checkpoint_dir': '/opt/ml/checkpoints',\n",
    "    'output_dir': '/opt/ml/output/data',\n",
    "}\n",
    "\n",
    "est = PyTorch(\n",
    "    entry_point=\"run-librispeech.sh\",\n",
    "    source_dir=\"/root/wenet-source/wenet\",\n",
    "    framework_version=\"1.11.0\",\n",
    "    py_version=\"py38\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=200,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=prefix,\n",
    "    hyperparameters = hyperparameters,\n",
    "    checkpoint_s3_uri = checkpoint_s3_uri,\n",
    "    output_path = f\"s3://{bucket}/{prefix}/\",\n",
    "    # keep_alive_period_in_seconds=1800,\n",
    "    max_run = max_run,\n",
    "    tags = [{\"Key\": \"team\", \"Value\": \"asr\"}, {\"Key\": \"person\", \"Value\": \"andrew\"}, {\"Key\": \"project\", \"Value\": \"abc\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653e3f8-9feb-449b-9dad-a53b1735e33d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: wenet220-2023-03-29-03-16-01-482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-29 03:17:23 Starting - Starting the training job...\n",
      "2023-03-29 03:18:00 Starting - Preparing the instances for training.........\n",
      "2023-03-29 03:19:30 Downloading - Downloading input data......\n",
      "2023-03-29 03:20:21 Training - Downloading the training image..................\n",
      "2023-03-29 03:23:27 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:13,504 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:13,566 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:13,575 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:13,577 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:19,523 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:19,596 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:19,666 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:19,675 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"checkpoint_dir\": \"/opt/ml/checkpoints\",\n",
      "        \"datadir\": \"/opt/ml/input/data/training\",\n",
      "        \"model_dir\": \"/opt/ml/model\",\n",
      "        \"output_dir\": \"/opt/ml/output/data\",\n",
      "        \"stage\": \"4\",\n",
      "        \"stop_stage\": \"4\",\n",
      "        \"train_config\": \"conf/train_conformer.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"wenet220-2023-03-29-03-16-01-482\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-348052051973/wenet220-2023-03-29-03-16-01-482/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run-librispeech.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run-librispeech.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"datadir\":\"/opt/ml/input/data/training\",\"model_dir\":\"/opt/ml/model\",\"output_dir\":\"/opt/ml/output/data\",\"stage\":\"4\",\"stop_stage\":\"4\",\"train_config\":\"conf/train_conformer.yaml\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run-librispeech.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run-librispeech.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-348052051973/wenet220-2023-03-29-03-16-01-482/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"datadir\":\"/opt/ml/input/data/training\",\"model_dir\":\"/opt/ml/model\",\"output_dir\":\"/opt/ml/output/data\",\"stage\":\"4\",\"stop_stage\":\"4\",\"train_config\":\"conf/train_conformer.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"wenet220-2023-03-29-03-16-01-482\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-348052051973/wenet220-2023-03-29-03-16-01-482/source/sourcedir.tar.gz\",\"module_name\":\"run-librispeech.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run-librispeech.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--checkpoint_dir\",\"/opt/ml/checkpoints\",\"--datadir\",\"/opt/ml/input/data/training\",\"--model_dir\",\"/opt/ml/model\",\"--output_dir\",\"/opt/ml/output/data\",\"--stage\",\"4\",\"--stop_stage\",\"4\",\"--train_config\",\"conf/train_conformer.yaml\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINT_DIR=/opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mSM_HP_DATADIR=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_HP_STAGE=4\u001b[0m\n",
      "\u001b[34mSM_HP_STOP_STAGE=4\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_CONFIG=conf/train_conformer.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./run-librispeech.sh --checkpoint_dir /opt/ml/checkpoints --datadir /opt/ml/input/data/training --model_dir /opt/ml/model --output_dir /opt/ml/output/data --stage 4 --stop_stage 4 --train_config conf/train_conformer.yaml\"\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:20,649 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 33.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 106.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboardX\u001b[0m\n",
      "\u001b[34mDownloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 24.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting typeguard==2.13.3\u001b[0m\n",
      "\u001b[34mDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting textgrid\u001b[0m\n",
      "\u001b[34mDownloading TextGrid-1.5-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytest\u001b[0m\n",
      "\u001b[34mDownloading pytest-7.2.2-py3-none-any.whl (317 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.2/317.2 kB 51.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting flake8==3.8.2\u001b[0m\n",
      "\u001b[34mDownloading flake8-3.8.2-py2.py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.7/72.7 kB 21.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting flake8-bugbear\u001b[0m\n",
      "\u001b[34mDownloading flake8_bugbear-23.3.23-py3-none-any.whl (28 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-comprehensions\u001b[0m\n",
      "\u001b[34mDownloading flake8_comprehensions-3.11.1-py3-none-any.whl (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-executable\u001b[0m\n",
      "\u001b[34mDownloading flake8_executable-2.1.3-py3-none-any.whl (35 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-pyi==20.5.0\u001b[0m\n",
      "\u001b[34mDownloading flake8_pyi-20.5.0-py36-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting mccabe\u001b[0m\n",
      "\u001b[34mDownloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pycodestyle==2.6.0\u001b[0m\n",
      "\u001b[34mDownloading pycodestyle-2.6.0-py2.py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.4/41.4 kB 11.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pyflakes==2.2.0\u001b[0m\n",
      "\u001b[34mDownloading pyflakes-2.2.0-py2.py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 19.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mccabe\u001b[0m\n",
      "\u001b[34mDownloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs in /opt/conda/lib/python3.8/site-packages (from flake8-pyi==20.5.0->-r requirements.txt (line 13)) (22.2.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 85.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<0.5,>=0.4.1\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (0.38.4)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.53.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/5.0 MB 115.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 33.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-plugin-wit>=1.6.0\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.3/781.3 kB 96.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (2.2.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.3-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.9/93.9 kB 24.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (1.22.2)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.17.0-py2.py3-none-any.whl (178 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 178.1/178.1 kB 39.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 4)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from tensorboardX->-r requirements.txt (line 5)) (23.0)\u001b[0m\n",
      "\u001b[34mCollecting tomli>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading tomli-2.0.1-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting iniconfig\u001b[0m\n",
      "\u001b[34mDownloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.8/site-packages (from pytest->-r requirements.txt (line 8)) (1.0.0)\u001b[0m\n",
      "\u001b[34mCollecting exceptiongroup>=1.0.0rc8\u001b[0m\n",
      "\u001b[34mDownloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting flake8-bugbear\u001b[0m\n",
      "\u001b[34mDownloading flake8_bugbear-23.3.12-py3-none-any.whl (28 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 28.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 4)) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 4)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 4)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 4)) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 4)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 39.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: textgrid, tensorboard-plugin-wit, sentencepiece, mccabe, typeguard, tomli, tensorboardX, tensorboard-data-server, pyflakes, pycodestyle, pyasn1-modules, oauthlib, iniconfig, grpcio, exceptiongroup, cachetools, absl-py, requests-oauthlib, pytest, markdown, google-auth, flake8, google-auth-oauthlib, flake8-pyi, flake8-executable, flake8-comprehensions, flake8-bugbear, tensorboard\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.4.0 cachetools-5.3.0 exceptiongroup-1.1.1 flake8-3.8.2 flake8-bugbear-23.3.12 flake8-comprehensions-3.11.1 flake8-executable-2.1.3 flake8-pyi-20.5.0 google-auth-2.17.0 google-auth-oauthlib-0.4.6 grpcio-1.53.0 iniconfig-2.0.0 markdown-3.4.3 mccabe-0.6.1 oauthlib-3.2.2 pyasn1-modules-0.2.8 pycodestyle-2.6.0 pyflakes-2.2.0 pytest-7.2.2 requests-oauthlib-1.3.1 sentencepiece-0.1.97 tensorboard-2.12.0 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 tensorboardX-2.6 textgrid-1.5 tomli-2.0.1 typeguard-2.13.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mdictionary: data/lang_char/train_960_unigram5000_units.txt\u001b[0m\n",
      "\u001b[34mrun.sh: init method is file:///opt/ml/code/examples/librispeech/s0/exp/sp_spec_aug/ddp_init\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,772 INFO training on multiple gpus, this gpu 7\u001b[0m\n",
      "\u001b[34mFailed to import k2 and icefall.         Notice that they are necessary for hlg_onebest and hlg_rescore\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,773 INFO training on multiple gpus, this gpu 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,786 INFO training on multiple gpus, this gpu 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,786 INFO training on multiple gpus, this gpu 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,794 INFO training on multiple gpus, this gpu 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,795 INFO training on multiple gpus, this gpu 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,796 INFO training on multiple gpus, this gpu 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,805 INFO training on multiple gpus, this gpu 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,866 INFO Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,876 INFO Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,887 INFO Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,898 INFO Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,908 INFO Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,918 INFO Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,928 INFO Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,928 INFO Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,928 INFO Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,928 INFO Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,929 INFO Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,929 INFO Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,929 INFO Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,938 INFO Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,938 INFO Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:26,938 INFO Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,271 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,283 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,283 INFO train dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,283 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:27,283 INFO validation dataset loaded\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34mthe number of model params: 46,788,626\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,730 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,731 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,792 INFO Checkpoint: save to checkpoint /opt/ml/checkpoints/init.pt\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,793 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,794 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,795 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,796 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,888 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,889 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,893 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,894 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,894 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,895 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,897 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:32,898 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.120 algo-1:217 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:33,121 INFO Epoch 0 TRAIN info lr 1.6e-07\u001b[0m\n",
      "\u001b[34m2023-03-28 23:24:33,122 INFO using accumulate grad, new batch size is 1 times larger than before\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.183 algo-1:218 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.202 algo-1:214 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.275 algo-1:215 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.278 algo-1:216 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.287 algo-1:219 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.311 algo-1:213 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.498 algo-1:217 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.550 algo-1:212 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.564 algo-1:218 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.603 algo-1:214 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.666 algo-1:216 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.666 algo-1:215 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.683 algo-1:219 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.768 algo-1:213 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-03-28 23:24:33.927 algo-1:212 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m[W reducer.cpp:1282] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,308 DEBUG TRAIN Batch 0/0 loss 204.432053 loss_att 81.260284 loss_ctc 491.832825 lr 0.00000016 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,310 DEBUG TRAIN Batch 0/0 loss 194.463852 loss_att 80.563660 loss_ctc 460.230927 lr 0.00000016 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,311 DEBUG TRAIN Batch 0/0 loss 181.749664 loss_att 73.846573 loss_ctc 433.523560 lr 0.00000016 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,312 DEBUG TRAIN Batch 0/0 loss 181.995453 loss_att 68.416588 loss_ctc 447.012787 lr 0.00000016 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,315 DEBUG TRAIN Batch 0/0 loss 162.911316 loss_att 56.654522 loss_ctc 410.843842 lr 0.00000016 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,317 DEBUG TRAIN Batch 0/0 loss 196.423889 loss_att 71.998436 loss_ctc 486.749939 lr 0.00000016 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,318 DEBUG TRAIN Batch 0/0 loss 188.555679 loss_att 67.069550 loss_ctc 472.023285 lr 0.00000016 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:27:49,324 DEBUG TRAIN Batch 0/0 loss 174.466858 loss_att 59.583115 loss_ctc 442.528900 lr 0.00000016 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,061 DEBUG TRAIN Batch 0/100 loss 401.154968 loss_att 366.788574 loss_ctc 481.343262 lr 0.00001616 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,062 DEBUG TRAIN Batch 0/100 loss 381.153015 loss_att 346.797546 loss_ctc 461.315674 lr 0.00001616 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,062 DEBUG TRAIN Batch 0/100 loss 402.054413 loss_att 369.423065 loss_ctc 478.194275 lr 0.00001616 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,069 DEBUG TRAIN Batch 0/100 loss 366.122437 loss_att 332.458496 loss_ctc 444.671631 lr 0.00001616 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,069 DEBUG TRAIN Batch 0/100 loss 380.871521 loss_att 346.682190 loss_ctc 460.646576 lr 0.00001616 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,070 DEBUG TRAIN Batch 0/100 loss 405.917328 loss_att 371.632629 loss_ctc 485.914978 lr 0.00001616 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,071 DEBUG TRAIN Batch 0/100 loss 401.492462 loss_att 367.003113 loss_ctc 481.967651 lr 0.00001616 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:28:21,074 DEBUG TRAIN Batch 0/100 loss 385.354675 loss_att 351.132507 loss_ctc 465.206390 lr 0.00001616 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,529 DEBUG TRAIN Batch 0/200 loss 342.505493 loss_att 320.458740 loss_ctc 393.947937 lr 0.00003216 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,530 DEBUG TRAIN Batch 0/200 loss 327.755890 loss_att 307.206360 loss_ctc 375.704803 lr 0.00003216 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,530 DEBUG TRAIN Batch 0/200 loss 340.735901 loss_att 319.379425 loss_ctc 390.567627 lr 0.00003216 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,534 DEBUG TRAIN Batch 0/200 loss 337.787292 loss_att 316.337616 loss_ctc 387.836548 lr 0.00003216 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,537 DEBUG TRAIN Batch 0/200 loss 314.421143 loss_att 294.174805 loss_ctc 361.662598 lr 0.00003216 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,539 DEBUG TRAIN Batch 0/200 loss 349.083435 loss_att 326.384796 loss_ctc 402.046844 lr 0.00003216 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,539 DEBUG TRAIN Batch 0/200 loss 333.304077 loss_att 312.207489 loss_ctc 382.529388 lr 0.00003216 rank 6\u001b[0m\n",
      "\u001b[34m2023-03-28 23:31:32,540 DEBUG TRAIN Batch 0/200 loss 313.206909 loss_att 292.456665 loss_ctc 361.624115 lr 0.00003216 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,792 DEBUG TRAIN Batch 0/300 loss 212.184799 loss_att 202.527954 loss_ctc 234.717453 lr 0.00004816 rank 5\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,794 DEBUG TRAIN Batch 0/300 loss 200.814972 loss_att 191.825485 loss_ctc 221.790451 lr 0.00004816 rank 7\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,795 DEBUG TRAIN Batch 0/300 loss 210.030151 loss_att 200.721848 loss_ctc 231.749512 lr 0.00004816 rank 4\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,795 DEBUG TRAIN Batch 0/300 loss 197.888428 loss_att 188.946869 loss_ctc 218.752106 lr 0.00004816 rank 3\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,796 DEBUG TRAIN Batch 0/300 loss 211.922333 loss_att 202.549347 loss_ctc 233.792587 lr 0.00004816 rank 1\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,799 DEBUG TRAIN Batch 0/300 loss 215.290100 loss_att 205.810791 loss_ctc 237.408493 lr 0.00004816 rank 2\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,804 DEBUG TRAIN Batch 0/300 loss 220.808731 loss_att 210.459335 loss_ctc 244.957321 lr 0.00004816 rank 0\u001b[0m\n",
      "\u001b[34m2023-03-28 23:34:40,805 DEBUG TRAIN Batch 0/300 loss 207.383881 loss_att 197.987350 loss_ctc 229.309128 lr 0.00004816 rank 6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "job_name = est.fit({\"training\":training})\n",
    "#job_name = est.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d07b7c9-acde-4d0a-b179-3134d28f6f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact saved at:\n",
      " s3://sagemaker-us-east-1-348052051973/wenet/wenet-2022-12-08-05-36-25-104/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_data = est.model_data\n",
    "print(\"Model artifact saved at:\\n\", model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55192e1-23f5-4c2f-8c0f-5d1c43655226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
